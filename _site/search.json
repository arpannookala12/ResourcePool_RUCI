[
  {
    "objectID": "smsm.html",
    "href": "smsm.html",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "",
    "text": "# Python code here\n# Loading Required Libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)"
  },
  {
    "objectID": "smsm.html#reading-the-individual-level-data",
    "href": "smsm.html#reading-the-individual-level-data",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Reading the Individual Level Data",
    "text": "Reading the Individual Level Data\n\n# Read the Household Pulse Survey data from a CSV file\nind &lt;- read.csv(\"hps_04_00_01_puf.csv\")\n# Display the data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#data-filtering-and-variable-selection",
    "href": "smsm.html#data-filtering-and-variable-selection",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Data Filtering and Variable Selection",
    "text": "Data Filtering and Variable Selection\n\n# Filter data for Middlesex County (EST_MSA=35620) and state of New Jersey (EST_ST=34)\nind &lt;- ind %&gt;% filter(EST_MSA==35620 & EST_ST==34)\n\n# Calculate Age from Birth Year\nind$AGE &lt;- 2024 - ind$TBIRTH_YEAR\n\n# Select variables of interest\nvariables_of_interest &lt;- c(\"EST_ST\",\"EST_MSA\", \"MS\", \"RRACE\", \"TWDAYS\", \"EEDUC\", \"KINDWORK\", \"THHLD_NUMPER\", \"INCOME\",\"EGENID_BIRTH\",\"AGE\",\"RHISPANIC\",\"ANYWORK\")\nind &lt;- ind %&gt;% select(all_of(variables_of_interest))\n\n# Display the filtered and selected data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#data-cleaning-and-transformations",
    "href": "smsm.html#data-cleaning-and-transformations",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Data Cleaning and Transformations",
    "text": "Data Cleaning and Transformations\n\n# Display unique values of variables for understanding\nsort(unique(ind$AGE))\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69\n[51] 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 88\n\nsort(unique(ind$EEDUC))\n\n[1] 1 2 3 4 5 6 7\n\nsort(unique(ind$EGENID_BIRTH))\n\n[1] 1 2\n\nsort(unique(ind$KINDWORK))\n\n[1] -99 -88   1   2   3   4   5\n\n# Replace special values with NA in education variable\nind$EEDUC[ind$EEDUC == -99] &lt;- NA\nind$EEDUC[ind$EEDUC == -88] &lt;- NA\n\n# Drop rows with NA values\nind &lt;- ind %&gt;% drop_na()\n\n# Filter data for individuals aged 25 and above\nind &lt;- ind[ind$AGE &gt;= 25,]\n\n# Display cleaned data\nhead(as.data.frame(ind))\n\n\n  \n\n\n# Display unique values again after cleaning\nsort(unique(ind$AGE))\n\n [1] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n[26] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n[51] 75 76 77 78 79 80 81 82 83 84 85 86 88\n\nsort(unique(ind$EEDUC))\n\n[1] 1 2 3 4 5 6 7\n\nsort(unique(ind$EGENID_BIRTH))\n\n[1] 1 2\n\nsort(unique(ind$KINDWORK))\n\n[1] -99 -88   1   2   3   4   5"
  },
  {
    "objectID": "smsm.html#recoding-variables",
    "href": "smsm.html#recoding-variables",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Recoding Variables",
    "text": "Recoding Variables\n\n# Define age group breaks and labels\nbrks &lt;- c(25, 30, 35, 40, 45, 50, 55, 60, 62, 65, 67, 70, 75, 80, 85, Inf)\nlabs &lt;- c(\"25 to 29 years\", \"30 to 34 years\", \"35 to 39 years\", \"40 to 44 years\", \n          \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\", \"60 and 61 years\", \n          \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\", \n          \"75 to 79 years\", \"80 to 84 years\",\"85 years and over\")\n\n# Recode age into groups\nind$AGE &lt;- cut(ind$AGE, breaks = brks, labels = labs, right=FALSE)\n\n# Recode gender variable\nind$EGENID_BIRTH &lt;- factor(ind$EGENID_BIRTH, levels = 1:2, labels = c(\"Male\", \"Female\"))\n\n# Recode education levels\nind$EEDUC &lt;- factor(ind$EEDUC, levels = 1:7, labels = c(\"Less than high school\",\"Some high school\",\"High school graduate or equivalent (for example GED)\",\"Some college, but degree not received or is in progress\",\"Associate’s degree (for example AA, AS)\",\"Bachelor's degree (for example BA, BS, AB)\",\"Graduate degree (for example master's, professional, doctorate)\"))\n\n# Display recoded data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#reading-and-validating-constraint-data",
    "href": "smsm.html#reading-and-validating-constraint-data",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Reading and Validating Constraint Data",
    "text": "Reading and Validating Constraint Data\n\n# Read constraint data for age, sex, and education from CSV files\ncon_age &lt;- read.csv(\"con_age.csv\")\ncon_sex &lt;- read.csv(\"con_sex.csv\")\ncon_edu &lt;- read.csv(\"con_edu.csv\")\n\n# Display the constraint data\nhead(as.data.frame(con_age))\n\n\n  \n\n\nhead(as.data.frame(con_sex))\n\n\n  \n\n\nhead(as.data.frame(con_edu))\n\n\n  \n\n\n# Validate the sums of the constraints\n# Sum of age constraints\nsum(con_age)\n\n[1] 591318\n\n# Sum of sex constraints\nsum(con_sex)\n\n[1] 591318\n\n# Sum of education constraints\nsum(con_edu)\n\n[1] 591318\n\n# Validate the row sums of the constraints\n# Row sums of age constraints\nrowSums(con_age)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Row sums of sex constraints\nrowSums(con_sex)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Row sums of education constraints\nrowSums(con_edu)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of different constraints are equal\n# Check if row sums of age and sex constraints are equal\nrowSums(con_age) == rowSums(con_sex)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n# Check if row sums of age and education constraints are equal\nrowSums(con_age) == rowSums(con_edu)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n# Check if row sums of sex and education constraints are equal\nrowSums(con_sex) == rowSums(con_edu)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "smsm.html#naming-columns-and-combining-constraints",
    "href": "smsm.html#naming-columns-and-combining-constraints",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Naming Columns and Combining Constraints",
    "text": "Naming Columns and Combining Constraints\n\n# Assign column names to age and education constraints based on the levels in the individual data\nnames(con_age) &lt;- levels(ind$AGE)\nnames(con_edu) &lt;- levels(ind$EEDUC)\n\n# Combine age, sex, and education constraints into a single data frame\ncons &lt;- cbind(con_age, con_sex, con_edu)\n\n# Display the combined constraints\nhead(as.data.frame(cons))"
  },
  {
    "objectID": "smsm.html#creating-categorical-indicator-matrices",
    "href": "smsm.html#creating-categorical-indicator-matrices",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Creating Categorical Indicator Matrices",
    "text": "Creating Categorical Indicator Matrices\n\n# Create binary indicator matrix for age categories\ncat_age &lt;- model.matrix(~ ind$AGE - 1)\n# Display the dimensions of the age indicator matrix\ndim(cat_age)\n\n[1] 821  15\n\n# Create binary indicator matrix for sex categories\ncat_sex &lt;- model.matrix(~ ind$EGENID_BIRTH - 1)\n# Display the dimensions of the sex indicator matrix\ndim(cat_sex)\n\n[1] 821   2\n\n# Create binary indicator matrix for education categories\ncat_edu &lt;- model.matrix(~ ind$EEDUC - 1)\n# Display the dimensions of the education indicator matrix\ndim(cat_edu)\n\n[1] 821   7\n\n# Combine age, sex, and education indicator matrices into a single matrix\nind_cat &lt;- cbind(cat_age, cat_sex, cat_edu)\n\n# Display the first few rows of the combined indicator matrix\nhead(as.data.frame(ind_cat))"
  },
  {
    "objectID": "smsm.html#aggregating-indicator-matrix-and-defining-dimensions",
    "href": "smsm.html#aggregating-indicator-matrix-and-defining-dimensions",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Aggregating Indicator Matrix and Defining Dimensions",
    "text": "Aggregating Indicator Matrix and Defining Dimensions\n\n# Calculate the column sums of the indicator matrix\ncolSums(ind_cat)\n\n                                                   ind$AGE25 to 29 years \n                                                                      27 \n                                                   ind$AGE30 to 34 years \n                                                                      56 \n                                                   ind$AGE35 to 39 years \n                                                                      79 \n                                                   ind$AGE40 to 44 years \n                                                                      99 \n                                                   ind$AGE45 to 49 years \n                                                                      99 \n                                                   ind$AGE50 to 54 years \n                                                                      76 \n                                                   ind$AGE55 to 59 years \n                                                                      73 \n                                                  ind$AGE60 and 61 years \n                                                                      29 \n                                                   ind$AGE62 to 64 years \n                                                                      64 \n                                                  ind$AGE65 and 66 years \n                                                                      23 \n                                                   ind$AGE67 to 69 years \n                                                                      41 \n                                                   ind$AGE70 to 74 years \n                                                                      75 \n                                                   ind$AGE75 to 79 years \n                                                                      50 \n                                                   ind$AGE80 to 84 years \n                                                                      18 \n                                                ind$AGE85 years and over \n                                                                      12 \n                                                    ind$EGENID_BIRTHMale \n                                                                     400 \n                                                  ind$EGENID_BIRTHFemale \n                                                                     421 \n                                          ind$EEDUCLess than high school \n                                                                      12 \n                                               ind$EEDUCSome high school \n                                                                      21 \n           ind$EEDUCHigh school graduate or equivalent (for example GED) \n                                                                     103 \n        ind$EEDUCSome college, but degree not received or is in progress \n                                                                     141 \n                        ind$EEDUCAssociate’s degree (for example AA, AS) \n                                                                      51 \n                     ind$EEDUCBachelor's degree (for example BA, BS, AB) \n                                                                     256 \nind$EEDUCGraduate degree (for example master's, professional, doctorate) \n                                                                     237 \n\n# Aggregate the individual categories\nind_agg &lt;- colSums(ind_cat)\n\n# Compare the first row of constraints with the aggregated individual categories\nrbind(cons[1,], ind_agg)\n\n\n  \n\n\n# Define the number of zones (rows) in the constraints data\nn_zone &lt;- nrow(cons)\n# Display the number of zones\nn_zone\n\n[1] 566\n\n# Define the number of individuals in the individual data\nn_ind &lt;- nrow(ind)\n# Display the number of individuals\nn_ind\n\n[1] 821\n\n# Define the number of age categories\nn_age &lt;- ncol(con_age)\n# Display the number of age categories\nn_age\n\n[1] 15\n\n# Define the number of sex categories\nn_sex &lt;- ncol(con_sex)\n# Display the number of sex categories\nn_sex\n\n[1] 2\n\n# Define the number of education categories\nn_edu &lt;- ncol(con_edu)\n# Display the number of education categories\nn_edu\n\n[1] 7"
  },
  {
    "objectID": "smsm.html#initializing-weight-matrices-and-performing-first-iteration-of-ipf",
    "href": "smsm.html#initializing-weight-matrices-and-performing-first-iteration-of-ipf",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Initializing Weight Matrices and Performing First Iteration of IPF",
    "text": "Initializing Weight Matrices and Performing First Iteration of IPF\n\n# Initialize weight matrices with uniform weights\nweights &lt;- matrix(data = 1, nrow = nrow(ind), ncol = nrow(cons))\nweights3 &lt;- weights1 &lt;- weights2 &lt;- weights\n# Display the dimensions of the weights matrix\ndim(weights)\n\n[1] 821 566\n\n# Aggregate the individual categories initially, based on constraints\nind_agg0 &lt;- t(apply(cons, 1, function(x) 1 * ind_agg))\ncolnames(ind_agg0) &lt;- names(cons)\n# Display the initial aggregated individual categories\nhead(as.data.frame(ind_agg0))\n\n\n  \n\n\n# First iteration of IPF to adjust weights based on age constraints\nfor(j in 1:n_zone){\n  for(i in 1:n_age){\n    index &lt;- ind_cat[, i] == 1\n    weights1[index, j] &lt;- weights[index, j] * con_age[j, i] / ind_agg0[j, i]\n  }\n}\n# Display the adjusted weights after the first iteration\nhead(as.data.frame(weights1))"
  },
  {
    "objectID": "smsm.html#aggregating-after-first-ipf-iteration",
    "href": "smsm.html#aggregating-after-first-ipf-iteration",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Aggregating After First IPF Iteration",
    "text": "Aggregating After First IPF Iteration\n\n# Initialize aggregated matrices to store results of each IPF iteration\nind_agg3 &lt;- ind_agg2 &lt;- ind_agg1 &lt;- ind_agg0 * NA\n\n# Aggregate individual categories after the first IPF iteration\nfor(i in 1:n_zone){\n  ind_agg1[i, ] &lt;- colSums(ind_cat * weights1[, i])\n}\n# Display the aggregated individual categories after the first iteration\nhead(as.data.frame(ind_agg1))\n\n\n  \n\n\n# Calculate the row sums of the aggregated categories and the constraints for age\nrowSums(ind_agg1[, 1:15])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\nrowSums(cons[, 1:15])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for age\nrowSums(ind_agg1[, 1:15]) == rowSums(cons[, 1:15])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[565]  TRUE  TRUE"
  },
  {
    "objectID": "smsm.html#second-iteration-of-ipf-and-validation",
    "href": "smsm.html#second-iteration-of-ipf-and-validation",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Second Iteration of IPF and Validation",
    "text": "Second Iteration of IPF and Validation\n\n# Second iteration of IPF to adjust weights based on sex and age constraints\nfor (j in 1:n_zone) {\n  for (i in 1:(n_sex + n_age)) {\n    index &lt;- ind_cat[, i] == 1\n    weights2[index, j] &lt;- weights1[index, j] * cons[j, i] / ind_agg1[j, i]\n  }\n}\n# Display the adjusted weights after the second iteration\nhead(as.data.frame(weights2))\n\n\n  \n\n\n# Aggregate individual categories after the second IPF iteration\nfor(i in 1:n_zone){\n  ind_agg2[i, ] &lt;- colSums(ind_cat * weights2[, i])\n}\n# Display the aggregated individual categories after the second iteration\nhead(as.data.frame(ind_agg2))\n\n\n  \n\n\n# Calculate and display the row sums of the aggregated categories for sex\nrowSums(ind_agg2[, 16:17])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Calculate and display the row sums of the constraints for sex\nrowSums(cons[, 16:17])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for sex\nrowSums(ind_agg2[, 16:17]) == rowSums(cons[, 16:17])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[565]  TRUE  TRUE"
  },
  {
    "objectID": "smsm.html#third-iteration-of-ipf-and-final-validation",
    "href": "smsm.html#third-iteration-of-ipf-and-final-validation",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Third Iteration of IPF and Final Validation",
    "text": "Third Iteration of IPF and Final Validation\n\n# Third iteration of IPF to adjust weights based on sex, age, and education constraints\nfor(j in 1:n_zone){\n  for(i in 1:(n_sex + n_age + n_edu)){\n    index &lt;- ind_cat[, i] == 1\n    if(ind_agg2[j, i] != 0) {  # Check to avoid division by zero\n      weights3[index, j] &lt;- weights2[index, j] * cons[j, i] / ind_agg2[j, i]\n    }\n  }\n}\n# Display the adjusted weights after the third iteration\nhead(as.data.frame(weights3))\n\n\n  \n\n\n# Aggregate individual categories after the third IPF iteration\nfor(i in 1:n_zone){\n  ind_agg3[i, ] &lt;- colSums(ind_cat * weights3[, i])\n}\n# Display the aggregated individual categories after the third iteration\nhead(as.data.frame(ind_agg3))\n\n\n  \n\n\n# Calculate and display the row sums of the aggregated categories for education\nrowSums(ind_agg3[, 18:24])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   61 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  117  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   24  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Calculate and display the row sums of the constraints for education\nrowSums(cons[, 18:24])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for education\nrowSums(ind_agg3[, 18:24]) == rowSums(cons[, 18:24])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[565]  TRUE FALSE"
  },
  {
    "objectID": "smsm.html#correlation-analysis-between-aggregated-data-and-constraints",
    "href": "smsm.html#correlation-analysis-between-aggregated-data-and-constraints",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Correlation Analysis Between Aggregated Data and Constraints",
    "text": "Correlation Analysis Between Aggregated Data and Constraints\n\n# Function to convert matrix to numeric vector\nvec &lt;- function(x) as.numeric(as.matrix(x))\n\n# Calculate and display the correlation between the initial aggregated data and constraints\ncor(vec(ind_agg0), vec(cons))\n\n[1] 0.723838\n\n# Calculate and display the correlation between the first iteration aggregated data and constraints\ncor(vec(ind_agg1), vec(cons))\n\n[1] 0.9052638\n\n# Calculate and display the correlation between the second iteration aggregated data and constraints\ncor(vec(ind_agg2), vec(cons))\n\n[1] 0.9197018\n\n# Calculate and display the correlation between the third iteration aggregated data and constraints\ncor(vec(ind_agg3), vec(cons))\n\n[1] 0.9967264"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resource List - RUCI LAB",
    "section": "",
    "text": "HPS, CPS, PUMS Binary Classification Modeling & Explainability\nResults & Analysis\n\n\nSpatial Microsimulation using the Household Pulse Survey & American Community Survey\nR markdown notebook\n\n\nLiterature Review (Use of Big Data & ML in Public Transport)\nSheet link\n\n\nOrdinal Regression Models (for Teleworking Days) using Household Pulse Survey\nEDA performed on the dataset\n\n\nData Dictionary (CDC, EPA, USGS)\nGo to dictionary\n\n\nMaps on Felt\n\nNew Brunswick Area Map Go to map\nJersey City Area Map Go to map\n\n\n\nBox Link\nGo to box"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "First few rows of dataframe",
    "section": "",
    "text": "First few rows of dataframe\n\n\n\n\n\n\n\n\n\n\nSCRAM\nCYCLE\nEST_ST\nEST_MSA\nREGION\nHWEIGHT\nPWEIGHT\nTBIRTH_YEAR\nABIRTH_YEAR\nRHISPANIC\nAHISPANIC\nRRACE\nARACE\nEEDUC\nAEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nGENID_DESCRIBE\nSEXUAL_ORIENTATION\nTHHLD_NUMPER\nAHHLD_NUMPER\nTHHLD_NUMKID\nAHHLD_NUMKID\nTHHLD_NUMADLT\nKIDS_LT5Y\nKIDS_5_11Y\nKIDS_12_17Y\nENRPUBCHK\nENRPRVCHK\nENRHMSCHK\nTENROLLPUB\nTENROLLPRV\nTENROLLHMSCH\nENROLLNONE\nACTVDUTY1\nACTVDUTY2\nACTVDUTY3\nACTVDUTY4\nACTVDUTY5\nRECVDVACC\nHADCOVIDRV\nSYMPTOMS\nLONGCOVID\nSYMPTMNOW\nWRKLOSSRV\nANYWORK\nKINDWORK\nRSNNOWRKRV\nEXPNS_DIF\nTWDAYS\nCURFOODSUF\nCHILDFOOD\nFOODRSNRV1\nFOODRSNRV2\nFOODRSNRV3\nFOODRSNRV4\nFREEFOOD\nSCHLFDHLP_RV1\nSCHLFDHLP_RV2\nSCHLFDHLP_RV3\nSCHLFDHLP_RV4\nSCHLFDHLP_RV5\nFDBENEFIT1\nANXIOUS\nWORRY\nINTEREST\nDOWN\nHLTHINS1\nHLTHINS2\nHLTHINS3\nHLTHINS4\nHLTHINS5\nHLTHINS6\nHLTHINS7\nHLTHINS8\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nRENTCHNG\nLIVQTRRV\nRENTCUR\nMORTCUR\nTMNTHSBHND\nEVICT\nFORCLOSE\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nSYMPTMIMPCT\nPRICECHNG\nPRICESTRESS\nPRICECONCRN\nTWDAYS_RESP\nFRMLA_YN\nFRMLA_AGE\nFRMLA_DIFFCLT\nGAS1\nGAS2\nGAS3\nGAS4\nSCHLFDHLP_RV6\nSCHLFDHLP_RV7\nSCHLFDHLP_RV8\nFDBENEFIT2\nSCHLFDEXPNS\nND_DISPLACE\nND_TYPE1\nND_TYPE2\nND_TYPE3\nND_TYPE4\nND_TYPE5\nND_HOWLONG\nND_DAMAGE\nND_FDSHRTAGE\nND_WATER\nND_ELCTRC\nND_UNSANITARY\nND_ISOLATE\nND_CRIME\nND_SCAM\nFDBENEFIT3\nBABY_FED\nMHLTH_NEED\nMHLTH_GET\nMHLTH_SATISFD\nMHLTH_DIFFCLT\nMOVEWHY1\nMOVEWHY2\nMOVEWHY3\nMOVEWHY4\nMOVEWHY5\nMOVEWHY6\nMOVEWHY7\nMOVEWHY8\nMOVED\nWHENCOVIDRV1\nWHENCOVIDRV2\nWHENCOVIDRV3\nVETERAN1\nVETERAN2\nVETERAN3\nVETERAN4\nVETERAN5\nCHILDCARE\nCHILDCARE_RSLT1\nCHILDCARE_RSLT2\nCHILDCARE_RSLT3\nCHILDCARE_RSLT4\nCHILDCARE_RSLT5\nCHILDCARE_RSLT6\nCHILDCARE_RSLT7\nCHILDCARE_RSLT8\nCHILDCARE_RSLT9\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nSUPPORT4\nSUPPORT1EXP\nRVACCDATE\nRSVVACC\n\n\n\n\n0\nP020000001\n2\n32\nNaN\n4\n704.966315\n2067.690868\n1976\n1\n1\n2\n2\n2\n6\n2\n4\n2\n2\n2\n2\n3\n2\n0\n2\n3\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n1\n-99\n2\n-88\n-99\n1\n1\n-99\n2\n-88\n-88\n-88\n-88\n-88\n1\n4\n4\n4\n4\n-99\n-99\n-99\n1\n-99\n-99\n-99\n-99\n3\n1\n3\n1\n3\n2\n3\n1\n3\n2\n6\n1\n-88\n-88\n-88\n-88\n2\n2\n3\n2\n-88\n4\n-88\n2\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-99\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n5\n3\n2\n1\n4\n1\n2\n-99\n-88\n\n\n1\nP020000002\n2\n53\nNaN\n4\n716.582115\n1359.474802\n1961\n2\n1\n2\n1\n2\n5\n2\n3\n2\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n-99\n4\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n3\n3\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n\n\n2\nP020000003\n2\n6\n31080.0\n4\n2439.529962\n4554.378984\n1988\n2\n1\n2\n1\n2\n7\n2\n1\n2\n2\n2\n3\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n3\n2\n2\n2\n1\n1\n-88\n1\n1\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n7\n-88\n4\n-88\n4\n1\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n-99\n1\n-99\n1\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n4\n2\n2\n1\n1\n4\n1\n-88\n\n\n3\nP020000004\n2\n48\nNaN\n2\n3945.461037\n7550.581707\n1956\n2\n1\n2\n1\n2\n5\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n2\n-88\n-88\n-88\n2\n1\n-99\n-88\n2\n4\n2\n-88\n1\n-99\n-99\n-99\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n2\n-88\n1\n2\n1\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n3\n3\n4\n2\n4\n1\n1\n\n\n4\nP020000005\n2\n53\n42660.0\n4\n489.900163\n929.421644\n1970\n2\n1\n2\n1\n2\n6\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n2\n2\n2\n2\n1\n1\n-88\n1\n3\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n8\n-88\n1\n2\n1\n4\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n2\n1\n1\n1\n4\n2\n-88\n\n\n\n\n\n\n\nSource: SAE.ipynb\n\n\nEST_ST distribution\n\n\n\nEST_ST\n06    1485\n53     842\n48     841\n51     596\n25     587\n08     572\n24     561\n42     496\n12     482\n41     459\n11     449\n13     449\n04     448\n17     424\n36     423\n26     413\n27     392\n49     373\n34     340\n37     323\n09     306\n55     303\n29     278\n39     261\n18     250\n47     248\n33     210\n19     203\n35     202\n20     202\n21     192\n45     185\n16     177\n40     164\n50     160\n31     155\n44     150\n32     145\n01     140\n02     138\n23     138\n05     137\n10     128\n30     118\n15     107\n22     103\n46     101\n28      83\n54      80\n38      71\n56      49\nName: count, dtype: int64\n\n\n\n\n\nEST_MSA distribution\n\n\n\nEST_MSA\n47900.0    1125\n42660.0     628\n41860.0     586\n35620.0     537\n14460.0     522\n37980.0     441\n31080.0     404\n16980.0     388\n19100.0     386\n12060.0     370\n38060.0     350\n26420.0     256\n19820.0     234\n33100.0     194\n40140.0     176\nName: count, dtype: int64\n\n\n\n\n\nState-Wise Telecommuting Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\nCity-Wise Telecommuting Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting Frequency Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Educational Attainment\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Employment Sector\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Number of People in the Household\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by presence of Children in the Household"
  },
  {
    "objectID": "baselinemodels.html",
    "href": "baselinemodels.html",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "",
    "text": "CPS Binary Classification Model Results & Explainability\nMulticollinearity Analysis\n\n\n\nPerforming multicollinearity analysis...\n\n\nVIF Results:\n     Feature        VIF\n0   hehousut   1.046027\n1   hetelhhd   2.254292\n2   hetelavl   2.282600\n3   hefaminc   1.434420\n4   hrnumhou   2.452866\n5    hrhtype   4.732866\n6      HUBUS   1.156651\n7      gereg   0.000000\n8      gediv   0.000000\n9   gestfips   2.652757\n10    gtcbsa   2.183617\n11      gtco   2.202980\n12  gtcbsast   4.391335\n13  gtmetsta   2.231556\n14  gtindvpc   2.651372\n15  gtcbsasz   9.794757\n16     gtcsa   5.458005\n17     perrp   5.387685\n18    prtage   1.678536\n19  pemaritl   3.464919\n20     pesex   1.148676\n21   peeduca   1.524835\n22  ptdtrace   1.271114\n23   prdthsp   5.773714\n24  PUCHINHH   1.015834\n25  prfamrel   6.719113\n26  prfamtyp   4.662360\n27  pehspnon   5.659180\n28  penatvty   7.903467\n29  pemntvty   8.318010\n30  pefntvty   7.946751\n31  prcitshp  12.204819\n32  prinuyer   8.446316\n33      PUWK   1.015886\n34    pemjot  83.410023\n35   pemjnum  83.510317\n36  pehruslt   2.022003\n37  pehractt   2.050188\n38  peio1cow   1.171097\n39  prdtind1   1.193875\n40  prdtocc1   1.359964\n41   pternwa   3.260463\n42      ptwk   3.171225\n43    prchld   4.825561\n44  prnmchld   5.667953\nDropping features with high VIF: ['prcitshp', 'pemjot', 'pemjnum']\n\n\nSource: HPS Binary Classification Model\nModel Peformance\n\n\n\n\nCross-validation results:\nBest AUC: 0.8887 (+/- 0.0034)\nBest Error: 0.1568 (+/- 0.0013)\n\nTraining final model...\n\n\n\nComprehensive Model Performance Metrics:\n\nAccuracy: 0.8443\nPrecision (Positive Predictive Value): 0.8764\nRecall (Sensitivity/True Positive Rate): 0.9254\nSpecificity (True Negative Rate): 0.5886\nFalse Positive Rate: 0.4114\nFalse Negative Rate: 0.0746\nF1 Score: 0.9002\nPrevalence: 0.7593\nNegative Predictive Value: 0.7143\nPositive Likelihood Ratio: 2.2492\nNegative Likelihood Ratio: 0.1268\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.59      0.65       858\n           1       0.88      0.93      0.90      2706\n\n    accuracy                           0.84      3564\n   macro avg       0.80      0.76      0.77      3564\nweighted avg       0.84      0.84      0.84      3564\n\n\nConfusion Matrix:\n[[ 505  353]\n [ 202 2504]]\n\nROC AUC Score: 0.8916\n\nTop 10 Most Important Features:\n     Feature  Importance\n26  prdtocc1       234.0\n25  prdtind1       223.0\n12    prtage       176.0\n6       gtco       134.0\n15   peeduca       127.0\n23  pehractt       126.0\n0   hefaminc       111.0\n24  peio1cow       108.0\n19  pemntvty        91.0\n3      HUBUS        77.0\n11     perrp        75.0\n1   hrnumhou        67.0\n22  pehruslt        66.0\n21  prinuyer        58.0\n20  pefntvty        55.0\n17  prfamrel        51.0\n5     gtcbsa        47.0\n27   pternwa        44.0\n28    prchld        43.0\n14     pesex        42.0\n10     gtcsa        36.0\n29  prnmchld        36.0\n16  ptdtrace        32.0\n18  penatvty        28.0\n2    hrhtype        27.0\n4   gestfips        26.0\n8   gtindvpc        24.0\n13  pemaritl        22.0\n9   gtcbsasz        14.0\n7   gtcbsast        12.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbit Model\n\n\n\n\nFitting Probit model for effect sizes and p-values...\nOptimization terminated successfully.\n         Current function value: 0.427806\n         Iterations 6\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14224\nMethod:                           MLE   Df Model:                           30\nDate:                Sun, 02 Feb 2025   Pseudo R-squ.:                  0.2249\nTime:                        05:33:30   Log-Likelihood:                -6098.4\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2208      0.902      2.463      0.014       0.454       3.988\nhefaminc      -0.0550      0.006     -9.743      0.000      -0.066      -0.044\nhrnumhou       0.0550      0.014      3.858      0.000       0.027       0.083\nhrhtype        0.0036      0.014      0.266      0.790      -0.023       0.030\nHUBUS          0.2808      0.031      9.035      0.000       0.220       0.342\ngestfips       0.1019      0.022      4.544      0.000       0.058       0.146\ngtcbsa     -3.337e-06   1.93e-06     -1.729      0.084   -7.12e-06    4.46e-07\ngtco        -8.21e-05      0.001     -0.151      0.880      -0.001       0.001\ngtcbsast       0.1007      0.032      3.175      0.001       0.039       0.163\ngtindvpc       0.0028      0.041      0.069      0.945      -0.077       0.083\ngtcbsasz       0.0894      0.020      4.542      0.000       0.051       0.128\ngtcsa         -0.0012      0.000     -6.856      0.000      -0.002      -0.001\nperrp          0.0007      0.005      0.137      0.891      -0.009       0.011\nprtage         0.0054      0.001      4.802      0.000       0.003       0.008\npemaritl       0.0251      0.011      2.260      0.024       0.003       0.047\npesex         -0.1291      0.027     -4.695      0.000      -0.183      -0.075\npeeduca       -0.1358      0.007    -19.551      0.000      -0.149      -0.122\nptdtrace      -0.0401      0.011     -3.645      0.000      -0.062      -0.019\nprfamrel       0.1337      0.022      5.951      0.000       0.090       0.178\npenatvty    7.458e-05      0.000      0.246      0.805      -0.001       0.001\npemntvty       0.0010      0.000      3.370      0.001       0.000       0.002\npefntvty      -0.0003      0.000     -1.054      0.292      -0.001       0.000\nprinuyer       0.0044      0.003      1.526      0.127      -0.001       0.010\npehruslt       0.0014      0.001      1.061      0.289      -0.001       0.004\npehractt      -0.0056      0.002     -3.699      0.000      -0.009      -0.003\npeio1cow      -0.1141      0.012     -9.191      0.000      -0.138      -0.090\nprdtind1       0.0064      0.001      5.538      0.000       0.004       0.009\nprdtocc1       0.0539      0.002     24.860      0.000       0.050       0.058\npternwa    -1.354e-07   9.27e-08     -1.461      0.144   -3.17e-07    4.62e-08\nprchld         0.0042      0.010      0.407      0.684      -0.016       0.024\nprnmchld      -0.0282      0.033     -0.859      0.390      -0.092       0.036\n==============================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nhefaminc      -0.0132      0.001     -9.814      0.000      -0.016      -0.011\nhrnumhou       0.0133      0.003      3.862      0.000       0.007       0.020\nhrhtype        0.0009      0.003      0.266      0.790      -0.006       0.007\nHUBUS          0.0676      0.007      9.111      0.000       0.053       0.082\ngestfips       0.0245      0.005      4.554      0.000       0.014       0.035\ngtcbsa     -8.035e-07   4.65e-07     -1.729      0.084   -1.71e-06    1.07e-07\ngtco       -1.977e-05      0.000     -0.151      0.880      -0.000       0.000\ngtcbsast       0.0242      0.008      3.179      0.001       0.009       0.039\ngtindvpc       0.0007      0.010      0.069      0.945      -0.019       0.020\ngtcbsasz       0.0215      0.005      4.552      0.000       0.012       0.031\ngtcsa         -0.0003   4.27e-05     -6.884      0.000      -0.000      -0.000\nperrp          0.0002      0.001      0.137      0.891      -0.002       0.003\nprtage         0.0013      0.000      4.810      0.000       0.001       0.002\npemaritl       0.0060      0.003      2.261      0.024       0.001       0.011\npesex         -0.0311      0.007     -4.706      0.000      -0.044      -0.018\npeeduca       -0.0327      0.002    -20.201      0.000      -0.036      -0.030\nptdtrace      -0.0097      0.003     -3.649      0.000      -0.015      -0.004\nprfamrel       0.0322      0.005      5.969      0.000       0.022       0.043\npenatvty    1.796e-05   7.29e-05      0.246      0.805      -0.000       0.000\npemntvty       0.0002   7.18e-05      3.374      0.001       0.000       0.000\npefntvty   -7.247e-05   6.87e-05     -1.054      0.292      -0.000    6.23e-05\nprinuyer       0.0011      0.001      1.527      0.127      -0.000       0.002\npehruslt       0.0003      0.000      1.061      0.289      -0.000       0.001\npehractt      -0.0014      0.000     -3.705      0.000      -0.002      -0.001\npeio1cow      -0.0275      0.003     -9.272      0.000      -0.033      -0.022\nprdtind1       0.0015      0.000      5.555      0.000       0.001       0.002\nprdtocc1       0.0130      0.000     26.497      0.000       0.012       0.014\npternwa    -3.261e-08   2.23e-08     -1.462      0.144   -7.63e-08    1.11e-08\nprchld         0.0010      0.002      0.407      0.684      -0.004       0.006\nprnmchld      -0.0068      0.008     -0.860      0.390      -0.022       0.009\n==============================================================================\n\n\n\nSHAP Results\n\n\n\n\nCalculating and plotting SHAP values for XGBoost model...\nGenerating SHAP Beeswarm plot...\n\n\n\n\n\n\n\n\n\nGenerating SHAP Bar plot...\n\n\n\n\n\n\n\n\n\n\n\n\nPUMS Binary Classification Model Results & Explainabilityactivat"
  },
  {
    "objectID": "datadict.html",
    "href": "datadict.html",
    "title": "Data Dictionary (Data from CDC PLACES, EPA SLD, USGS Building Footprint)",
    "section": "",
    "text": "CDC PLACES\nGo to metadata source\n\nimport pandas as pd\nfrom itables import show\ndf1 = pd.read_csv(\"CDC_metadata.csv\")\nshow(df1)\n\n\n\n    \n      \n      Column Name\n      Description\n      Type\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\nEPA SLD\nGo to metadata source\n\ndf2 = pd.read_csv(\"EPA_metadata.csv\")\nshow(df2)\n\n\n\n    \n      \n      Field Name\n      Description\n      Data Source\n      Geographic Coverage*\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\nUSGS Building Footprint\nGo to metadata source\n\ndf3 = pd.read_csv(\"USGSBFP_metadata.csv\")\nshow(df3)\n\n\n\n    \n      \n      Attribute Label\n      Attribute Definition\n      Attribute Definition Source\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "hps_cps_pums.html",
    "href": "hps_cps_pums.html",
    "title": "HPS Binary Classification Model",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\npd.set_option(\"display.max_columns\",None)\n\n\n# Load and prepare data\nfiltered_df = pd.read_csv(\"/content/combinedphase4hps.csv\")\nX = filtered_df.drop(['Unnamed: 0','PWEIGHT', 'HWEIGHT', 'SCRAM', 'TWDAYS','TWDAYS_RESP','AHHLD_NUMPER','AHHLD_NUMKID','ABIRTH_YEAR','AEDUC','ARACE'], axis=1)\ny = filtered_df['TWDAYS_RESP']\n\n\nX\n\n\n  \n    \n\n\n\n\n\n\nCYCLE\nEST_ST\nREGION\nRHISPANIC\nAHISPANIC\nRRACE\nEEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nTHHLD_NUMPER\nTHHLD_NUMKID\nTHHLD_NUMADLT\nACTVDUTY1\nRECVDVACC\nHADCOVIDRV\nWRKLOSSRV\nANYWORK\nKINDWORK\nEXPNS_DIF\nCURFOODSUF\nFREEFOOD\nANXIOUS\nWORRY\nINTEREST\nDOWN\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nLIVQTRRV\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nPRICECHNG\nPRICECONCRN\nND_DISPLACE\nVETERAN1\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nAGE\n\n\n\n\n0\n1.0\n36.0\n1.0\n1.0\n2.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n4.0\n3.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n6.0\n1.0\n3.0\n2.0\n1.0\n1.0\n4.0\n4.0\n2.0\n1.0\n55.0\n\n\n1\n1.0\n34.0\n1.0\n1.0\n2.0\n1.0\n6.0\n5.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n1.0\n1.0\n1.0\n3.0\n2.0\n2.0\n4.0\n4.0\n4.0\n4.0\n2.0\n2.0\n1.0\n2.0\n3.0\n3.0\n2.0\n1.0\n28.0\n\n\n2\n1.0\n36.0\n1.0\n2.0\n2.0\n1.0\n5.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n3.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n6.0\n1.0\n1.0\n2.0\n1.0\n5.0\n3.0\n2.0\n2.0\n2.0\n49.0\n\n\n3\n1.0\n34.0\n1.0\n1.0\n2.0\n1.0\n7.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n3.0\n2.0\n1.0\n2.0\n4.0\n4.0\n2.0\n1.0\n50.0\n\n\n4\n1.0\n36.0\n1.0\n1.0\n2.0\n1.0\n6.0\n1.0\n1.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n6.0\n4.0\n4.0\n4.0\n7.0\n1.0\n1.0\n2.0\n1.0\n2.0\n4.0\n3.0\n2.0\n2.0\n30.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5660\n9.0\n34.0\n1.0\n1.0\n2.0\n3.0\n5.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n4.0\n2.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n1.0\n2.0\n1.0\n2.0\n3.0\n1.0\n2.0\n4.0\n49.0\n\n\n5661\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n7.0\n5.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n2.0\n3.0\n2.0\n1.0\n2.0\n4.0\n3.0\n2.0\n3.0\n42.0\n\n\n5662\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n7.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n2.0\n2.0\n1.0\n3.0\n4.0\n4.0\n1.0\n1.0\n33.0\n\n\n5663\n9.0\n34.0\n1.0\n1.0\n2.0\n1.0\n5.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n3.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n3.0\n2.0\n3.0\n4.0\n1.0\n1.0\n2.0\n1.0\n1.0\n4.0\n4.0\n2.0\n1.0\n55.0\n\n\n5664\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n6.0\n5.0\n1.0\n2.0\n3.0\n0.0\n3.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n5.0\n2.0\n4.0\n4.0\n5.0\n4.0\n2.0\n2.0\n1.0\n2.0\n4.0\n2.0\n3.0\n1.0\n27.0\n\n\n\n\n5665 rows × 50 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert labels from 1.0/2.0 to 0/1\ny = (y == 2.0).astype(int)\n\n\nprint(\"Performing initial feature selection...\")\nselector = SelectKBest(mutual_info_classif, k=50)  # Keep top 50 features\nX_reduced = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support(indices=True)]\nprint(\"Top 50 features selected.\")\n\nPerforming initial feature selection...\nTop 50 features selected.\n\n\n\n# Create a DataFrame with reduced features for VIF and further steps\nX = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 15][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n  return 1 - self.ssr/self.centered_tss\n/usr/local/lib/python3.11/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n  vif = 1. / (1. - r_squared_i)\n\n\nVIF Results:\n          Feature       VIF\n0           CYCLE  1.017447\n1          EST_ST  1.177024\n2          REGION  0.000000\n3       RHISPANIC  1.042880\n4       AHISPANIC  1.004881\n5           RRACE  1.078979\n6           EEDUC  1.222808\n7              MS  1.753173\n8    EGENID_BIRTH  1.112458\n9    AGENID_BIRTH  1.011530\n10   THHLD_NUMPER       inf\n11   THHLD_NUMKID       inf\n12  THHLD_NUMADLT       inf\n13      ACTVDUTY1  0.000000\n14      RECVDVACC  1.068647\n15     HADCOVIDRV  1.041604\n16      WRKLOSSRV  1.048995\n17        ANYWORK  0.000000\n18       KINDWORK  1.110101\n19      EXPNS_DIF  2.112060\n20     CURFOODSUF  1.742856\n21       FREEFOOD  1.055199\n22        ANXIOUS  2.953812\n23          WORRY  2.912706\n24       INTEREST  2.490250\n25           DOWN  2.924415\n26       PRIVHLTH  1.254752\n27        PUBHLTH  1.224917\n28         SEEING  1.174728\n29        HEARING  1.139989\n30    REMEMBERING  1.370348\n31       MOBILITY  1.233866\n32       SELFCARE  1.274685\n33     UNDERSTAND  1.213315\n34         TENURE  1.668701\n35       LIVQTRRV  1.898722\n36         ENERGY  1.902153\n37       HSE_TEMP  1.230725\n38     ENRGY_BILL  1.621682\n39         INCOME  1.693776\n40      PRICECHNG  1.231449\n41    PRICECONCRN  1.629774\n42    ND_DISPLACE  1.017538\n43       VETERAN1  0.000000\n44        SOCIAL1  1.384256\n45        SOCIAL2  1.625487\n46       SUPPORT1  1.204116\n47       SUPPORT2  1.235698\n48       SUPPORT3  1.128506\n49            AGE  1.693441\nDropping features with high VIF: ['THHLD_NUMPER', 'THHLD_NUMKID', 'THHLD_NUMADLT']\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\nPerforming Recursive Feature Elimination...\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# X_train_final = X_train_final[non_outliers]\n# y_train = y_train[non_outliers]\n\n\nX_train_final\n\n\n  \n    \n\n\n\n\n\n\nCYCLE\nEST_ST\nRRACE\nEEDUC\nMS\nEGENID_BIRTH\nHADCOVIDRV\nKINDWORK\nEXPNS_DIF\nCURFOODSUF\nANXIOUS\nWORRY\nINTEREST\nDOWN\nPUBHLTH\nSEEING\nREMEMBERING\nTENURE\nLIVQTRRV\nENERGY\nINCOME\nPRICECHNG\nPRICECONCRN\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nAGE\n\n\n\n\n2128\n4.0\n34.0\n1.0\n6.0\n1.0\n1.0\n1.0\n4.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n4.0\n6.0\n2.0\n4.0\n1.0\n4.0\n4.0\n4.0\n1.0\n66.0\n\n\n691\n2.0\n36.0\n1.0\n6.0\n1.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n3.0\n4.0\n8.0\n2.0\n4.0\n2.0\n4.0\n1.0\n1.0\n1.0\n61.0\n\n\n730\n2.0\n36.0\n1.0\n7.0\n1.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n4.0\n6.0\n4.0\n3.0\n3.0\n3.0\n2.0\n2.0\n1.0\n49.0\n\n\n5443\n9.0\n36.0\n1.0\n6.0\n1.0\n2.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n7.0\n1.0\n3.0\n3.0\n4.0\n1.0\n1.0\n1.0\n54.0\n\n\n1984\n3.0\n34.0\n3.0\n7.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n3.0\n4.0\n7.0\n1.0\n1.0\n4.0\n3.0\n2.0\n1.0\n2.0\n48.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4296\n7.0\n36.0\n1.0\n6.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n8.0\n1.0\n2.0\n2.0\n4.0\n2.0\n1.0\n1.0\n25.0\n\n\n2763\n5.0\n34.0\n1.0\n6.0\n1.0\n2.0\n1.0\n2.0\n4.0\n2.0\n3.0\n3.0\n2.0\n2.0\n2.0\n2.0\n3.0\n3.0\n6.0\n4.0\n7.0\n1.0\n1.0\n2.0\n3.0\n2.0\n1.0\n1.0\n34.0\n\n\n3593\n6.0\n36.0\n3.0\n7.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n8.0\n2.0\n4.0\n2.0\n4.0\n3.0\n3.0\n1.0\n58.0\n\n\n3277\n5.0\n34.0\n1.0\n7.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n8.0\n1.0\n2.0\n1.0\n5.0\n3.0\n2.0\n1.0\n42.0\n\n\n5389\n9.0\n34.0\n1.0\n4.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n2.0\n4.0\n8.0\n2.0\n3.0\n1.0\n5.0\n4.0\n1.0\n1.0\n39.0\n\n\n\n\n4532 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\nPerforming cross-validation...\n[0] train-auc:0.72904+0.00717   train-error:0.13923+0.00246 test-auc:0.63914+0.03029    test-error:0.13923+0.00984\n[1] train-auc:0.74121+0.00650   train-error:0.13923+0.00246 test-auc:0.65050+0.03281    test-error:0.13923+0.00984\n[2] train-auc:0.74980+0.00536   train-error:0.13923+0.00246 test-auc:0.65789+0.03393    test-error:0.13923+0.00984\n[3] train-auc:0.75701+0.00727   train-error:0.13923+0.00246 test-auc:0.65717+0.03223    test-error:0.13923+0.00984\n[4] train-auc:0.76338+0.00684   train-error:0.13923+0.00246 test-auc:0.66330+0.03274    test-error:0.13923+0.00984\n[5] train-auc:0.76952+0.00707   train-error:0.13923+0.00246 test-auc:0.66412+0.03152    test-error:0.13923+0.00984\n[6] train-auc:0.77329+0.00713   train-error:0.13912+0.00241 test-auc:0.66570+0.02975    test-error:0.13923+0.00984\n[7] train-auc:0.77670+0.00816   train-error:0.13885+0.00223 test-auc:0.66564+0.02755    test-error:0.13923+0.00984\n[8] train-auc:0.78105+0.00848   train-error:0.13857+0.00220 test-auc:0.66675+0.02709    test-error:0.13945+0.00967\n[9] train-auc:0.78542+0.00873   train-error:0.13813+0.00239 test-auc:0.66726+0.02838    test-error:0.13945+0.00967\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:35] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n[10]    train-auc:0.79012+0.00617   train-error:0.13758+0.00225 test-auc:0.66581+0.02624    test-error:0.13945+0.00967\n[11]    train-auc:0.79638+0.00740   train-error:0.13692+0.00228 test-auc:0.66725+0.02750    test-error:0.13945+0.00967\n[12]    train-auc:0.79997+0.00864   train-error:0.13653+0.00261 test-auc:0.66868+0.02840    test-error:0.13967+0.00952\n[13]    train-auc:0.80394+0.00838   train-error:0.13609+0.00258 test-auc:0.66558+0.02804    test-error:0.13967+0.00918\n[14]    train-auc:0.80669+0.00852   train-error:0.13576+0.00253 test-auc:0.66478+0.02999    test-error:0.13967+0.00918\n[15]    train-auc:0.81104+0.00836   train-error:0.13537+0.00250 test-auc:0.66511+0.03062    test-error:0.13945+0.00936\n[16]    train-auc:0.81227+0.00863   train-error:0.13482+0.00277 test-auc:0.66637+0.03134    test-error:0.13923+0.00946\n[17]    train-auc:0.81605+0.00882   train-error:0.13438+0.00338 test-auc:0.66839+0.03072    test-error:0.13945+0.00931\n[18]    train-auc:0.81879+0.00957   train-error:0.13361+0.00334 test-auc:0.66744+0.02975    test-error:0.13967+0.00858\n[19]    train-auc:0.82114+0.01037   train-error:0.13305+0.00338 test-auc:0.66828+0.03086    test-error:0.13945+0.00933\n[20]    train-auc:0.82445+0.00874   train-error:0.13245+0.00387 test-auc:0.66960+0.02902    test-error:0.13901+0.00908\n[21]    train-auc:0.82624+0.00880   train-error:0.13162+0.00414 test-auc:0.66994+0.02746    test-error:0.13879+0.00901\n[22]    train-auc:0.82979+0.00980   train-error:0.13112+0.00404 test-auc:0.66941+0.02966    test-error:0.13879+0.00880\n[23]    train-auc:0.83073+0.00946   train-error:0.13041+0.00371 test-auc:0.67039+0.02904    test-error:0.13857+0.00841\n[24]    train-auc:0.83414+0.01029   train-error:0.12991+0.00358 test-auc:0.66796+0.03002    test-error:0.13857+0.00841\n[25]    train-auc:0.83735+0.01163   train-error:0.12925+0.00328 test-auc:0.66852+0.02923    test-error:0.13857+0.00841\n[26]    train-auc:0.84148+0.01130   train-error:0.12870+0.00330 test-auc:0.66800+0.03058    test-error:0.13812+0.00874\n[27]    train-auc:0.84529+0.01061   train-error:0.12836+0.00327 test-auc:0.66701+0.02953    test-error:0.13790+0.00887\n[28]    train-auc:0.84720+0.01062   train-error:0.12770+0.00327 test-auc:0.66731+0.02853    test-error:0.13812+0.00874\n[29]    train-auc:0.84962+0.01130   train-error:0.12715+0.00349 test-auc:0.66684+0.02826    test-error:0.13813+0.00861\n[30]    train-auc:0.85275+0.01050   train-error:0.12610+0.00355 test-auc:0.66614+0.02878    test-error:0.13813+0.00861\n[31]    train-auc:0.85416+0.01075   train-error:0.12572+0.00319 test-auc:0.66590+0.02810    test-error:0.13790+0.00871\n[32]    train-auc:0.85600+0.01087   train-error:0.12483+0.00317 test-auc:0.66428+0.02854    test-error:0.13813+0.00861\n[33]    train-auc:0.85837+0.01135   train-error:0.12450+0.00339 test-auc:0.66299+0.02943    test-error:0.13790+0.00871\n[34]    train-auc:0.86169+0.00901   train-error:0.12401+0.00302 test-auc:0.66171+0.02896    test-error:0.13790+0.00851\n[35]    train-auc:0.86323+0.00908   train-error:0.12346+0.00322 test-auc:0.66093+0.02910    test-error:0.13835+0.00821\n[36]    train-auc:0.86474+0.00881   train-error:0.12329+0.00319 test-auc:0.66113+0.02873    test-error:0.13857+0.00810\n[37]    train-auc:0.86716+0.00913   train-error:0.12257+0.00315 test-auc:0.66113+0.02934    test-error:0.13857+0.00810\n[38]    train-auc:0.86932+0.00907   train-error:0.12202+0.00332 test-auc:0.66081+0.02915    test-error:0.13835+0.00824\n[39]    train-auc:0.87094+0.00852   train-error:0.12180+0.00307 test-auc:0.66066+0.03035    test-error:0.13835+0.00824\n[40]    train-auc:0.87232+0.00831   train-error:0.12119+0.00273 test-auc:0.65967+0.02967    test-error:0.13857+0.00810\n[41]    train-auc:0.87502+0.00781   train-error:0.12048+0.00315 test-auc:0.65826+0.02838    test-error:0.13857+0.00810\n[42]    train-auc:0.87668+0.00808   train-error:0.11981+0.00271 test-auc:0.65678+0.02758    test-error:0.13879+0.00809\n[43]    train-auc:0.87866+0.00812   train-error:0.11970+0.00257 test-auc:0.65662+0.02838    test-error:0.13901+0.00810\n[44]    train-auc:0.88010+0.00810   train-error:0.11904+0.00251 test-auc:0.65730+0.02788    test-error:0.13901+0.00810\n[45]    train-auc:0.88106+0.00816   train-error:0.11860+0.00274 test-auc:0.65704+0.02792    test-error:0.13923+0.00852\n[46]    train-auc:0.88228+0.00824   train-error:0.11766+0.00298 test-auc:0.65611+0.02854    test-error:0.13945+0.00841\n\n\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate and print metrics\nprint(\"\\nModel Performance Metrics:\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plot feature importances\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.show()\n\n\nCross-validation results:\nBest AUC: 0.6704 (+/- 0.0262)\nBest Error: 0.1379 (+/- 0.0084)\n\nTraining final model...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:07:14] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nModel Performance Metrics:\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.99      0.92       975\n           1       0.31      0.03      0.06       158\n\n    accuracy                           0.86      1133\n   macro avg       0.59      0.51      0.49      1133\nweighted avg       0.79      0.86      0.80      1133\n\n\nConfusion Matrix:\n[[964  11]\n [153   5]]\n\nROC AUC Score: 0.6858\n\nTop 10 Most Important Features:\n         Feature  Importance\n28           AGE       332.0\n0          CYCLE       138.0\n7       KINDWORK       136.0\n20        INCOME       100.0\n3          EEDUC        96.0\n18      LIVQTRRV        82.0\n25      SUPPORT1        77.0\n27      SUPPORT3        76.0\n24       SOCIAL2        75.0\n8      EXPNS_DIF        72.0\n5   EGENID_BIRTH        71.0\n22   PRICECONCRN        63.0\n9     CURFOODSUF        57.0\n4             MS        56.0\n19        ENERGY        56.0\n23       SOCIAL1        55.0\n26      SUPPORT2        54.0\n10       ANXIOUS        53.0\n15        SEEING        50.0\n21     PRICECHNG        44.0\n1         EST_ST        38.0\n14       PUBHLTH        37.0\n6     HADCOVIDRV        36.0\n13          DOWN        35.0\n16   REMEMBERING        33.0\n2          RRACE        31.0\n17        TENURE        30.0\n11         WORRY        29.0\n12      INTEREST        17.0\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# Add constant for statsmodels\nX_train_probit = sm.add_constant(X_train_final)\n\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit(disp=0)  # disp=0 -&gt; no iteration details\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# If you want to see marginal effects (average partial effects), uncomment below:\nme = probit_results.get_margeff()\nprint(\"\\nMarginal Effects:\")\nprint(me.summary())\n\n\nFitting Probit model for effect sizes and p-values...\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:            TWDAYS_RESP   No. Observations:                 4532\nModel:                         Probit   Df Residuals:                     4502\nMethod:                           MLE   Df Model:                           29\nDate:                Fri, 31 Jan 2025   Pseudo R-squ.:                 0.05405\nTime:                        04:00:44   Log-Likelihood:                -1730.1\nconverged:                       True   LL-Null:                       -1829.0\nCovariance Type:            nonrobust   LLR p-value:                 4.968e-27\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           -1.6437      1.003     -1.639      0.101      -3.609       0.322\nCYCLE           -0.0102      0.010     -1.053      0.293      -0.029       0.009\nEST_ST           0.0590      0.026      2.302      0.021       0.009       0.109\nRRACE           -0.0138      0.029     -0.475      0.635      -0.071       0.043\nEEDUC           -0.1432      0.023     -6.185      0.000      -0.189      -0.098\nMS              -0.0579      0.017     -3.412      0.001      -0.091      -0.025\nEGENID_BIRTH     0.0153      0.050      0.307      0.759      -0.082       0.113\nHADCOVIDRV      -0.0206      0.057     -0.362      0.717      -0.132       0.091\nKINDWORK        -0.0680      0.029     -2.381      0.017      -0.124      -0.012\nEXPNS_DIF        0.0525      0.037      1.434      0.152      -0.019       0.124\nCURFOODSUF       0.0481      0.060      0.799      0.425      -0.070       0.166\nANXIOUS         -0.1246      0.050     -2.516      0.012      -0.222      -0.028\nWORRY            0.0650      0.053      1.220      0.222      -0.039       0.169\nINTEREST        -0.0053      0.055     -0.097      0.923      -0.114       0.103\nDOWN             0.1047      0.059      1.767      0.077      -0.011       0.221\nPUBHLTH         -0.0555      0.053     -1.043      0.297      -0.160       0.049\nSEEING           0.0497      0.055      0.901      0.368      -0.058       0.158\nREMEMBERING     -0.0178      0.058     -0.307      0.759      -0.131       0.096\nTENURE           0.0076      0.046      0.165      0.869      -0.083       0.098\nLIVQTRRV        -0.0896      0.018     -4.977      0.000      -0.125      -0.054\nENERGY           0.0316      0.039      0.808      0.419      -0.045       0.108\nINCOME          -0.0499      0.018     -2.846      0.004      -0.084      -0.016\nPRICECHNG       -0.0007      0.031     -0.021      0.983      -0.062       0.061\nPRICECONCRN      0.0165      0.030      0.553      0.580      -0.042       0.075\nSOCIAL1         -0.0213      0.027     -0.775      0.438      -0.075       0.032\nSOCIAL2          0.1105      0.033      3.343      0.001       0.046       0.175\nSUPPORT1        -0.0149      0.024     -0.616      0.538      -0.062       0.032\nSUPPORT2        -0.0592      0.032     -1.824      0.068      -0.123       0.004\nSUPPORT3         0.0615      0.022      2.771      0.006       0.018       0.105\nAGE             -0.0058      0.002     -2.494      0.013      -0.010      -0.001\n================================================================================\n\nMarginal Effects:\n       Probit Marginal Effects       \n=====================================\nDep. Variable:            TWDAYS_RESP\nMethod:                          dydx\nAt:                           overall\n================================================================================\n                  dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nCYCLE           -0.0021      0.002     -1.053      0.292      -0.006       0.002\nEST_ST           0.0124      0.005      2.303      0.021       0.002       0.023\nRRACE           -0.0029      0.006     -0.475      0.635      -0.015       0.009\nEEDUC           -0.0301      0.005     -6.217      0.000      -0.040      -0.021\nMS              -0.0122      0.004     -3.416      0.001      -0.019      -0.005\nEGENID_BIRTH     0.0032      0.010      0.307      0.759      -0.017       0.024\nHADCOVIDRV      -0.0043      0.012     -0.362      0.717      -0.028       0.019\nKINDWORK        -0.0143      0.006     -2.383      0.017      -0.026      -0.003\nEXPNS_DIF        0.0110      0.008      1.434      0.151      -0.004       0.026\nCURFOODSUF       0.0101      0.013      0.799      0.425      -0.015       0.035\nANXIOUS         -0.0262      0.010     -2.518      0.012      -0.047      -0.006\nWORRY            0.0136      0.011      1.221      0.222      -0.008       0.036\nINTEREST        -0.0011      0.012     -0.097      0.923      -0.024       0.022\nDOWN             0.0220      0.012      1.767      0.077      -0.002       0.046\nPUBHLTH         -0.0117      0.011     -1.043      0.297      -0.034       0.010\nSEEING           0.0104      0.012      0.901      0.368      -0.012       0.033\nREMEMBERING     -0.0037      0.012     -0.307      0.759      -0.028       0.020\nTENURE           0.0016      0.010      0.165      0.869      -0.017       0.021\nLIVQTRRV        -0.0188      0.004     -4.987      0.000      -0.026      -0.011\nENERGY           0.0066      0.008      0.808      0.419      -0.009       0.023\nINCOME          -0.0105      0.004     -2.847      0.004      -0.018      -0.003\nPRICECHNG       -0.0001      0.007     -0.021      0.983      -0.013       0.013\nPRICECONCRN      0.0035      0.006      0.553      0.580      -0.009       0.016\nSOCIAL1         -0.0045      0.006     -0.775      0.438      -0.016       0.007\nSOCIAL2          0.0232      0.007      3.348      0.001       0.010       0.037\nSUPPORT1        -0.0031      0.005     -0.616      0.538      -0.013       0.007\nSUPPORT2        -0.0124      0.007     -1.825      0.068      -0.026       0.001\nSUPPORT3         0.0129      0.005      2.773      0.006       0.004       0.022\nAGE             -0.0012      0.000     -2.495      0.013      -0.002      -0.000\n================================================================================\n\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )\n\n\nCalculating and plotting SHAP values for XGBoost model...\nGenerating SHAP Beeswarm plot...\n\n\n\n\n\n\n\n\n\nGenerating SHAP Bar plot...\n\n\n\n\n\n\n\n\n\n\nCPS Binary Classification Model\n\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Load and prepare data\nfiltered_df = pd.read_csv(\"filtered_df.csv\")\nfiltered_df = filtered_df[(filtered_df['gestfips']==34) | (filtered_df['gestfips']==36)]\n\n\nX = filtered_df[['hehousut','hetelhhd','hetelavl','hefaminc','hrnumhou','hrhtype','HUBUS','gereg','gediv','gestfips','gtcbsa','gtco','gtcbsast','gtmetsta','gtindvpc','gtcbsasz','gtcsa','perrp','prtage','pemaritl','pesex','peeduca','ptdtrace','prdthsp','PUCHINHH','prfamrel','prfamtyp','pehspnon','penatvty','pemntvty','pefntvty','prcitshp','prinuyer','PUWK','pemjot','pemjnum','pehruslt','pehractt','peio1cow','prdtind1','prdtocc1','pternwa','ptwk','prchld','prnmchld']]\ny = filtered_df['pttlwk']\n\n\nX\n\n\n  \n    \n\n\n\n\n\n\nhehousut\nhetelhhd\nhetelavl\nhefaminc\nhrnumhou\nhrhtype\nHUBUS\ngereg\ngediv\ngestfips\n...\npemjnum\npehruslt\npehractt\npeio1cow\nprdtind1\nprdtocc1\npternwa\nptwk\nprchld\nprnmchld\n\n\n\n\n651\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n34\n...\n-1.0\n40.0\n48.0\n4.0\n25.0\n21.0\n-1.0\n0\n0.0\n0.0\n\n\n652\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n2.0\n51.0\n14.0\n-1.0\n0\n0.0\n0.0\n\n\n653\n1\n1\n-1\n11\n2\n6\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n14.0\n22.0\n-1.0\n0\n0.0\n0.0\n\n\n654\n1\n1\n-1\n11\n2\n6\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n22.0\n17.0\n-1.0\n0\n0.0\n0.0\n\n\n655\n1\n2\n2\n12\n2\n4\n2\n1\n2\n34\n...\n-1.0\n20.0\n20.0\n7.0\n36.0\n7.0\n-1.0\n0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n318282\n1\n1\n2\n10\n2\n4\n2\n1\n2\n36\n...\n-1.0\n40.0\n40.0\n4.0\n42.0\n11.0\n-1.0\n0\n0.0\n0.0\n\n\n318283\n1\n1\n-1\n15\n1\n6\n2\n1\n2\n36\n...\n-1.0\n40.0\n40.0\n1.0\n51.0\n20.0\n-1.0\n0\n0.0\n0.0\n\n\n318284\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n36\n...\n-1.0\n50.0\n50.0\n4.0\n40.0\n13.0\n-1.0\n0\n0.0\n0.0\n\n\n318399\n1\n1\n-1\n16\n3\n4\n2\n1\n2\n34\n...\n-1.0\n20.0\n20.0\n4.0\n22.0\n16.0\n-1.0\n0\n0.0\n0.0\n\n\n318400\n1\n1\n-1\n16\n3\n4\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n38.0\n22.0\n-1.0\n0\n4.0\n1.0\n\n\n\n\n17819 rows × 45 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert labels from 1.0/2.0 to 0/1\ny = (y == 2.0).astype(int)\n\n\ny.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\npttlwk\n\n\n\n\n\n1\n13529\n\n\n0\n4290\n\n\n\n\ndtype: int64\n\n\n\n# # Initialize label encoders dictionary\n# encoders = {}\n\n# # Label encode all feature columns\n# for column in X.columns:\n#     encoders[column] = LabelEncoder()\n#     X[column] = encoders[column].fit_transform(X[column])\n\n# # Label encode target variable\n# target_encoder = LabelEncoder()\n# y = target_encoder.fit_transform(y)\n\n# print(\"Shape of X after encoding:\", X.shape)\n# print(\"Shape of y after encoding:\", y.shape)\n# print(\"\\nUnique values in target variable:\", np.unique(y))\n\n\n# print(\"Performing initial feature selection...\")\n# selector = SelectKBest(mutual_info_classif, k=100)  # Keep top 50 features\n# X_reduced = selector.fit_transform(X, y)\n# selected_features = X.columns[selector.get_support(indices=True)]\n# print(\"Top 50 features selected.\")\n\n\n# Create a DataFrame with reduced features for VIF and further steps\n# X = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n  return 1 - self.ssr/self.centered_tss\n\n\nVIF Results:\n     Feature        VIF\n0   hehousut   1.046027\n1   hetelhhd   2.254292\n2   hetelavl   2.282600\n3   hefaminc   1.434420\n4   hrnumhou   2.452866\n5    hrhtype   4.732866\n6      HUBUS   1.156651\n7      gereg   0.000000\n8      gediv   0.000000\n9   gestfips   2.652757\n10    gtcbsa   2.183617\n11      gtco   2.202980\n12  gtcbsast   4.391335\n13  gtmetsta   2.231556\n14  gtindvpc   2.651372\n15  gtcbsasz   9.794757\n16     gtcsa   5.458005\n17     perrp   5.387685\n18    prtage   1.678536\n19  pemaritl   3.464919\n20     pesex   1.148676\n21   peeduca   1.524835\n22  ptdtrace   1.271114\n23   prdthsp   5.773714\n24  PUCHINHH   1.015834\n25  prfamrel   6.719113\n26  prfamtyp   4.662360\n27  pehspnon   5.659180\n28  penatvty   7.903467\n29  pemntvty   8.318010\n30  pefntvty   7.946751\n31  prcitshp  12.204819\n32  prinuyer   8.446316\n33      PUWK   1.015886\n34    pemjot  83.410023\n35   pemjnum  83.510317\n36  pehruslt   2.022003\n37  pehractt   2.050188\n38  peio1cow   1.171097\n39  prdtind1   1.193875\n40  prdtocc1   1.359964\n41   pternwa   3.260463\n42      ptwk   3.171225\n43    prchld   4.825561\n44  prnmchld   5.667953\nDropping features with high VIF: ['prcitshp', 'pemjot', 'pemjnum']\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\nPerforming Recursive Feature Elimination...\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# # Align indices before filtering\n# X_train_final = X_train_final[non_outliers].reset_index(drop=True)\n# # Use the index from X_train_final before reset to filter y_train\n# y_train = y_train[X_train_final.index[non_outliers]].reset_index(drop=True)\n\n\nX_train_final\n\n\n  \n    \n\n\n\n\n\n\nhefaminc\nhrnumhou\nhrhtype\nHUBUS\ngestfips\ngtcbsa\ngtco\ngtcbsast\ngtindvpc\ngtcbsasz\n...\npefntvty\nprinuyer\npehruslt\npehractt\npeio1cow\nprdtind1\nprdtocc1\npternwa\nprchld\nprnmchld\n\n\n\n\n143322\n9\n2\n6\n2\n36\n15380\n0\n1\n1\n5\n...\n57.0\n0.0\n30.0\n30.0\n4.0\n46.0\n13.0\n-1.0\n0.0\n0.0\n\n\n318280\n15\n2\n1\n2\n36\n10580\n0\n1\n0\n4\n...\n117.0\n0.0\n40.0\n40.0\n4.0\n19.0\n22.0\n-1.0\n0.0\n0.0\n\n\n316593\n16\n9\n1\n2\n36\n35620\n81\n1\n1\n7\n...\n333.0\n0.0\n25.0\n25.0\n1.0\n51.0\n12.0\n-1.0\n0.0\n0.0\n\n\n78397\n11\n4\n1\n2\n34\n0\n0\n4\n0\n0\n...\n57.0\n0.0\n40.0\n40.0\n3.0\n40.0\n22.0\n-1.0\n0.0\n0.0\n\n\n92298\n15\n2\n1\n2\n34\n45940\n21\n4\n0\n3\n...\n57.0\n0.0\n40.0\n1.0\n4.0\n19.0\n17.0\n-1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n143188\n16\n4\n1\n2\n34\n35620\n13\n2\n0\n7\n...\n57.0\n0.0\n40.0\n40.0\n1.0\n51.0\n7.0\n-1.0\n5.0\n2.0\n\n\n164531\n11\n5\n3\n2\n36\n35620\n5\n1\n1\n7\n...\n328.0\n24.0\n25.0\n25.0\n3.0\n51.0\n12.0\n-1.0\n0.0\n0.0\n\n\n44316\n13\n1\n6\n2\n36\n0\n0\n3\n0\n0\n...\n57.0\n0.0\n40.0\n40.0\n4.0\n42.0\n21.0\n72000.0\n0.0\n0.0\n\n\n297427\n12\n3\n1\n2\n34\n35620\n13\n2\n0\n7\n...\n370.0\n28.0\n60.0\n60.0\n7.0\n36.0\n9.0\n-1.0\n3.0\n1.0\n\n\n36780\n16\n2\n1\n2\n36\n15380\n0\n2\n0\n5\n...\n57.0\n0.0\n20.0\n20.0\n4.0\n29.0\n20.0\n-1.0\n0.0\n0.0\n\n\n\n\n14255 rows × 30 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\nPerforming cross-validation...\n[0] train-auc:0.84550+0.00162   train-error:0.24076+0.00117 test-auc:0.83084+0.00405    test-error:0.24076+0.00468\n[1] train-auc:0.85108+0.00091   train-error:0.24076+0.00117 test-auc:0.83766+0.00555    test-error:0.24076+0.00468\n[2] train-auc:0.85450+0.00175   train-error:0.24076+0.00117 test-auc:0.84107+0.00493    test-error:0.24076+0.00468\n[3] train-auc:0.85600+0.00190   train-error:0.24076+0.00117 test-auc:0.84246+0.00456    test-error:0.24076+0.00468\n[4] train-auc:0.85822+0.00136   train-error:0.24076+0.00117 test-auc:0.84453+0.00454    test-error:0.24076+0.00468\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:33:21] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n[5] train-auc:0.86179+0.00331   train-error:0.24046+0.00104 test-auc:0.84622+0.00409    test-error:0.24062+0.00477\n[6] train-auc:0.86704+0.00245   train-error:0.22038+0.01036 test-auc:0.85064+0.00415    test-error:0.22448+0.00933\n[7] train-auc:0.86990+0.00301   train-error:0.20051+0.00366 test-auc:0.85308+0.00421    test-error:0.20744+0.00496\n[8] train-auc:0.87272+0.00163   train-error:0.18886+0.00164 test-auc:0.85495+0.00438    test-error:0.19551+0.00420\n[9] train-auc:0.87495+0.00115   train-error:0.18346+0.00179 test-auc:0.85689+0.00411    test-error:0.19074+0.00270\n[10]    train-auc:0.87717+0.00113   train-error:0.18143+0.00199 test-auc:0.85855+0.00410    test-error:0.18871+0.00339\n[11]    train-auc:0.87898+0.00138   train-error:0.17866+0.00137 test-auc:0.86038+0.00398    test-error:0.18702+0.00132\n[12]    train-auc:0.88076+0.00112   train-error:0.17608+0.00184 test-auc:0.86169+0.00396    test-error:0.18471+0.00357\n[13]    train-auc:0.88210+0.00097   train-error:0.17420+0.00198 test-auc:0.86243+0.00393    test-error:0.18302+0.00244\n[14]    train-auc:0.88343+0.00097   train-error:0.17220+0.00197 test-auc:0.86327+0.00391    test-error:0.18281+0.00254\n[15]    train-auc:0.88504+0.00105   train-error:0.17092+0.00188 test-auc:0.86466+0.00360    test-error:0.18141+0.00310\n[16]    train-auc:0.88660+0.00101   train-error:0.16984+0.00225 test-auc:0.86528+0.00350    test-error:0.18064+0.00284\n[17]    train-auc:0.88781+0.00117   train-error:0.16819+0.00211 test-auc:0.86620+0.00336    test-error:0.18085+0.00317\n[18]    train-auc:0.88933+0.00129   train-error:0.16654+0.00178 test-auc:0.86720+0.00352    test-error:0.17994+0.00236\n[19]    train-auc:0.89063+0.00128   train-error:0.16519+0.00214 test-auc:0.86812+0.00367    test-error:0.17952+0.00156\n[20]    train-auc:0.89191+0.00111   train-error:0.16373+0.00224 test-auc:0.86901+0.00388    test-error:0.17790+0.00179\n[21]    train-auc:0.89321+0.00127   train-error:0.16242+0.00249 test-auc:0.86994+0.00382    test-error:0.17608+0.00234\n[22]    train-auc:0.89441+0.00090   train-error:0.16122+0.00216 test-auc:0.87096+0.00396    test-error:0.17454+0.00127\n[23]    train-auc:0.89540+0.00084   train-error:0.15982+0.00222 test-auc:0.87171+0.00389    test-error:0.17355+0.00153\n[24]    train-auc:0.89649+0.00108   train-error:0.15805+0.00215 test-auc:0.87229+0.00390    test-error:0.17257+0.00222\n[25]    train-auc:0.89755+0.00107   train-error:0.15724+0.00207 test-auc:0.87280+0.00417    test-error:0.17264+0.00185\n[26]    train-auc:0.89841+0.00106   train-error:0.15617+0.00233 test-auc:0.87343+0.00418    test-error:0.17166+0.00228\n[27]    train-auc:0.89925+0.00103   train-error:0.15582+0.00247 test-auc:0.87383+0.00411    test-error:0.17089+0.00294\n[28]    train-auc:0.89986+0.00095   train-error:0.15498+0.00219 test-auc:0.87411+0.00405    test-error:0.17096+0.00324\n[29]    train-auc:0.90054+0.00077   train-error:0.15479+0.00231 test-auc:0.87449+0.00400    test-error:0.17054+0.00290\n[30]    train-auc:0.90109+0.00080   train-error:0.15423+0.00193 test-auc:0.87485+0.00391    test-error:0.16976+0.00328\n[31]    train-auc:0.90180+0.00098   train-error:0.15370+0.00153 test-auc:0.87537+0.00397    test-error:0.17005+0.00264\n[32]    train-auc:0.90244+0.00104   train-error:0.15303+0.00169 test-auc:0.87572+0.00379    test-error:0.17026+0.00178\n[33]    train-auc:0.90308+0.00094   train-error:0.15209+0.00183 test-auc:0.87614+0.00382    test-error:0.16955+0.00313\n[34]    train-auc:0.90397+0.00126   train-error:0.15137+0.00201 test-auc:0.87652+0.00370    test-error:0.16906+0.00203\n[35]    train-auc:0.90459+0.00148   train-error:0.15021+0.00207 test-auc:0.87698+0.00376    test-error:0.16794+0.00238\n[36]    train-auc:0.90521+0.00113   train-error:0.14951+0.00167 test-auc:0.87729+0.00403    test-error:0.16836+0.00251\n[37]    train-auc:0.90611+0.00129   train-error:0.14865+0.00175 test-auc:0.87794+0.00388    test-error:0.16829+0.00231\n[38]    train-auc:0.90679+0.00124   train-error:0.14793+0.00162 test-auc:0.87813+0.00384    test-error:0.16773+0.00213\n[39]    train-auc:0.90777+0.00116   train-error:0.14681+0.00206 test-auc:0.87855+0.00389    test-error:0.16689+0.00238\n[40]    train-auc:0.90814+0.00126   train-error:0.14646+0.00206 test-auc:0.87881+0.00389    test-error:0.16626+0.00218\n[41]    train-auc:0.90889+0.00155   train-error:0.14590+0.00229 test-auc:0.87915+0.00367    test-error:0.16619+0.00193\n[42]    train-auc:0.90923+0.00147   train-error:0.14581+0.00214 test-auc:0.87937+0.00376    test-error:0.16626+0.00290\n[43]    train-auc:0.91005+0.00140   train-error:0.14562+0.00221 test-auc:0.87985+0.00373    test-error:0.16535+0.00228\n[44]    train-auc:0.91060+0.00165   train-error:0.14507+0.00241 test-auc:0.88012+0.00353    test-error:0.16499+0.00316\n[45]    train-auc:0.91126+0.00124   train-error:0.14411+0.00181 test-auc:0.88032+0.00366    test-error:0.16478+0.00338\n[46]    train-auc:0.91179+0.00127   train-error:0.14318+0.00208 test-auc:0.88063+0.00373    test-error:0.16436+0.00293\n[47]    train-auc:0.91219+0.00126   train-error:0.14297+0.00178 test-auc:0.88077+0.00366    test-error:0.16415+0.00310\n[48]    train-auc:0.91255+0.00120   train-error:0.14274+0.00142 test-auc:0.88100+0.00367    test-error:0.16373+0.00295\n[49]    train-auc:0.91312+0.00143   train-error:0.14228+0.00146 test-auc:0.88124+0.00372    test-error:0.16352+0.00332\n[50]    train-auc:0.91370+0.00138   train-error:0.14179+0.00166 test-auc:0.88155+0.00392    test-error:0.16401+0.00376\n[51]    train-auc:0.91428+0.00138   train-error:0.14106+0.00132 test-auc:0.88190+0.00387    test-error:0.16345+0.00382\n[52]    train-auc:0.91454+0.00131   train-error:0.14081+0.00148 test-auc:0.88203+0.00388    test-error:0.16303+0.00330\n[53]    train-auc:0.91490+0.00138   train-error:0.14093+0.00146 test-auc:0.88219+0.00393    test-error:0.16310+0.00358\n[54]    train-auc:0.91560+0.00137   train-error:0.14025+0.00157 test-auc:0.88252+0.00398    test-error:0.16233+0.00383\n[55]    train-auc:0.91624+0.00121   train-error:0.13964+0.00160 test-auc:0.88279+0.00404    test-error:0.16247+0.00426\n[56]    train-auc:0.91667+0.00102   train-error:0.13934+0.00142 test-auc:0.88295+0.00409    test-error:0.16226+0.00444\n[57]    train-auc:0.91727+0.00089   train-error:0.13885+0.00166 test-auc:0.88305+0.00415    test-error:0.16212+0.00437\n[58]    train-auc:0.91782+0.00097   train-error:0.13837+0.00158 test-auc:0.88327+0.00409    test-error:0.16191+0.00414\n[59]    train-auc:0.91839+0.00102   train-error:0.13820+0.00180 test-auc:0.88357+0.00392    test-error:0.16135+0.00434\n[60]    train-auc:0.91892+0.00119   train-error:0.13799+0.00179 test-auc:0.88376+0.00386    test-error:0.16135+0.00373\n[61]    train-auc:0.91948+0.00100   train-error:0.13707+0.00138 test-auc:0.88398+0.00393    test-error:0.16170+0.00351\n[62]    train-auc:0.91976+0.00105   train-error:0.13686+0.00150 test-auc:0.88413+0.00391    test-error:0.16135+0.00371\n[63]    train-auc:0.92011+0.00101   train-error:0.13688+0.00147 test-auc:0.88425+0.00384    test-error:0.16107+0.00405\n[64]    train-auc:0.92049+0.00096   train-error:0.13625+0.00195 test-auc:0.88441+0.00384    test-error:0.16093+0.00393\n[65]    train-auc:0.92116+0.00111   train-error:0.13515+0.00195 test-auc:0.88482+0.00391    test-error:0.16043+0.00405\n[66]    train-auc:0.92142+0.00109   train-error:0.13515+0.00200 test-auc:0.88494+0.00388    test-error:0.16036+0.00346\n[67]    train-auc:0.92189+0.00098   train-error:0.13462+0.00145 test-auc:0.88500+0.00398    test-error:0.16058+0.00312\n[68]    train-auc:0.92222+0.00093   train-error:0.13441+0.00159 test-auc:0.88515+0.00404    test-error:0.15994+0.00343\n[69]    train-auc:0.92261+0.00099   train-error:0.13427+0.00156 test-auc:0.88534+0.00390    test-error:0.15952+0.00294\n[70]    train-auc:0.92307+0.00098   train-error:0.13369+0.00142 test-auc:0.88556+0.00374    test-error:0.15938+0.00370\n[71]    train-auc:0.92326+0.00094   train-error:0.13350+0.00150 test-auc:0.88563+0.00379    test-error:0.15910+0.00379\n[72]    train-auc:0.92360+0.00107   train-error:0.13336+0.00175 test-auc:0.88580+0.00368    test-error:0.15896+0.00353\n[73]    train-auc:0.92397+0.00106   train-error:0.13302+0.00189 test-auc:0.88590+0.00364    test-error:0.15875+0.00353\n[74]    train-auc:0.92440+0.00094   train-error:0.13269+0.00158 test-auc:0.88608+0.00381    test-error:0.15875+0.00372\n[75]    train-auc:0.92480+0.00085   train-error:0.13260+0.00128 test-auc:0.88610+0.00374    test-error:0.15854+0.00379\n[76]    train-auc:0.92521+0.00081   train-error:0.13178+0.00091 test-auc:0.88624+0.00354    test-error:0.15826+0.00320\n[77]    train-auc:0.92546+0.00085   train-error:0.13166+0.00107 test-auc:0.88640+0.00357    test-error:0.15854+0.00322\n[78]    train-auc:0.92572+0.00086   train-error:0.13153+0.00111 test-auc:0.88647+0.00350    test-error:0.15861+0.00319\n[79]    train-auc:0.92608+0.00096   train-error:0.13127+0.00140 test-auc:0.88659+0.00346    test-error:0.15861+0.00337\n[80]    train-auc:0.92632+0.00089   train-error:0.13129+0.00167 test-auc:0.88657+0.00355    test-error:0.15805+0.00395\n[81]    train-auc:0.92655+0.00102   train-error:0.13113+0.00173 test-auc:0.88667+0.00344    test-error:0.15812+0.00375\n[82]    train-auc:0.92680+0.00105   train-error:0.13064+0.00193 test-auc:0.88679+0.00341    test-error:0.15854+0.00440\n[83]    train-auc:0.92715+0.00113   train-error:0.13041+0.00216 test-auc:0.88689+0.00344    test-error:0.15833+0.00357\n[84]    train-auc:0.92752+0.00122   train-error:0.12966+0.00201 test-auc:0.88708+0.00349    test-error:0.15791+0.00325\n[85]    train-auc:0.92778+0.00119   train-error:0.12952+0.00188 test-auc:0.88720+0.00356    test-error:0.15833+0.00332\n[86]    train-auc:0.92810+0.00139   train-error:0.12918+0.00232 test-auc:0.88729+0.00366    test-error:0.15805+0.00358\n[87]    train-auc:0.92849+0.00158   train-error:0.12857+0.00272 test-auc:0.88746+0.00353    test-error:0.15805+0.00347\n[88]    train-auc:0.92877+0.00170   train-error:0.12848+0.00293 test-auc:0.88760+0.00348    test-error:0.15784+0.00336\n[89]    train-auc:0.92925+0.00168   train-error:0.12822+0.00280 test-auc:0.88770+0.00357    test-error:0.15805+0.00365\n[90]    train-auc:0.92955+0.00162   train-error:0.12764+0.00314 test-auc:0.88780+0.00361    test-error:0.15749+0.00369\n[91]    train-auc:0.92986+0.00176   train-error:0.12732+0.00306 test-auc:0.88789+0.00363    test-error:0.15728+0.00408\n[92]    train-auc:0.93029+0.00168   train-error:0.12692+0.00266 test-auc:0.88805+0.00364    test-error:0.15763+0.00388\n[93]    train-auc:0.93049+0.00166   train-error:0.12673+0.00269 test-auc:0.88812+0.00359    test-error:0.15784+0.00354\n[94]    train-auc:0.93089+0.00157   train-error:0.12632+0.00278 test-auc:0.88828+0.00360    test-error:0.15735+0.00387\n[95]    train-auc:0.93134+0.00150   train-error:0.12587+0.00270 test-auc:0.88841+0.00373    test-error:0.15756+0.00378\n[96]    train-auc:0.93165+0.00156   train-error:0.12555+0.00268 test-auc:0.88859+0.00370    test-error:0.15763+0.00388\n[97]    train-auc:0.93201+0.00156   train-error:0.12513+0.00286 test-auc:0.88865+0.00362    test-error:0.15756+0.00412\n[98]    train-auc:0.93224+0.00155   train-error:0.12508+0.00271 test-auc:0.88867+0.00359    test-error:0.15728+0.00424\n[99]    train-auc:0.93247+0.00148   train-error:0.12461+0.00257 test-auc:0.88874+0.00368    test-error:0.15679+0.00387\n\n\n\nimport numpy as np\nfrom sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate additional metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\n# Basic Metrics\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)  # Also known as True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Additional Metrics\nprevalence = (tp + fn) / total\npositive_predictive_value = tp / (tp + fp)  # Same as precision\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"Precision (Positive Predictive Value): {precision:.4f}\")\nprint(f\"Recall (Sensitivity/True Positive Rate): {recall:.4f}\")\nprint(f\"Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"False Positive Rate: {false_positive_rate:.4f}\")\nprint(f\"False Negative Rate: {false_negative_rate:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Calculate ROC and PR curves\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plotting\nplt.figure(figsize=(15, 10))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred):.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nplt.plot(recall_curve, precision_curve, color='blue', lw=2,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\n\n# Plot 4: Confusion Matrix\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Print calibration metrics\nfrom sklearn.calibration import calibration_curve\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n\n# Plot calibration curve\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nCross-validation results:\nBest AUC: 0.8887 (+/- 0.0034)\nBest Error: 0.1568 (+/- 0.0013)\n\nTraining final model...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:33:24] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nComprehensive Model Performance Metrics:\n\nAccuracy: 0.8443\nPrecision (Positive Predictive Value): 0.8764\nRecall (Sensitivity/True Positive Rate): 0.9254\nSpecificity (True Negative Rate): 0.5886\nFalse Positive Rate: 0.4114\nFalse Negative Rate: 0.0746\nF1 Score: 0.9002\nPrevalence: 0.7593\nNegative Predictive Value: 0.7143\nPositive Likelihood Ratio: 2.2492\nNegative Likelihood Ratio: 0.1268\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.59      0.65       858\n           1       0.88      0.93      0.90      2706\n\n    accuracy                           0.84      3564\n   macro avg       0.80      0.76      0.77      3564\nweighted avg       0.84      0.84      0.84      3564\n\n\nConfusion Matrix:\n[[ 505  353]\n [ 202 2504]]\n\nROC AUC Score: 0.8916\n\nTop 10 Most Important Features:\n     Feature  Importance\n26  prdtocc1       234.0\n25  prdtind1       223.0\n12    prtage       176.0\n6       gtco       134.0\n15   peeduca       127.0\n23  pehractt       126.0\n0   hefaminc       111.0\n24  peio1cow       108.0\n19  pemntvty        91.0\n3      HUBUS        77.0\n11     perrp        75.0\n1   hrnumhou        67.0\n22  pehruslt        66.0\n21  prinuyer        58.0\n20  pefntvty        55.0\n17  prfamrel        51.0\n5     gtcbsa        47.0\n27   pternwa        44.0\n28    prchld        43.0\n14     pesex        42.0\n10     gtcsa        36.0\n29  prnmchld        36.0\n16  ptdtrace        32.0\n18  penatvty        28.0\n2    hrhtype        27.0\n4   gestfips        26.0\n8   gtindvpc        24.0\n13  pemaritl        22.0\n9   gtcbsasz        14.0\n7   gtcbsast        12.0\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# We add a constant for statsmodels\nX_train_final.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train_probit = sm.add_constant(X_train_final)\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit()\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# Optionally compute marginal effects\nprint(\"\\nMarginal Effects (Probit):\")\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nFitting Probit model for effect sizes and p-values...\nOptimization terminated successfully.\n         Current function value: 0.427806\n         Iterations 6\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14224\nMethod:                           MLE   Df Model:                           30\nDate:                Sun, 02 Feb 2025   Pseudo R-squ.:                  0.2249\nTime:                        05:33:30   Log-Likelihood:                -6098.4\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2208      0.902      2.463      0.014       0.454       3.988\nhefaminc      -0.0550      0.006     -9.743      0.000      -0.066      -0.044\nhrnumhou       0.0550      0.014      3.858      0.000       0.027       0.083\nhrhtype        0.0036      0.014      0.266      0.790      -0.023       0.030\nHUBUS          0.2808      0.031      9.035      0.000       0.220       0.342\ngestfips       0.1019      0.022      4.544      0.000       0.058       0.146\ngtcbsa     -3.337e-06   1.93e-06     -1.729      0.084   -7.12e-06    4.46e-07\ngtco        -8.21e-05      0.001     -0.151      0.880      -0.001       0.001\ngtcbsast       0.1007      0.032      3.175      0.001       0.039       0.163\ngtindvpc       0.0028      0.041      0.069      0.945      -0.077       0.083\ngtcbsasz       0.0894      0.020      4.542      0.000       0.051       0.128\ngtcsa         -0.0012      0.000     -6.856      0.000      -0.002      -0.001\nperrp          0.0007      0.005      0.137      0.891      -0.009       0.011\nprtage         0.0054      0.001      4.802      0.000       0.003       0.008\npemaritl       0.0251      0.011      2.260      0.024       0.003       0.047\npesex         -0.1291      0.027     -4.695      0.000      -0.183      -0.075\npeeduca       -0.1358      0.007    -19.551      0.000      -0.149      -0.122\nptdtrace      -0.0401      0.011     -3.645      0.000      -0.062      -0.019\nprfamrel       0.1337      0.022      5.951      0.000       0.090       0.178\npenatvty    7.458e-05      0.000      0.246      0.805      -0.001       0.001\npemntvty       0.0010      0.000      3.370      0.001       0.000       0.002\npefntvty      -0.0003      0.000     -1.054      0.292      -0.001       0.000\nprinuyer       0.0044      0.003      1.526      0.127      -0.001       0.010\npehruslt       0.0014      0.001      1.061      0.289      -0.001       0.004\npehractt      -0.0056      0.002     -3.699      0.000      -0.009      -0.003\npeio1cow      -0.1141      0.012     -9.191      0.000      -0.138      -0.090\nprdtind1       0.0064      0.001      5.538      0.000       0.004       0.009\nprdtocc1       0.0539      0.002     24.860      0.000       0.050       0.058\npternwa    -1.354e-07   9.27e-08     -1.461      0.144   -3.17e-07    4.62e-08\nprchld         0.0042      0.010      0.407      0.684      -0.016       0.024\nprnmchld      -0.0282      0.033     -0.859      0.390      -0.092       0.036\n==============================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nhefaminc      -0.0132      0.001     -9.814      0.000      -0.016      -0.011\nhrnumhou       0.0133      0.003      3.862      0.000       0.007       0.020\nhrhtype        0.0009      0.003      0.266      0.790      -0.006       0.007\nHUBUS          0.0676      0.007      9.111      0.000       0.053       0.082\ngestfips       0.0245      0.005      4.554      0.000       0.014       0.035\ngtcbsa     -8.035e-07   4.65e-07     -1.729      0.084   -1.71e-06    1.07e-07\ngtco       -1.977e-05      0.000     -0.151      0.880      -0.000       0.000\ngtcbsast       0.0242      0.008      3.179      0.001       0.009       0.039\ngtindvpc       0.0007      0.010      0.069      0.945      -0.019       0.020\ngtcbsasz       0.0215      0.005      4.552      0.000       0.012       0.031\ngtcsa         -0.0003   4.27e-05     -6.884      0.000      -0.000      -0.000\nperrp          0.0002      0.001      0.137      0.891      -0.002       0.003\nprtage         0.0013      0.000      4.810      0.000       0.001       0.002\npemaritl       0.0060      0.003      2.261      0.024       0.001       0.011\npesex         -0.0311      0.007     -4.706      0.000      -0.044      -0.018\npeeduca       -0.0327      0.002    -20.201      0.000      -0.036      -0.030\nptdtrace      -0.0097      0.003     -3.649      0.000      -0.015      -0.004\nprfamrel       0.0322      0.005      5.969      0.000       0.022       0.043\npenatvty    1.796e-05   7.29e-05      0.246      0.805      -0.000       0.000\npemntvty       0.0002   7.18e-05      3.374      0.001       0.000       0.000\npefntvty   -7.247e-05   6.87e-05     -1.054      0.292      -0.000    6.23e-05\nprinuyer       0.0011      0.001      1.527      0.127      -0.000       0.002\npehruslt       0.0003      0.000      1.061      0.289      -0.000       0.001\npehractt      -0.0014      0.000     -3.705      0.000      -0.002      -0.001\npeio1cow      -0.0275      0.003     -9.272      0.000      -0.033      -0.022\nprdtind1       0.0015      0.000      5.555      0.000       0.001       0.002\nprdtocc1       0.0130      0.000     26.497      0.000       0.012       0.014\npternwa    -3.261e-08   2.23e-08     -1.462      0.144   -7.63e-08    1.11e-08\nprchld         0.0010      0.002      0.407      0.684      -0.004       0.006\nprnmchld      -0.0068      0.008     -0.860      0.390      -0.022       0.009\n==============================================================================\n\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )\n\n\nCalculating and plotting SHAP values for XGBoost model...\nGenerating SHAP Beeswarm plot...\n\n\n\n\n\n\n\n\n\nGenerating SHAP Bar plot...\n\n\n\n\n\n\n\n\n\n\n# pip install bayesian-optimization\n\nCollecting bayesian-optimization\n  Downloading bayesian_optimization-2.0.3-py3-none-any.whl.metadata (9.0 kB)\nCollecting colorama&lt;0.5.0,&gt;=0.4.6 (from bayesian-optimization)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy&gt;=1.25 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.26.4)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.6.1)\nRequirement already satisfied: scipy&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.13.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.0-&gt;bayesian-optimization) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.0-&gt;bayesian-optimization) (3.5.0)\nDownloading bayesian_optimization-2.0.3-py3-none-any.whl (31 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: colorama, bayesian-optimization\nSuccessfully installed bayesian-optimization-2.0.3 colorama-0.4.6\n\n\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.feature_selection import SelectKBest, f_classif\n# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from imblearn.over_sampling import SMOTE\n# from bayes_opt import BayesianOptimization\n\n# # Load and prepare data\n# filtered_df = pd.read_csv(\"filtered_df.csv\")\n# X = filtered_df.drop(['pttlwk', 'pxtlwkhr', 'pttlwkhr', 'pxtlwk'], axis=1)\n# y = filtered_df['pttlwk']\n\n# # Convert labels from 1.0/2.0 to 0/1\n# y = (y == 2.0).astype(int)\n\n# # Split the data first\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# # Reset indices to ensure alignment\n# X_train = X_train.reset_index(drop=True)\n# X_test = X_test.reset_index(drop=True)\n# y_train = y_train.reset_index(drop=True)\n# y_test = y_test.reset_index(drop=True)\n\n# # Scale the features\n# print(\"Scaling features...\")\n# scaler = StandardScaler()\n# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n# X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n# # Feature selection\n# print(\"Performing feature selection...\")\n# selector = SelectKBest(f_classif, k=50)\n# X_train_reduced = selector.fit_transform(X_train_scaled, y_train)\n# X_test_reduced = selector.transform(X_test_scaled)\n\n# # Get selected feature names\n# selected_features = X_train.columns[selector.get_support()].tolist()\n# X_train_reduced = pd.DataFrame(X_train_reduced, columns=selected_features)\n# X_test_reduced = pd.DataFrame(X_test_reduced, columns=selected_features)\n\n# # Calculate VIF\n# print(\"Performing multicollinearity analysis...\")\n# vif_data = pd.DataFrame()\n# vif_data[\"Feature\"] = selected_features\n# vif_data[\"VIF\"] = [variance_inflation_factor(X_train_reduced.values, i)\n#                    for i in range(X_train_reduced.shape[1])]\n# print(\"\\nVIF Results:\")\n# print(vif_data)\n\n# # Drop high VIF features\n# high_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\n# print(f\"\\nDropping features with high VIF: {high_vif_features}\")\n# X_train_vif = X_train_reduced.drop(columns=high_vif_features)\n# X_test_vif = X_test_reduced.drop(columns=high_vif_features)\n\n# # Outlier removal\n# print(\"\\nPerforming outlier analysis...\")\n# z_scores = np.abs((X_train_vif - X_train_vif.mean()) / X_train_vif.std())\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n\n# # Make sure indices align before filtering\n# X_train_clean = X_train_vif[non_outliers].reset_index(drop=True)\n# y_train_clean = pd.Series(y_train.values[non_outliers]).reset_index(drop=True)\n\n# # Apply SMOTE for class imbalance\n# print(\"\\nApplying SMOTE for class balancing...\")\n# smote = SMOTE(random_state=42)\n# X_train_balanced, y_train_balanced = smote.fit_resample(X_train_clean, y_train_clean)\n\n# # Convert to numpy arrays for XGBoost\n# X_train_balanced = np.array(X_train_balanced)\n# y_train_balanced = np.array(y_train_balanced)\n# X_test_vif_array = np.array(X_test_vif)\n\n# # Function for Bayesian Optimization\n# def xgb_evaluate(max_depth, learning_rate, n_estimators, subsample, colsample_bytree):\n#     params = {\n#         'max_depth': int(max_depth),\n#         'learning_rate': learning_rate,\n#         'n_estimators': int(n_estimators),\n#         'subsample': subsample,\n#         'colsample_bytree': colsample_bytree,\n#         'objective': 'binary:logistic',\n#         'eval_metric': 'logloss',\n#         'seed': 42\n#     }\n\n#     # Convert to DMatrix format\n#     dtrain = xgb.DMatrix(X_train_balanced, label=y_train_balanced)\n#     dtest = xgb.DMatrix(X_test_vif_array)\n\n#     # Train model\n#     model = xgb.train(params, dtrain, num_boost_round=int(n_estimators))\n\n#     # Make predictions\n#     y_pred = (model.predict(dtest) &gt; 0.5).astype(int)\n\n#     return accuracy_score(y_test, y_pred)\n\n# # Initialize Bayesian Optimization\n# print(\"\\nStarting Bayesian Optimization...\")\n# optimizer = BayesianOptimization(\n#     f=xgb_evaluate,\n#     pbounds={\n#         'max_depth': (3, 10),\n#         'learning_rate': (0.01, 0.3),\n#         'n_estimators': (50, 200),\n#         'subsample': (0.5, 1.0),\n#         'colsample_bytree': (0.5, 1.0)\n#     },\n#     random_state=42\n# )\n\n# # Run optimization\n# optimizer.maximize(\n#     init_points=5,\n#     n_iter=15\n# )\n\n# # Get best parameters\n# best_params = optimizer.max['params']\n# print(\"\\nBest parameters found:\")\n# print(best_params)\n\n# # Train final model with best parameters\n# print(\"\\nTraining final model with best parameters...\")\n# final_params = {\n#     'max_depth': int(best_params['max_depth']),\n#     'learning_rate': best_params['learning_rate'],\n#     'n_estimators': int(best_params['n_estimators']),\n#     'subsample': best_params['subsample'],\n#     'colsample_bytree': best_params['colsample_bytree'],\n#     'objective': 'binary:logistic',\n#     'eval_metric': 'logloss'\n# }\n\n# # Create final DMatrix objects\n# dtrain = xgb.DMatrix(X_train_balanced, label=y_train_balanced)\n# dtest = xgb.DMatrix(X_test_vif_array)\n\n# # Train final model\n# final_model = xgb.train(final_params, dtrain)\n# y_pred = (final_model.predict(dtest) &gt; 0.5).astype(int)\n\n# # Print results\n# print(\"\\nModel Performance Metrics:\")\n# print(\"\\nClassification Report:\")\n# print(classification_report(y_test, y_pred))\n\n# # Plot confusion matrix\n# plt.figure(figsize=(8, 6))\n# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n# plt.title('XGBoost Confusion Matrix')\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.tight_layout()\n# plt.show()\n\n# # Plot optimization history\n# plt.figure(figsize=(10, 5))\n# plt.plot(range(len(optimizer.space.target)), optimizer.space.target, '-o')\n# plt.xlabel('Iteration')\n# plt.ylabel('Model Accuracy')\n# plt.title('Bayesian Optimization History')\n# plt.axvline(x=5, color='r', linestyle='--', label='End of Random Search')\n# plt.legend()\n# plt.show()\n\n# # Feature importance\n# importance = final_model.get_score(importance_type='weight')\n# importance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\n# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# plt.figure(figsize=(12, 6))\n# sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n# plt.title('Top 10 Feature Importances')\n# plt.xlabel('Importance')\n# plt.ylabel('Feature')\n# plt.tight_layout()\n# plt.show()\n\nScaling features...\nPerforming feature selection...\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [  7   8   9  16  17  24  26  32  42  53  66  69  71  72  76  77  78  79\n  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n 107 109 115 160 173 174 175 176 177 178 179 180 181 182 195 196 197 198\n 199 200 201 202 203 204 205 206 207 208 211 212 213 214 215 218 219 220\n 221 222 223 224 225 238 239 240 241 242 243 244 245 246 247 248 249 250\n 251 255 256 257 258 259 260 261 263 264 267 270 271 272 274 291] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n  vif = 1. / (1. - r_squared_i)\n\n\n\nVIF Results:\n      Feature         VIF\n0    OCCURNUM         inf\n1       HUBUS    3.104158\n2     HUBUSL1    3.040994\n3    hehousut    1.053932\n4    hrnumhou    1.613758\n5    hefaminc    1.366005\n6    hxfaminc    1.041178\n7    PULINENO         inf\n8    PUIO1MFG    1.200420\n9    PUSLFPRX    1.730368\n10      perrp    3.068334\n11   pemaritl  450.277952\n12    pxrace1    1.239154\n13   pehspnon    3.504174\n14    peeduca    1.865216\n15   pespouse   10.326498\n16   pemntvty    5.751234\n17   pefntvty    5.795244\n18   pedipged    1.620007\n19      pecyc    1.261176\n20     pepar1    6.813180\n21     pepar2    7.247503\n22  pepar1typ    8.514617\n23  pepar2typ    8.421113\n24   prmarsta  547.232596\n25    prdthsp    3.022894\n26   prfamrel    2.549348\n27     prtage    1.580120\n28    pemjnum    1.034042\n29   peio1icd   11.798984\n30   prmjind1   31.276246\n31   prmjocc1   92.513207\n32   prdtind1   41.036307\n33   prdtocc1   34.092564\n34    prcowpg    1.424787\n35   prmjocgr   48.168168\n36   peernwkp    1.834483\n37    peernrt    2.477255\n38   peernhro    6.011818\n39   prhernal    4.562487\n40   pternhly    5.746613\n41   pternh1o    7.337398\n42    pternwa    1.804345\n43   ptio1ocd   94.423709\n44     gtcbsa    2.323201\n45   gtcbsast    1.893200\n46   gtcbsasz    3.823655\n47      gtcsa    1.555382\n48   gtmetsta    2.227311\n49   gtindvpc    1.217917\n\nDropping features with high VIF: ['OCCURNUM', 'PULINENO', 'pemaritl', 'pespouse', 'prmarsta', 'peio1icd', 'prmjind1', 'prmjocc1', 'prdtind1', 'prdtocc1', 'prmjocgr', 'ptio1ocd']\n\nPerforming outlier analysis...\n\nApplying SMOTE for class balancing...\n\nStarting Bayesian Optimization...\n|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n-------------------------------------------------------------------------------------\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:48] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 1         | 0.7732    | 0.6873    | 0.2857    | 8.124     | 139.8     | 0.578     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:49] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 2         | 0.7872    | 0.578     | 0.02684   | 9.063     | 140.2     | 0.854     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:51] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 3         | 0.7812    | 0.5103    | 0.2913    | 8.827     | 81.85     | 0.5909    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:53] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 4         | 0.7909    | 0.5917    | 0.09823   | 6.673     | 114.8     | 0.6456    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:54] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 5         | 0.7862    | 0.8059    | 0.05045   | 5.045     | 105.0     | 0.728     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:54] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 6         | 0.7849    | 0.68      | 0.2875    | 6.503     | 114.9     | 0.7288    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:55] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 7         | 0.7828    | 0.7325    | 0.2368    | 6.817     | 114.4     | 0.7645    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:55] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 8         | 0.7914    | 0.8502    | 0.08643   | 6.483     | 114.6     | 0.742     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:56] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 9         | 0.7805    | 0.6778    | 0.02618   | 6.569     | 114.7     | 0.8147    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:57] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 10        | 0.786     | 0.9038    | 0.06098   | 5.217     | 90.33     | 0.8629    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:57] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 11        | 0.7864    | 0.7456    | 0.08416   | 8.146     | 139.4     | 0.899     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:58] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 12        | 0.785     | 0.5377    | 0.2455    | 6.267     | 95.87     | 0.6494    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:59] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 13        | 0.781     | 0.5413    | 0.2721    | 7.156     | 162.5     | 0.874     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:59] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 14        | 0.7825    | 0.8641    | 0.2036    | 9.962     | 111.0     | 0.7694    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:00] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 15        | 0.7889    | 0.5668    | 0.2238    | 5.109     | 78.43     | 0.6134    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:01] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 16        | 0.7899    | 0.6357    | 0.199     | 3.435     | 162.1     | 0.9796    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:01] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 17        | 0.7862    | 0.5819    | 0.03932   | 4.665     | 186.8     | 0.8976    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:02] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 18        | 0.7827    | 0.8887    | 0.1137    | 8.863     | 190.2     | 0.5815    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:03] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 19        | 0.7818    | 0.7528    | 0.1584    | 8.54      | 99.56     | 0.9612    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:04] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 20        | 0.7884    | 0.9319    | 0.1759    | 5.914     | 77.49     | 0.6407    |\n=====================================================================================\n\nBest parameters found:\n{'colsample_bytree': 0.8502042604408235, 'learning_rate': 0.08643263337802379, 'max_depth': 6.482992630463139, 'n_estimators': 114.61319594670005, 'subsample': 0.7420110182828307}\n\nTraining final model with best parameters...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:05] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nModel Performance Metrics:\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.48      0.55      0.51      1364\n           1       0.86      0.82      0.84      4618\n\n    accuracy                           0.76      5982\n   macro avg       0.67      0.69      0.68      5982\nweighted avg       0.77      0.76      0.77      5982\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\nPUMS Classification model\n\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv(\"psam_p34.csv\")\ndf2 = pd.read_csv(\"psam_p36.csv\")\n\n\n# Concatenate dataframes\nmerged_df = pd.concat([df, df2], axis=0)\n\n# Drop weight columns\npwgtp_cols = [col for col in merged_df.columns if 'PWGTP' in col]\nfiltered_df = merged_df.drop(columns=pwgtp_cols)\n\n# Transform JWTRNS (0 for values 1-10, 1 for value 11)\nfiltered_df['JWTRNS'] = (filtered_df['JWTRNS'] == 11).astype(int)\n\nprint(f\"Original shapes: df {df.shape}, df2 {df2.shape}\")\nprint(f\"Combined shape: {merged_df.shape}\")\nprint(f\"Final shape: {filtered_df.shape}\")\nprint(\"\\nJWTRNS distribution:\")\nprint(filtered_df['JWTRNS'].value_counts())\n\nOriginal shapes: df (94314, 287), df2 (206408, 287)\nCombined shape: (300722, 287)\nFinal shape: (300722, 206)\n\nJWTRNS distribution:\nJWTRNS\n0    281466\n1     19256\nName: count, dtype: int64\n\n\n\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nAGEP\nCIT\n...\nFSEMP\nFSEXP\nFSSIP\nFSSP\nFWAGP\nFWKHP\nFWKLP\nFWKWNP\nFWRKP\nFYOEP\n\n\n\n\n0\nP\n2023GQ0000001\n2\n1\n2501\n1\n34\n1019518\n94\n1\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n1\nP\n2023GQ0000002\n2\n1\n603\n1\n34\n1019518\n18\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nP\n2023GQ0000021\n2\n1\n2103\n1\n34\n1019518\n78\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nP\n2023GQ0000058\n2\n1\n307\n1\n34\n1019518\n79\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nP\n2023GQ0000158\n2\n1\n1201\n1\n34\n1019518\n63\n1\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n206403\nP\n2023HU1459885\n2\n4\n4503\n1\n36\n1019518\n61\n5\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206404\nP\n2023HU1459885\n2\n5\n4503\n1\n36\n1019518\n60\n5\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206405\nP\n2023HU1459895\n2\n1\n2201\n1\n36\n1019518\n28\n1\n...\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n\n\n206406\nP\n2023HU1459895\n2\n2\n2201\n1\n36\n1019518\n4\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206407\nP\n2023HU1459895\n2\n3\n2201\n1\n36\n1019518\n27\n3\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n300722 rows × 206 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\npd.set_option('display.max_columns', None)\n\n\ndef clean_dataframe(df, column_threshold=0.1):\n    \"\"\"\n    Removes columns with more than `column_threshold`% NaN values,\n    then removes remaining rows with NaNs.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame\n    column_threshold (float): Threshold for dropping columns (default 40%)\n\n    Returns:\n    pd.DataFrame: Cleaned DataFrame\n    \"\"\"\n    # Calculate percentage of NaNs per column\n    nan_percentage = df.isna().mean()\n\n    # Drop columns exceeding the threshold\n    cols_to_drop = nan_percentage[nan_percentage &gt; column_threshold].index\n    df_cleaned = df.drop(columns=cols_to_drop)\n\n    # Drop remaining rows with NaNs\n    df_cleaned = df_cleaned.dropna()\n\n    return df_cleaned\n\nfiltered_df_new = clean_dataframe(filtered_df, column_threshold=0.1)  # Set 40% threshold\nprint(f\"Shape before: {filtered_df_new.shape}, Shape after: {filtered_df.shape}\")\n\nShape before: (269827, 131), Shape after: (300722, 206)\n\n\n\nfiltered_df_new\n\n\n  \n    \n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nAGEP\nCIT\nDDRS\nDEAR\nDEYE\nDPHY\nDREM\nHIMRKS\nHINS1\nHINS2\nHINS3\nHINS4\nHINS5\nHINS6\nHINS7\nJWTRNS\nLANX\nMAR\nMIG\nRELSHIPP\nSCH\nSCHL\nSEX\nANC\nANC1P\nANC2P\nDIS\nHICOV\nHISP\nNATIVITY\nOC\nPOBP\nPOVPIP\nPRIVCOV\nPUBCOV\nQTRBIR\nRAC1P\nRAC2P\nRAC3P\nRACAIAN\nRACASN\nRACBLK\nRACNH\nRACNUM\nRACPI\nRACSOR\nRACWHT\nRC\nWAOB\nFAGEP\nFANCP\nFCITP\nFCITWP\nFCOWP\nFDDRSP\nFDEARP\nFDEYEP\nFDISP\nFDOUTP\nFDPHYP\nFDRATP\nFDRATXP\nFDREMP\nFENGP\nFESRP\nFFERP\nFFODP\nFGCLP\nFGCMP\nFGCRP\nFHICOVP\nFHIMRKSP\nFHINS1P\nFHINS2P\nFHINS3P\nFHINS4P\nFHINS5P\nFHINS6P\nFHINS7P\nFHISP\nFINDP\nFINTP\nFJWDP\nFJWMNP\nFJWRIP\nFJWTRNSP\nFLANP\nFLANXP\nFMARP\nFMARHDP\nFMARHMP\nFMARHTP\nFMARHWP\nFMARHYP\nFMIGP\nFMIGSP\nFMILPP\nFMILSP\nFOCCP\nFOIP\nFPAP\nFPERNP\nFPINCP\nFPOBP\nFPOWSP\nFPRIVCOVP\nFPUBCOVP\nFRACP\nFRELSHIPP\nFRETP\nFSCHGP\nFSCHLP\nFSCHP\nFSEMP\nFSEXP\nFSSIP\nFSSP\nFWAGP\nFWKHP\nFWKLP\nFWKWNP\nFWRKP\nFYOEP\n\n\n\n\n3350\nP\n2023HU0000023\n2\n1\n1501\n1\n34\n1019518\n61\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n1\n2.0\n1\n1.0\n20\n1.0\n22.0\n1\n2\n51\n148\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n1\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3351\nP\n2023HU0000023\n2\n2\n1501\n1\n34\n1019518\n57\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n0\n2.0\n1\n1.0\n21\n1.0\n21.0\n2\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3352\nP\n2023HU0000023\n2\n3\n1501\n1\n34\n1019518\n25\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n0\n2.0\n5\n1.0\n25\n1.0\n21.0\n1\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3353\nP\n2023HU0000023\n2\n4\n1501\n1\n34\n1019518\n22\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n0\n1.0\n5\n3.0\n25\n1.0\n21.0\n2\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n2\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3354\nP\n2023HU0000229\n2\n1\n602\n1\n34\n1019518\n36\n5\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n0\n1.0\n5\n1.0\n20\n1.0\n22.0\n1\n1\n785\n999\n2\n1\n1\n2\n0.0\n247\n501.0\n1\n2\n3\n6\n4014\n9\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n206401\nP\n2023HU1459885\n2\n2\n4503\n1\n36\n1019518\n37\n1\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n0\n1.0\n1\n3.0\n21\n1.0\n15.0\n2\n1\n706\n999\n2\n1\n1\n1\n0.0\n36\n269.0\n2\n1\n1\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206403\nP\n2023HU1459885\n2\n4\n4503\n1\n36\n1019518\n61\n5\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n0\n1.0\n1\n3.0\n29\n1.0\n1.0\n1\n1\n706\n999\n2\n1\n1\n2\n0.0\n207\n269.0\n2\n1\n3\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206404\nP\n2023HU1459885\n2\n5\n4503\n1\n36\n1019518\n60\n5\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n0\n2.0\n1\n3.0\n29\n1.0\n1.0\n2\n1\n706\n999\n2\n1\n1\n2\n0.0\n207\n269.0\n2\n1\n2\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206405\nP\n2023HU1459895\n2\n1\n2201\n1\n36\n1019518\n28\n1\n2.0\n2\n2\n2.0\n1.0\n0\n2\n2\n2\n1\n2\n2\n2\n0\n2.0\n5\n1.0\n20\n1.0\n17.0\n2\n1\n261\n999\n1\n1\n3\n1\n0.0\n36\n284.0\n2\n1\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n\n\n206407\nP\n2023HU1459895\n2\n3\n2201\n1\n36\n1019518\n27\n3\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n0\n2.0\n5\n3.0\n34\n1.0\n16.0\n2\n2\n195\n22\n2\n1\n1\n1\n0.0\n138\n259.0\n1\n2\n2\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n5\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n269827 rows × 131 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nX = filtered_df_new.drop(['RT','SERIALNO','JWTRNS'], axis=1)\ny = filtered_df_new['JWTRNS']\n\n\nX\n\n\n  \n    \n\n\n\n\n\n\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nAGEP\nCIT\nDDRS\nDEAR\nDEYE\nDPHY\nDREM\nHIMRKS\nHINS1\nHINS2\nHINS3\nHINS4\nHINS5\nHINS6\nHINS7\nLANX\nMAR\nMIG\nRELSHIPP\nSCH\nSCHL\nSEX\nANC\nANC1P\nANC2P\nDIS\nHICOV\nHISP\nNATIVITY\nOC\nPOBP\nPOVPIP\nPRIVCOV\nPUBCOV\nQTRBIR\nRAC1P\nRAC2P\nRAC3P\nRACAIAN\nRACASN\nRACBLK\nRACNH\nRACNUM\nRACPI\nRACSOR\nRACWHT\nRC\nWAOB\nFAGEP\nFANCP\nFCITP\nFCITWP\nFCOWP\nFDDRSP\nFDEARP\nFDEYEP\nFDISP\nFDOUTP\nFDPHYP\nFDRATP\nFDRATXP\nFDREMP\nFENGP\nFESRP\nFFERP\nFFODP\nFGCLP\nFGCMP\nFGCRP\nFHICOVP\nFHIMRKSP\nFHINS1P\nFHINS2P\nFHINS3P\nFHINS4P\nFHINS5P\nFHINS6P\nFHINS7P\nFHISP\nFINDP\nFINTP\nFJWDP\nFJWMNP\nFJWRIP\nFJWTRNSP\nFLANP\nFLANXP\nFMARP\nFMARHDP\nFMARHMP\nFMARHTP\nFMARHWP\nFMARHYP\nFMIGP\nFMIGSP\nFMILPP\nFMILSP\nFOCCP\nFOIP\nFPAP\nFPERNP\nFPINCP\nFPOBP\nFPOWSP\nFPRIVCOVP\nFPUBCOVP\nFRACP\nFRELSHIPP\nFRETP\nFSCHGP\nFSCHLP\nFSCHP\nFSEMP\nFSEXP\nFSSIP\nFSSP\nFWAGP\nFWKHP\nFWKLP\nFWKWNP\nFWRKP\nFYOEP\n\n\n\n\n3350\n2\n1\n1501\n1\n34\n1019518\n61\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n2.0\n1\n1.0\n20\n1.0\n22.0\n1\n2\n51\n148\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n1\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3351\n2\n2\n1501\n1\n34\n1019518\n57\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n2.0\n1\n1.0\n21\n1.0\n21.0\n2\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3352\n2\n3\n1501\n1\n34\n1019518\n25\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n2.0\n5\n1.0\n25\n1.0\n21.0\n1\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3353\n2\n4\n1501\n1\n34\n1019518\n22\n1\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n1.0\n5\n3.0\n25\n1.0\n21.0\n2\n2\n51\n50\n2\n1\n1\n1\n0.0\n34\n501.0\n1\n2\n2\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3354\n2\n1\n602\n1\n34\n1019518\n36\n5\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n1.0\n5\n1.0\n20\n1.0\n22.0\n1\n1\n785\n999\n2\n1\n1\n2\n0.0\n247\n501.0\n1\n2\n3\n6\n4014\n9\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n206401\n2\n2\n4503\n1\n36\n1019518\n37\n1\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n1.0\n1\n3.0\n21\n1.0\n15.0\n2\n1\n706\n999\n2\n1\n1\n1\n0.0\n36\n269.0\n2\n1\n1\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206403\n2\n4\n4503\n1\n36\n1019518\n61\n5\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n1.0\n1\n3.0\n29\n1.0\n1.0\n1\n1\n706\n999\n2\n1\n1\n2\n0.0\n207\n269.0\n2\n1\n3\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206404\n2\n5\n4503\n1\n36\n1019518\n60\n5\n2.0\n2\n2\n2.0\n2.0\n0\n2\n2\n2\n1\n2\n2\n2\n2.0\n1\n3.0\n29\n1.0\n1.0\n2\n1\n706\n999\n2\n1\n1\n2\n0.0\n207\n269.0\n2\n1\n2\n6\n4000\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0.0\n4\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n206405\n2\n1\n2201\n1\n36\n1019518\n28\n1\n2.0\n2\n2\n2.0\n1.0\n0\n2\n2\n2\n1\n2\n2\n2\n2.0\n5\n1.0\n20\n1.0\n17.0\n2\n1\n261\n999\n1\n1\n3\n1\n0.0\n36\n284.0\n2\n1\n3\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n\n\n206407\n2\n3\n2201\n1\n36\n1019518\n27\n3\n2.0\n2\n2\n2.0\n2.0\n0\n1\n2\n2\n2\n2\n2\n2\n2.0\n5\n3.0\n34\n1.0\n16.0\n2\n2\n195\n22\n2\n1\n1\n1\n0.0\n138\n259.0\n1\n2\n2\n1\n1000\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0.0\n5\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n269827 rows × 128 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nprint(\"Performing initial feature selection...\")\nselector = SelectKBest(mutual_info_classif, k=50)  # Keep top 50 features\nX_reduced = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support(indices=True)]\nprint(\"Top 50 features selected.\")\n\nPerforming initial feature selection...\nTop 50 features selected.\n\n\n\n# Create a DataFrame with reduced features for VIF and further steps\nX = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n  return 1 - self.ssr/self.centered_tss\n\n\nVIF Results:\n     Feature        VIF\n0   DIVISION   0.000000\n1    SPORDER   2.869733\n2     REGION   0.000000\n3      STATE   1.027068\n4     ADJINC   0.000000\n5       AGEP   5.067786\n6        CIT  29.190795\n7       DDRS   1.456627\n8       DEAR   1.486323\n9       DEYE   1.213110\n10      DPHY   2.468438\n11      DREM   1.734816\n12     HINS1   8.008333\n13     HINS2   3.408955\n14     HINS3   6.312910\n15     HINS4   4.484648\n16     HINS5   1.151792\n17     HINS6   1.137001\n18     HINS7   1.008831\n19      LANX   2.060376\n20       MAR   1.973975\n21       MIG   1.039952\n22  RELSHIPP   2.605885\n23       SCH   2.207972\n24      SCHL   2.225258\n25       SEX   1.025181\n26       ANC   1.598622\n27     ANC2P   1.236159\n28       DIS   4.025628\n29     HICOV   1.757198\n30      HISP   1.516314\n31  NATIVITY  20.347995\n32        OC  10.232945\n33      POBP   6.726340\n34    POVPIP   1.522446\n35   PRIVCOV  10.015254\n36    PUBCOV   9.146812\n37    QTRBIR   1.000340\n38     RAC1P  22.747278\n39     RAC2P  27.650659\n40    RACNUM   3.897050\n41    RACWHT   3.066811\n42        RC  12.664471\n43      WAOB   6.800092\n44   FHICOVP  28.691421\n45     FJWDP   2.857438\n46    FJWMNP   4.163790\n47    FPINCP   1.455027\n48    FPOWSP   3.514944\n49  FPUBCOVP  28.977155\nDropping features with high VIF: ['CIT', 'NATIVITY', 'OC', 'PRIVCOV', 'RAC1P', 'RAC2P', 'RC', 'FHICOVP', 'FPUBCOVP']\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\nPerforming Recursive Feature Elimination with XGBoost (GPU)...\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-15-9179f3fec620&gt; in &lt;cell line: 0&gt;()\n     56         n_jobs=-1\n     57     )\n---&gt; 58     rfecv.fit(X_train, y_train)\n     59 \n     60 # Get selected features\n\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)\n     61             extra_args = len(args) - len(all_args)\n     62             if extra_args &lt;= 0:\n---&gt; 63                 return f(*args, **kwargs)\n     64 \n     65             # extra_args &gt; 0\n\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py in wrapper(estimator, *args, **kwargs)\n   1387                 )\n   1388             ):\n-&gt; 1389                 return fit_method(estimator, *args, **kwargs)\n   1390 \n   1391         return wrapper\n\n/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_rfe.py in fit(self, X, y, groups, **params)\n    830 \n    831         # Initialization\n--&gt; 832         cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n    833         scorer = self._get_scorer()\n    834 \n\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py in is_classifier(estimator)\n   1235         return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n   1236 \n-&gt; 1237     return get_tags(estimator).estimator_type == \"classifier\"\n   1238 \n   1239 \n\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py in get_tags(estimator)\n    428         for klass in reversed(type(estimator).mro()):\n    429             if \"__sklearn_tags__\" in vars(klass):\n--&gt; 430                 sklearn_tags_provider[klass] = klass.__sklearn_tags__(estimator)  # type: ignore[attr-defined]\n    431                 class_order.append(klass)\n    432             elif \"_more_tags\" in vars(klass):\n\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py in __sklearn_tags__(self)\n    538 \n    539     def __sklearn_tags__(self):\n--&gt; 540         tags = super().__sklearn_tags__()\n    541         tags.estimator_type = \"classifier\"\n    542         tags.classifier_tags = ClassifierTags()\n\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\n\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# # Align indices before filtering\n# X_train_final = X_train_final[non_outliers].reset_index(drop=True)\n# y_train = y_train.iloc[X_train.index[non_outliers]].reset_index(drop=True) # Use iloc and original X_train index to align\n\n\nX_train_final\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\n\nimport numpy as np\nfrom sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate additional metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\n# Basic Metrics\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)  # Also known as True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Additional Metrics\nprevalence = (tp + fn) / total\npositive_predictive_value = tp / (tp + fp)  # Same as precision\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"Precision (Positive Predictive Value): {precision:.4f}\")\nprint(f\"Recall (Sensitivity/True Positive Rate): {recall:.4f}\")\nprint(f\"Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"False Positive Rate: {false_positive_rate:.4f}\")\nprint(f\"False Negative Rate: {false_negative_rate:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Calculate ROC and PR curves\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plotting\nplt.figure(figsize=(15, 10))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred):.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nplt.plot(recall_curve, precision_curve, color='blue', lw=2,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\n\n# Plot 4: Confusion Matrix\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Print calibration metrics\nfrom sklearn.calibration import calibration_curve\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n\n# Plot calibration curve\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# We add a constant for statsmodels\nX_train_final.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train_probit = sm.add_constant(X_train_final)\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit()\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# Optionally compute marginal effects\nprint(\"\\nMarginal Effects (Probit):\")\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )"
  },
  {
    "objectID": "SAE.html",
    "href": "SAE.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\n\npd.set_option('display.max_columns', None)\n\n\ndf1 = pd.read_csv('hps_04_00_02_puf.csv')\ndf1.head()\n\n\n\n\n\n\n\n\nSCRAM\nCYCLE\nEST_ST\nEST_MSA\nREGION\nHWEIGHT\nPWEIGHT\nTBIRTH_YEAR\nABIRTH_YEAR\nRHISPANIC\nAHISPANIC\nRRACE\nARACE\nEEDUC\nAEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nGENID_DESCRIBE\nSEXUAL_ORIENTATION\nTHHLD_NUMPER\nAHHLD_NUMPER\nTHHLD_NUMKID\nAHHLD_NUMKID\nTHHLD_NUMADLT\nKIDS_LT5Y\nKIDS_5_11Y\nKIDS_12_17Y\nENRPUBCHK\nENRPRVCHK\nENRHMSCHK\nTENROLLPUB\nTENROLLPRV\nTENROLLHMSCH\nENROLLNONE\nACTVDUTY1\nACTVDUTY2\nACTVDUTY3\nACTVDUTY4\nACTVDUTY5\nRECVDVACC\nHADCOVIDRV\nSYMPTOMS\nLONGCOVID\nSYMPTMNOW\nWRKLOSSRV\nANYWORK\nKINDWORK\nRSNNOWRKRV\nEXPNS_DIF\nTWDAYS\nCURFOODSUF\nCHILDFOOD\nFOODRSNRV1\nFOODRSNRV2\nFOODRSNRV3\nFOODRSNRV4\nFREEFOOD\nSCHLFDHLP_RV1\nSCHLFDHLP_RV2\nSCHLFDHLP_RV3\nSCHLFDHLP_RV4\nSCHLFDHLP_RV5\nFDBENEFIT1\nANXIOUS\nWORRY\nINTEREST\nDOWN\nHLTHINS1\nHLTHINS2\nHLTHINS3\nHLTHINS4\nHLTHINS5\nHLTHINS6\nHLTHINS7\nHLTHINS8\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nRENTCHNG\nLIVQTRRV\nRENTCUR\nMORTCUR\nTMNTHSBHND\nEVICT\nFORCLOSE\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nSYMPTMIMPCT\nPRICECHNG\nPRICESTRESS\nPRICECONCRN\nTWDAYS_RESP\nFRMLA_YN\nFRMLA_AGE\nFRMLA_DIFFCLT\nGAS1\nGAS2\nGAS3\nGAS4\nSCHLFDHLP_RV6\nSCHLFDHLP_RV7\nSCHLFDHLP_RV8\nFDBENEFIT2\nSCHLFDEXPNS\nND_DISPLACE\nND_TYPE1\nND_TYPE2\nND_TYPE3\nND_TYPE4\nND_TYPE5\nND_HOWLONG\nND_DAMAGE\nND_FDSHRTAGE\nND_WATER\nND_ELCTRC\nND_UNSANITARY\nND_ISOLATE\nND_CRIME\nND_SCAM\nFDBENEFIT3\nBABY_FED\nMHLTH_NEED\nMHLTH_GET\nMHLTH_SATISFD\nMHLTH_DIFFCLT\nMOVEWHY1\nMOVEWHY2\nMOVEWHY3\nMOVEWHY4\nMOVEWHY5\nMOVEWHY6\nMOVEWHY7\nMOVEWHY8\nMOVED\nWHENCOVIDRV1\nWHENCOVIDRV2\nWHENCOVIDRV3\nVETERAN1\nVETERAN2\nVETERAN3\nVETERAN4\nVETERAN5\nCHILDCARE\nCHILDCARE_RSLT1\nCHILDCARE_RSLT2\nCHILDCARE_RSLT3\nCHILDCARE_RSLT4\nCHILDCARE_RSLT5\nCHILDCARE_RSLT6\nCHILDCARE_RSLT7\nCHILDCARE_RSLT8\nCHILDCARE_RSLT9\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nSUPPORT4\nSUPPORT1EXP\nRVACCDATE\nRSVVACC\n\n\n\n\n0\nP020000001\n2\n32\nNaN\n4\n704.966315\n2067.690868\n1976\n1\n1\n2\n2\n2\n6\n2\n4\n2\n2\n2\n2\n3\n2\n0\n2\n3\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n1\n-99\n2\n-88\n-99\n1\n1\n-99\n2\n-88\n-88\n-88\n-88\n-88\n1\n4\n4\n4\n4\n-99\n-99\n-99\n1\n-99\n-99\n-99\n-99\n3\n1\n3\n1\n3\n2\n3\n1\n3\n2\n6\n1\n-88\n-88\n-88\n-88\n2\n2\n3\n2\n-88\n4\n-88\n2\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-99\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n5\n3\n2\n1\n4\n1\n2\n-99\n-88\n\n\n1\nP020000002\n2\n53\nNaN\n4\n716.582115\n1359.474802\n1961\n2\n1\n2\n1\n2\n5\n2\n3\n2\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n-99\n4\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n3\n3\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n\n\n2\nP020000003\n2\n6\n31080.0\n4\n2439.529962\n4554.378984\n1988\n2\n1\n2\n1\n2\n7\n2\n1\n2\n2\n2\n3\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n3\n2\n2\n2\n1\n1\n-88\n1\n1\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n7\n-88\n4\n-88\n4\n1\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n-99\n1\n-99\n1\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n4\n2\n2\n1\n1\n4\n1\n-88\n\n\n3\nP020000004\n2\n48\nNaN\n2\n3945.461037\n7550.581707\n1956\n2\n1\n2\n1\n2\n5\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n2\n-88\n-88\n-88\n2\n1\n-99\n-88\n2\n4\n2\n-88\n1\n-99\n-99\n-99\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n2\n-88\n1\n2\n1\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n3\n3\n4\n2\n4\n1\n1\n\n\n4\nP020000005\n2\n53\n42660.0\n4\n489.900163\n929.421644\n1970\n2\n1\n2\n1\n2\n6\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n2\n2\n2\n2\n1\n1\n-88\n1\n3\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n8\n-88\n1\n2\n1\n4\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n2\n1\n1\n1\n4\n2\n-88\n\n\n\n\n\n\n\n\ndf1['EST_ST'] = df1['EST_ST'].astype(str).str.pad(2, side='left', fillchar='0')\n\n\n# Filter the dataset to include only records with TWDAYS values 1, 2, or 3\nfiltered_data = df1[df1['TWDAYS_RESP'].isin([1, 2, 3])]\n\n# Basic EDA on the filtered dataset\n# Count of records by state\nstate_distribution = filtered_data['EST_ST'].value_counts()\nmsa_distribution = filtered_data['EST_MSA'].value_counts()\n# Summary statistics for telework days\ntelework_days_summary = filtered_data['TWDAYS_RESP'].describe()\n\n\nprint(state_distribution)\n\nEST_ST\n06    1485\n53     842\n48     841\n51     596\n25     587\n08     572\n24     561\n42     496\n12     482\n41     459\n11     449\n13     449\n04     448\n17     424\n36     423\n26     413\n27     392\n49     373\n34     340\n37     323\n09     306\n55     303\n29     278\n39     261\n18     250\n47     248\n33     210\n19     203\n35     202\n20     202\n21     192\n45     185\n16     177\n40     164\n50     160\n31     155\n44     150\n32     145\n01     140\n02     138\n23     138\n05     137\n10     128\n30     118\n15     107\n22     103\n46     101\n28      83\n54      80\n38      71\n56      49\nName: count, dtype: int64\n\n\n\nprint(msa_distribution)\n\nEST_MSA\n47900.0    1125\n42660.0     628\n41860.0     586\n35620.0     537\n14460.0     522\n37980.0     441\n31080.0     404\n16980.0     388\n19100.0     386\n12060.0     370\n38060.0     350\n26420.0     256\n19820.0     234\n33100.0     194\n40140.0     176\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Telecommuting Frequency Distribution\nplt.figure(figsize=(10, 6))\nsns.countplot(x='TWDAYS_RESP', data=filtered_data)\nplt.title('Distribution of Telecommuting Frequency')\nplt.xlabel('Number of Telework Days')\nplt.ylabel('Count')\nplt.xticks([0, 1, 2], ['1-2 Days', '3-4 Days', '5+ Days'])\nplt.show()\n\n\n\n\n\n\n\n\n\n# State-wise Telecommuting Patterns\nplt.figure(figsize=(15, 8))\nstate_counts = filtered_data['EST_ST'].value_counts().sort_index()\nsns.barplot(x=state_counts.index, y=state_counts.values)\nplt.title('State-wise Telecommuting Patterns')\nplt.xlabel('State Identifier')\nplt.ylabel('Count of Telecommuting Reports')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Given the potentially large number of MSAs, we'll focus on the top 10 MSAs by count of telecommuting reports for clarity in visualization.\n\n# Identifying the top 10 MSAs by count of telecommuting reports\n# top_msas = filtered_data['EST_MSA'].value_counts().nlargest(10).index\n\n# # Filtering data for top 10 MSAs\n# top_msa_data = filtered_data[filtered_data['EST_MSA'].isin(top_msas)]\n\n# # Visualizing Telecommuting by MSA\n# plt.figure(figsize=(14, 8))\n# sns.countplot(y='EST_MSA', hue='TWDAYS', data=top_msa_data, palette='coolwarm', order=top_msas)\n# plt.title('Telecommuting by Metropolitan Statistical Area (Top 10 MSAs)')\n# plt.xlabel('Count')\n# plt.ylabel('MSA')\n# plt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\n# plt.tight_layout()\n\n# plt.show()\n\nplt.figure(figsize=(15, 8))\ncity_counts = filtered_data['EST_MSA'].value_counts().sort_index()\nsns.barplot(x=city_counts.index, y=city_counts.values)\nplt.title('MSA-wise Telecommuting Patterns')\nplt.xlabel('MSA Identifier')\nplt.ylabel('Count of Telecommuting Reports')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\nfiltered_data = filtered_data[filtered_data['EEDUC'].isin(range(1, 8))]\neducation_map = {\n    1: \"Less than high school\",\n    2: \"Some high school\",\n    3: \"High school graduate or equivalent (for example GED)\",\n    4: \"Some college, but degree not received or is in progress\",\n    5: \"Associate’s degree (for example AA, AS)\",\n    6: \"Bachelor's degree (for example BA, BS, AB)\",\n    7: \"Graduate degree (for example master's, professional, doctorate)\"\n}\nfiltered_data['EEDUC'] = filtered_data['EEDUC'].map(education_map)\n\n# Now plotting the filtered and mapped data\nplt.figure(figsize=(12, 8))\nsns.countplot(x='EEDUC', hue='TWDAYS_RESP', data=filtered_data)\nplt.title('Telecommuting by Educational Attainment')\nplt.xlabel('Educational Attainment')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfiltered_data = filtered_data[filtered_data['KINDWORK'].isin(range(1, 6))]\n\n# Mapping the 'KINDWORK' codes to the text labels\nwork_kind_map = {\n    1: \"Government\",\n    2: \"Private company\",\n    3: \"Non-profit organization including tax exempt and charitable organizations\",\n    4: \"Self-employed\",\n    5: \"Working in a family business\"\n}\nfiltered_data['KINDWORK'] = filtered_data['KINDWORK'].map(work_kind_map)\n\n# Now plotting the filtered and mapped data\nplt.figure(figsize=(12, 8))\nsns.countplot(x='KINDWORK', hue='TWDAYS_RESP', data=filtered_data)\nplt.title('Telecommuting by Employment Sector')\nplt.xlabel('Employment Sector')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualizing Telecommuting by Total Number of People in the Household\nplt.figure(figsize=(14, 7))\nsns.countplot(x='THHLD_NUMPER', hue='TWDAYS_RESP', data=filtered_data, palette='viridis')\nplt.title('Telecommuting by Total Number of People in the Household')\nplt.xlabel('Total Number of People in Household')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Visualizing Telecommuting by Presence of Children in the Household\n# For simplicity, combining children count variables into a binary presence of children in household variable\nfiltered_data['CHILDREN_PRESENT'] = filtered_data[['KIDS_LT5Y', 'KIDS_5_11Y', 'KIDS_12_17Y']].max(axis=1) &gt; 0\nplt.figure(figsize=(10, 6))\nsns.countplot(x='CHILDREN_PRESENT', hue='TWDAYS_RESP', data=filtered_data, palette='Set2')\nplt.title('Telecommuting by Presence of Children in the Household')\nplt.xlabel('Children Present in Household')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.tight_layout()\n\nplt.show()"
  }
]