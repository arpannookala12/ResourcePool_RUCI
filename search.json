[
  {
    "objectID": "smsm.html",
    "href": "smsm.html",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "",
    "text": "# Python code here\n# Loading Required Libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)"
  },
  {
    "objectID": "smsm.html#reading-the-individual-level-data",
    "href": "smsm.html#reading-the-individual-level-data",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Reading the Individual Level Data",
    "text": "Reading the Individual Level Data\n\n# Read the Household Pulse Survey data from a CSV file\nind &lt;- read.csv(\"hps_04_00_01_puf.csv\")\n# Display the data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#data-filtering-and-variable-selection",
    "href": "smsm.html#data-filtering-and-variable-selection",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Data Filtering and Variable Selection",
    "text": "Data Filtering and Variable Selection\n\n# Filter data for Middlesex County (EST_MSA=35620) and state of New Jersey (EST_ST=34)\nind &lt;- ind %&gt;% filter(EST_MSA==35620 & EST_ST==34)\n\n# Calculate Age from Birth Year\nind$AGE &lt;- 2024 - ind$TBIRTH_YEAR\n\n# Select variables of interest\nvariables_of_interest &lt;- c(\"EST_ST\",\"EST_MSA\", \"MS\", \"RRACE\", \"TWDAYS\", \"EEDUC\", \"KINDWORK\", \"THHLD_NUMPER\", \"INCOME\",\"EGENID_BIRTH\",\"AGE\",\"RHISPANIC\",\"ANYWORK\")\nind &lt;- ind %&gt;% select(all_of(variables_of_interest))\n\n# Display the filtered and selected data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#data-cleaning-and-transformations",
    "href": "smsm.html#data-cleaning-and-transformations",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Data Cleaning and Transformations",
    "text": "Data Cleaning and Transformations\n\n# Display unique values of variables for understanding\nsort(unique(ind$AGE))\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69\n[51] 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 88\n\nsort(unique(ind$EEDUC))\n\n[1] 1 2 3 4 5 6 7\n\nsort(unique(ind$EGENID_BIRTH))\n\n[1] 1 2\n\nsort(unique(ind$KINDWORK))\n\n[1] -99 -88   1   2   3   4   5\n\n# Replace special values with NA in education variable\nind$EEDUC[ind$EEDUC == -99] &lt;- NA\nind$EEDUC[ind$EEDUC == -88] &lt;- NA\n\n# Drop rows with NA values\nind &lt;- ind %&gt;% drop_na()\n\n# Filter data for individuals aged 25 and above\nind &lt;- ind[ind$AGE &gt;= 25,]\n\n# Display cleaned data\nhead(as.data.frame(ind))\n\n\n  \n\n\n# Display unique values again after cleaning\nsort(unique(ind$AGE))\n\n [1] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n[26] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n[51] 75 76 77 78 79 80 81 82 83 84 85 86 88\n\nsort(unique(ind$EEDUC))\n\n[1] 1 2 3 4 5 6 7\n\nsort(unique(ind$EGENID_BIRTH))\n\n[1] 1 2\n\nsort(unique(ind$KINDWORK))\n\n[1] -99 -88   1   2   3   4   5"
  },
  {
    "objectID": "smsm.html#recoding-variables",
    "href": "smsm.html#recoding-variables",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Recoding Variables",
    "text": "Recoding Variables\n\n# Define age group breaks and labels\nbrks &lt;- c(25, 30, 35, 40, 45, 50, 55, 60, 62, 65, 67, 70, 75, 80, 85, Inf)\nlabs &lt;- c(\"25 to 29 years\", \"30 to 34 years\", \"35 to 39 years\", \"40 to 44 years\", \n          \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\", \"60 and 61 years\", \n          \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\", \n          \"75 to 79 years\", \"80 to 84 years\",\"85 years and over\")\n\n# Recode age into groups\nind$AGE &lt;- cut(ind$AGE, breaks = brks, labels = labs, right=FALSE)\n\n# Recode gender variable\nind$EGENID_BIRTH &lt;- factor(ind$EGENID_BIRTH, levels = 1:2, labels = c(\"Male\", \"Female\"))\n\n# Recode education levels\nind$EEDUC &lt;- factor(ind$EEDUC, levels = 1:7, labels = c(\"Less than high school\",\"Some high school\",\"High school graduate or equivalent (for example GED)\",\"Some college, but degree not received or is in progress\",\"Associate’s degree (for example AA, AS)\",\"Bachelor's degree (for example BA, BS, AB)\",\"Graduate degree (for example master's, professional, doctorate)\"))\n\n# Display recoded data\nhead(as.data.frame(ind))"
  },
  {
    "objectID": "smsm.html#reading-and-validating-constraint-data",
    "href": "smsm.html#reading-and-validating-constraint-data",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Reading and Validating Constraint Data",
    "text": "Reading and Validating Constraint Data\n\n# Read constraint data for age, sex, and education from CSV files\ncon_age &lt;- read.csv(\"con_age.csv\")\ncon_sex &lt;- read.csv(\"con_sex.csv\")\ncon_edu &lt;- read.csv(\"con_edu.csv\")\n\n# Display the constraint data\nhead(as.data.frame(con_age))\n\n\n  \n\n\nhead(as.data.frame(con_sex))\n\n\n  \n\n\nhead(as.data.frame(con_edu))\n\n\n  \n\n\n# Validate the sums of the constraints\n# Sum of age constraints\nsum(con_age)\n\n[1] 591318\n\n# Sum of sex constraints\nsum(con_sex)\n\n[1] 591318\n\n# Sum of education constraints\nsum(con_edu)\n\n[1] 591318\n\n# Validate the row sums of the constraints\n# Row sums of age constraints\nrowSums(con_age)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Row sums of sex constraints\nrowSums(con_sex)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Row sums of education constraints\nrowSums(con_edu)\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of different constraints are equal\n# Check if row sums of age and sex constraints are equal\nrowSums(con_age) == rowSums(con_sex)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n# Check if row sums of age and education constraints are equal\nrowSums(con_age) == rowSums(con_edu)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n# Check if row sums of sex and education constraints are equal\nrowSums(con_sex) == rowSums(con_edu)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[511] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[526] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[556] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "smsm.html#naming-columns-and-combining-constraints",
    "href": "smsm.html#naming-columns-and-combining-constraints",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Naming Columns and Combining Constraints",
    "text": "Naming Columns and Combining Constraints\n\n# Assign column names to age and education constraints based on the levels in the individual data\nnames(con_age) &lt;- levels(ind$AGE)\nnames(con_edu) &lt;- levels(ind$EEDUC)\n\n# Combine age, sex, and education constraints into a single data frame\ncons &lt;- cbind(con_age, con_sex, con_edu)\n\n# Display the combined constraints\nhead(as.data.frame(cons))"
  },
  {
    "objectID": "smsm.html#creating-categorical-indicator-matrices",
    "href": "smsm.html#creating-categorical-indicator-matrices",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Creating Categorical Indicator Matrices",
    "text": "Creating Categorical Indicator Matrices\n\n# Create binary indicator matrix for age categories\ncat_age &lt;- model.matrix(~ ind$AGE - 1)\n# Display the dimensions of the age indicator matrix\ndim(cat_age)\n\n[1] 821  15\n\n# Create binary indicator matrix for sex categories\ncat_sex &lt;- model.matrix(~ ind$EGENID_BIRTH - 1)\n# Display the dimensions of the sex indicator matrix\ndim(cat_sex)\n\n[1] 821   2\n\n# Create binary indicator matrix for education categories\ncat_edu &lt;- model.matrix(~ ind$EEDUC - 1)\n# Display the dimensions of the education indicator matrix\ndim(cat_edu)\n\n[1] 821   7\n\n# Combine age, sex, and education indicator matrices into a single matrix\nind_cat &lt;- cbind(cat_age, cat_sex, cat_edu)\n\n# Display the first few rows of the combined indicator matrix\nhead(as.data.frame(ind_cat))"
  },
  {
    "objectID": "smsm.html#aggregating-indicator-matrix-and-defining-dimensions",
    "href": "smsm.html#aggregating-indicator-matrix-and-defining-dimensions",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Aggregating Indicator Matrix and Defining Dimensions",
    "text": "Aggregating Indicator Matrix and Defining Dimensions\n\n# Calculate the column sums of the indicator matrix\ncolSums(ind_cat)\n\n                                                   ind$AGE25 to 29 years \n                                                                      27 \n                                                   ind$AGE30 to 34 years \n                                                                      56 \n                                                   ind$AGE35 to 39 years \n                                                                      79 \n                                                   ind$AGE40 to 44 years \n                                                                      99 \n                                                   ind$AGE45 to 49 years \n                                                                      99 \n                                                   ind$AGE50 to 54 years \n                                                                      76 \n                                                   ind$AGE55 to 59 years \n                                                                      73 \n                                                  ind$AGE60 and 61 years \n                                                                      29 \n                                                   ind$AGE62 to 64 years \n                                                                      64 \n                                                  ind$AGE65 and 66 years \n                                                                      23 \n                                                   ind$AGE67 to 69 years \n                                                                      41 \n                                                   ind$AGE70 to 74 years \n                                                                      75 \n                                                   ind$AGE75 to 79 years \n                                                                      50 \n                                                   ind$AGE80 to 84 years \n                                                                      18 \n                                                ind$AGE85 years and over \n                                                                      12 \n                                                    ind$EGENID_BIRTHMale \n                                                                     400 \n                                                  ind$EGENID_BIRTHFemale \n                                                                     421 \n                                          ind$EEDUCLess than high school \n                                                                      12 \n                                               ind$EEDUCSome high school \n                                                                      21 \n           ind$EEDUCHigh school graduate or equivalent (for example GED) \n                                                                     103 \n        ind$EEDUCSome college, but degree not received or is in progress \n                                                                     141 \n                        ind$EEDUCAssociate’s degree (for example AA, AS) \n                                                                      51 \n                     ind$EEDUCBachelor's degree (for example BA, BS, AB) \n                                                                     256 \nind$EEDUCGraduate degree (for example master's, professional, doctorate) \n                                                                     237 \n\n# Aggregate the individual categories\nind_agg &lt;- colSums(ind_cat)\n\n# Compare the first row of constraints with the aggregated individual categories\nrbind(cons[1,], ind_agg)\n\n\n  \n\n\n# Define the number of zones (rows) in the constraints data\nn_zone &lt;- nrow(cons)\n# Display the number of zones\nn_zone\n\n[1] 566\n\n# Define the number of individuals in the individual data\nn_ind &lt;- nrow(ind)\n# Display the number of individuals\nn_ind\n\n[1] 821\n\n# Define the number of age categories\nn_age &lt;- ncol(con_age)\n# Display the number of age categories\nn_age\n\n[1] 15\n\n# Define the number of sex categories\nn_sex &lt;- ncol(con_sex)\n# Display the number of sex categories\nn_sex\n\n[1] 2\n\n# Define the number of education categories\nn_edu &lt;- ncol(con_edu)\n# Display the number of education categories\nn_edu\n\n[1] 7"
  },
  {
    "objectID": "smsm.html#initializing-weight-matrices-and-performing-first-iteration-of-ipf",
    "href": "smsm.html#initializing-weight-matrices-and-performing-first-iteration-of-ipf",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Initializing Weight Matrices and Performing First Iteration of IPF",
    "text": "Initializing Weight Matrices and Performing First Iteration of IPF\n\n# Initialize weight matrices with uniform weights\nweights &lt;- matrix(data = 1, nrow = nrow(ind), ncol = nrow(cons))\nweights3 &lt;- weights1 &lt;- weights2 &lt;- weights\n# Display the dimensions of the weights matrix\ndim(weights)\n\n[1] 821 566\n\n# Aggregate the individual categories initially, based on constraints\nind_agg0 &lt;- t(apply(cons, 1, function(x) 1 * ind_agg))\ncolnames(ind_agg0) &lt;- names(cons)\n# Display the initial aggregated individual categories\nhead(as.data.frame(ind_agg0))\n\n\n  \n\n\n# First iteration of IPF to adjust weights based on age constraints\nfor(j in 1:n_zone){\n  for(i in 1:n_age){\n    index &lt;- ind_cat[, i] == 1\n    weights1[index, j] &lt;- weights[index, j] * con_age[j, i] / ind_agg0[j, i]\n  }\n}\n# Display the adjusted weights after the first iteration\nhead(as.data.frame(weights1))"
  },
  {
    "objectID": "smsm.html#aggregating-after-first-ipf-iteration",
    "href": "smsm.html#aggregating-after-first-ipf-iteration",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Aggregating After First IPF Iteration",
    "text": "Aggregating After First IPF Iteration\n\n# Initialize aggregated matrices to store results of each IPF iteration\nind_agg3 &lt;- ind_agg2 &lt;- ind_agg1 &lt;- ind_agg0 * NA\n\n# Aggregate individual categories after the first IPF iteration\nfor(i in 1:n_zone){\n  ind_agg1[i, ] &lt;- colSums(ind_cat * weights1[, i])\n}\n# Display the aggregated individual categories after the first iteration\nhead(as.data.frame(ind_agg1))\n\n\n  \n\n\n# Calculate the row sums of the aggregated categories and the constraints for age\nrowSums(ind_agg1[, 1:15])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\nrowSums(cons[, 1:15])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for age\nrowSums(ind_agg1[, 1:15]) == rowSums(cons[, 1:15])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[565]  TRUE  TRUE"
  },
  {
    "objectID": "smsm.html#second-iteration-of-ipf-and-validation",
    "href": "smsm.html#second-iteration-of-ipf-and-validation",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Second Iteration of IPF and Validation",
    "text": "Second Iteration of IPF and Validation\n\n# Second iteration of IPF to adjust weights based on sex and age constraints\nfor (j in 1:n_zone) {\n  for (i in 1:(n_sex + n_age)) {\n    index &lt;- ind_cat[, i] == 1\n    weights2[index, j] &lt;- weights1[index, j] * cons[j, i] / ind_agg1[j, i]\n  }\n}\n# Display the adjusted weights after the second iteration\nhead(as.data.frame(weights2))\n\n\n  \n\n\n# Aggregate individual categories after the second IPF iteration\nfor(i in 1:n_zone){\n  ind_agg2[i, ] &lt;- colSums(ind_cat * weights2[, i])\n}\n# Display the aggregated individual categories after the second iteration\nhead(as.data.frame(ind_agg2))\n\n\n  \n\n\n# Calculate and display the row sums of the aggregated categories for sex\nrowSums(ind_agg2[, 16:17])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Calculate and display the row sums of the constraints for sex\nrowSums(cons[, 16:17])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for sex\nrowSums(ind_agg2[, 16:17]) == rowSums(cons[, 16:17])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[565]  TRUE  TRUE"
  },
  {
    "objectID": "smsm.html#third-iteration-of-ipf-and-final-validation",
    "href": "smsm.html#third-iteration-of-ipf-and-final-validation",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Third Iteration of IPF and Final Validation",
    "text": "Third Iteration of IPF and Final Validation\n\n# Third iteration of IPF to adjust weights based on sex, age, and education constraints\nfor(j in 1:n_zone){\n  for(i in 1:(n_sex + n_age + n_edu)){\n    index &lt;- ind_cat[, i] == 1\n    if(ind_agg2[j, i] != 0) {  # Check to avoid division by zero\n      weights3[index, j] &lt;- weights2[index, j] * cons[j, i] / ind_agg2[j, i]\n    }\n  }\n}\n# Display the adjusted weights after the third iteration\nhead(as.data.frame(weights3))\n\n\n  \n\n\n# Aggregate individual categories after the third IPF iteration\nfor(i in 1:n_zone){\n  ind_agg3[i, ] &lt;- colSums(ind_cat * weights3[, i])\n}\n# Display the aggregated individual categories after the third iteration\nhead(as.data.frame(ind_agg3))\n\n\n  \n\n\n# Calculate and display the row sums of the aggregated categories for education\nrowSums(ind_agg3[, 18:24])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   61 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  117  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   24  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Calculate and display the row sums of the constraints for education\nrowSums(cons[, 18:24])\n\n  [1] 1031 1435  946 1615  520  117 1381  629 2265  736 1602  954 1022 1367  823\n [16]  819  629 1493 1280 1523  450  710 1371 1964 1203  942 1166  431 1552 1333\n [31] 1704  503  987  541 1271 1211  388 1457 2439  511 1124  744 1110  494 1654\n [46]  964 1126  579  962 1253  741 1704  856  877  717  458  460 2395  581 1553\n [61] 1861  610 1007 1330  825 1734  886  449  993 1658  850 1244  914  693 1184\n [76]  812 1200 1809  733  888 1051 1140 1476 1453 1043 1184 1050  628  837 1131\n [91]  665  387 1449  719  435  838  634 1193 1368  760  478   23 2161  820 1184\n[106]  736 1286  559  688 1564  922  702 1437 1780 1180 1584  539 1001  806  803\n[121]  478  612  565 1505 1324  575 1482  572  983  575 1148  878 1027  942 1339\n[136] 1120  921  618 1679  619   26   90 1252 1011 1376  511  547  558  665  844\n[151] 1268 1871  557  992  613  901  748  618 1198 1096  567 1132 1426 1167 1310\n[166] 1462 1805 1045  378  492  661 1149  897  927 1080  821  920 2294 1411  782\n[181]  811 1537  510  736  521  573 1553 1370 1344  794  507  453 1661 1654  500\n[196]  901  894  768 1730  475  887  614 1100 1527  753 1405  975 1250  827  716\n[211]  492  929 1480 1221  920 1140  376  935 1198  431  758  580 1201 2201  884\n[226]  982  958  531  451 1060  818  802 1618 1922 1606  757  945  777 1146  991\n[241]  982  688 1145 1384  203 2486  886 1021 1283 1209 1121  486 1134  686  784\n[256] 1085  967  824  391 1252  640 1988  775 1379  889  674  419  645 1173  834\n[271] 1075  836 2283 1993  368 1338  544 1248 1372  372  476  193  530  933  756\n[286]  282  103  858  866  910 1501 1338  854 1337  607 1156  695  967 1631  947\n[301]  789 1372  620 1788 1085  863  789  704  488  890  295 1069 1307  646 1151\n[316]  912  937 1277 1309  912 1404  966 2266  832  785  750 1066  907 1694  229\n[331] 1516 1815 1692  866  437  714  748  914  605  844 1250  690  826 1105 1080\n[346] 1096  961 1495  916  868 1736 1782 1695  843 2051  852 1196 1207  889 1059\n[361] 1864 1170 1464  972  523  427 1053 1308  812 1062  556  780  401  433  873\n[376]  703  711 1181 1608 2065 1633  536  479  619 1277 1166 1245  170 2319  658\n[391] 1950  522 1330 1506  942  942 1361 1305 1001 1606 1292 1045  798  403  918\n[406] 1200  695 1095  523  755  851  897 1174  625 1185 2224 1137 1713 1289 1238\n[421]  851 1125 1034  622  710  536 1841 1690  751 1318  825  970 1901  965  776\n[436]  870  993 1459 1832 1616 1384 1133 1666 1254  889 1897 1462 1241  469  994\n[451] 1445  745 1849  350 1524  500  723  509 1158  955  638 2051 1116  580 1341\n[466]  984 1263  618  786  871 1053  917  969  993  465  810  238 1101 1672 1243\n[481]  975  487 1162 1588 1316  830 1706 1622 1136 1263  764 2339 1230 1908 1522\n[496] 2502  463 1547 1886 1224  934 1374 1888 1183  684 1145 2033 1491  746  723\n[511] 1134 2189  394 1122 1596 1060   64 1458 1638 1361  860 1155  923 1163 1693\n[526] 1166 1198 1871  777  621 1201 2353  668  876  893 1709 1006 1025  382 1248\n[541] 1447 1779 1115   19  668   52  725 3240 1123  916 1065  759 1210  809 1336\n[556]  303  271  800  605 1183 1667  561  611 1197   21  645\n\n# Check if the row sums of the aggregated categories match the constraints for education\nrowSums(ind_agg3[, 18:24]) == rowSums(cons[, 18:24])\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[325]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[433] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[445]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[505]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[541]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[565]  TRUE FALSE"
  },
  {
    "objectID": "smsm.html#correlation-analysis-between-aggregated-data-and-constraints",
    "href": "smsm.html#correlation-analysis-between-aggregated-data-and-constraints",
    "title": "Spatial Microsimulation for Middlesex County, NJ",
    "section": "Correlation Analysis Between Aggregated Data and Constraints",
    "text": "Correlation Analysis Between Aggregated Data and Constraints\n\n# Function to convert matrix to numeric vector\nvec &lt;- function(x) as.numeric(as.matrix(x))\n\n# Calculate and display the correlation between the initial aggregated data and constraints\ncor(vec(ind_agg0), vec(cons))\n\n[1] 0.723838\n\n# Calculate and display the correlation between the first iteration aggregated data and constraints\ncor(vec(ind_agg1), vec(cons))\n\n[1] 0.9052638\n\n# Calculate and display the correlation between the second iteration aggregated data and constraints\ncor(vec(ind_agg2), vec(cons))\n\n[1] 0.9197018\n\n# Calculate and display the correlation between the third iteration aggregated data and constraints\ncor(vec(ind_agg3), vec(cons))\n\n[1] 0.9967264"
  },
  {
    "objectID": "pums_with_dummies.html",
    "href": "pums_with_dummies.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,roc_curve,precision_recall_curve, average_precision_score,\n                           mean_squared_error, mean_absolute_error, r2_score, accuracy_score)\nfrom sklearn.calibration import calibration_curve\nimport statsmodels.api as sm\nimport shap\n\n\ndf1 = pd.read_csv(\"psam_h34.csv\")\ndf2 = pd.read_csv(\"psam_p34.csv\")\nmerged_df = pd.merge(df1, df2, on='SERIALNO', how='inner')\n\n\nmerged_df\n\n\n  \n    \n\n\n\n\n\n\nRT_x\nSERIALNO\nDIVISION_x\nPUMA_x\nREGION_x\nSTATE_x\nADJHSG\nADJINC_x\nWGTP\nNP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nH\n2023GQ0000001\n2\n2501\n1\n34\n1000000\n1019518\n0\n1\n...\n68\n66\n67\n69\n69\n67\n68\n71\n65\n67\n\n\n1\nH\n2023GQ0000002\n2\n603\n1\n34\n1000000\n1019518\n0\n1\n...\n4\n49\n94\n78\n66\n45\n56\n50\n39\n84\n\n\n2\nH\n2023GQ0000021\n2\n2103\n1\n34\n1000000\n1019518\n0\n1\n...\n14\n12\n12\n11\n15\n15\n14\n12\n12\n12\n\n\n3\nH\n2023GQ0000058\n2\n307\n1\n34\n1000000\n1019518\n0\n1\n...\n3\n2\n2\n4\n4\n2\n4\n2\n4\n2\n\n\n4\nH\n2023GQ0000158\n2\n1201\n1\n34\n1000000\n1019518\n0\n1\n...\n2\n2\n20\n20\n16\n24\n29\n3\n16\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n94309\nH\n2023HU1459771\n2\n907\n1\n34\n1000000\n1019518\n67\n2\n...\n146\n120\n94\n67\n69\n47\n57\n76\n23\n117\n\n\n94310\nH\n2023HU1459772\n2\n904\n1\n34\n1000000\n1019518\n64\n1\n...\n63\n18\n21\n66\n63\n64\n60\n75\n97\n18\n\n\n94311\nH\n2023HU1459852\n2\n602\n1\n34\n1000000\n1019518\n78\n2\n...\n21\n83\n20\n21\n146\n114\n77\n80\n90\n18\n\n\n94312\nH\n2023HU1459852\n2\n602\n1\n34\n1000000\n1019518\n78\n2\n...\n32\n84\n25\n19\n149\n118\n90\n111\n84\n19\n\n\n94313\nH\n2023HU1459867\n2\n2002\n1\n34\n1000000\n1019518\n50\n1\n...\n89\n15\n93\n83\n44\n43\n15\n13\n78\n45\n\n\n\n\n94314 rows × 527 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmerged_df['JWTRNS'] = (merged_df['JWTRNS'] == 11).astype(int)\nprint(merged_df['JWTRNS'].value_counts())\n\nJWTRNS\n0    87181\n1     7133\nName: count, dtype: int64\n\n\n\nall_cols = ['ACCESSINET', 'AGEP', 'BROADBND', 'COW', 'CPLT', 'DDRS', 'DEAR', 'DEYE', 'DIALUP', 'DOUT', 'DPHY', 'DREM', 'ESR', 'GCL', 'HFL', 'HHL', 'HHLDRAGEP', 'HHLDRHISP', 'HHT', 'HICOV', 'HINCP', 'HISP', 'HISPEED', 'HUGCL', 'HUPAC', 'HUPAOC', 'HUPARC', 'INDP', 'JWAP', 'JWDP', 'JWMNP', 'JWRIP', 'LANX', 'LAPTOP', 'MAR', 'MIG', 'PRIVCOV', 'PUBCOV', 'POWPUMA', 'R18', 'R60', 'R65', 'RAC1P', 'SATELLITE', 'SCHL', 'SMARTPHONE', 'TABLET', 'TEL', 'WIF', 'WKEXREL', 'WKHP', 'WKL', 'WKWN', 'WORKSTAT', 'WRK', 'JWTRNS']\n\n# First filter for JWTRNS values of 0 and 1\nfiltered_df = merged_df[merged_df['JWTRNS'].isin([0, 1])]\n\n# Calculate percentage of NaN values for each column\nnan_percentages = (filtered_df[all_cols].isna().sum() / len(filtered_df)) * 100\n\n# Identify columns with less than 20% NaN values\ncols_to_keep = nan_percentages[nan_percentages &lt; 20].index.tolist()\n\n# Print columns being dropped due to high NaN percentage\ndropped_cols = set(all_cols) - set(cols_to_keep)\nprint(\"\\nColumns dropped due to &gt;20% NaN values:\")\nfor col in dropped_cols:\n    print(f\"{col}: {nan_percentages[col]:.2f}% NaN\")\n\n# Select columns with acceptable NaN levels\ndf_cleaned = filtered_df[cols_to_keep]\n\n# Before dropping NaN rows, check class distribution\nprint(\"\\nClass distribution before dropping NaN rows:\")\nprint(df_cleaned['JWTRNS'].value_counts(normalize=True))\n\n# Drop rows with NaN values\ndf_final = df_cleaned.dropna()\n\n# Check final class distribution\nprint(\"\\nClass distribution after dropping NaN rows:\")\nprint(df_final['JWTRNS'].value_counts(normalize=True))\n\n# Print shape at each step\nprint(\"\\nShape at each step:\")\nprint(f\"Original: {merged_df.shape}\")\nprint(f\"After filtering JWTRNS: {filtered_df.shape}\")\nprint(f\"After dropping high-NaN columns: {df_cleaned.shape}\")\nprint(f\"Final after dropping NaN rows: {df_final.shape}\")\n\n# If you want to proceed with creating X and y:\nX = df_final.drop('JWTRNS', axis=1)\ny = df_final['JWTRNS']\n\n# Optional: Print final number of records for each class\nprint(\"\\nFinal number of records per class:\")\nprint(y.value_counts())\n\n\nColumns dropped due to &gt;20% NaN values:\nCOW: 37.87% NaN\nWKWN: 45.27% NaN\nGCL: 32.15% NaN\nWRK: 28.03% NaN\nCPLT: 28.92% NaN\nJWRIP: 64.80% NaN\nINDP: 37.87% NaN\nJWMNP: 58.33% NaN\nJWAP: 58.33% NaN\nJWDP: 58.33% NaN\nWKHP: 45.27% NaN\nPOWPUMA: 50.76% NaN\n\nClass distribution before dropping NaN rows:\nJWTRNS\n0    0.92437\n1    0.07563\nName: proportion, dtype: float64\n\nClass distribution after dropping NaN rows:\nJWTRNS\n0    0.90297\n1    0.09703\nName: proportion, dtype: float64\n\nShape at each step:\nOriginal: (94314, 527)\nAfter filtering JWTRNS: (94314, 527)\nAfter dropping high-NaN columns: (94314, 44)\nFinal after dropping NaN rows: (59590, 44)\n\nFinal number of records per class:\nJWTRNS\n0    53808\n1     5782\nName: count, dtype: int64\n\n\n\n# Define known continuous columns\nKNOWN_CONTINUOUS = [\n    'HINCP',      # Household income\n    'AGEP',       # Age\n    'HHLDRAGEP'   # Householder age\n]\n\ndef identify_continuous_columns(df, known_continuous, unique_threshold=0.05):\n    \"\"\"\n    Identify continuous columns combining known continuous variables with automatic detection\n    \"\"\"\n    continuous_columns = known_continuous.copy()  # Start with known continuous columns\n    n_rows = len(df)\n\n    # Check remaining columns for potential continuous variables\n    remaining_columns = [col for col in df.columns if col not in known_continuous]\n\n    for column in remaining_columns:\n        # Try to convert to numeric\n        series = pd.to_numeric(df[column], errors='coerce')\n        n_unique = series.nunique()\n\n        # Calculate ratio of unique values\n        unique_ratio = n_unique / n_rows\n\n        # Conditions for continuous variable\n        if (series.dtype in ['int64', 'float64'] and\n            unique_ratio &gt; unique_threshold and\n            n_unique &gt; 2):\n            continuous_columns.append(column)\n\n    print(\"\\nContinuous columns:\")\n    print(\"Known continuous columns:\")\n    for col in known_continuous:\n        if col in df.columns:\n            print(f\"- {col}: {df[col].nunique()} unique values\")\n\n    print(\"\\nAutomatically identified continuous columns:\")\n    auto_continuous = [col for col in continuous_columns if col not in known_continuous]\n    for col in auto_continuous:\n        print(f\"- {col}: {df[col].nunique()} unique values\")\n\n    return continuous_columns\n\ndef create_dummies_with_base(df, known_continuous, unique_threshold=0.05):\n    \"\"\"\n    Create dummy variables while handling both known and automatically detected continuous features\n    \"\"\"\n    print(\"\\nAnalyzing and creating dummy variables...\")\n    df_processed = df.copy()\n\n    # Identify all continuous columns\n    continuous_columns = identify_continuous_columns(df_processed, known_continuous, unique_threshold)\n    dummies_list = []\n\n    for column in df_processed.columns:\n        if column in continuous_columns:\n            # Handle continuous variables\n            print(f\"\\nProcessing continuous variable: {column}\")\n            df_processed[column] = pd.to_numeric(df_processed[column], errors='coerce')\n            dummies_list.append(df_processed[[column]])\n        else:\n            # Handle categorical variables\n            print(f\"\\nProcessing categorical variable: {column}\")\n            df_temp = df_processed[column].copy()\n\n            # Convert to string and handle special cases\n            df_temp = df_temp.map(lambda x: str(x) if pd.notnull(x) else np.nan)\n\n            # Create dummies with drop_first=True\n            dummies = pd.get_dummies(df_temp, prefix=f\"{column}_dummy\", drop_first=True)\n            dummies_list.append(dummies)\n\n            if len(dummies.columns) &gt; 0:\n                base_category = df_temp.dropna().unique()[0]\n                print(f\"- Dropped base category for {column}: {base_category}\")\n                print(f\"- Created {len(dummies.columns)} dummy variables\")\n\n    # Combine all processed columns\n    final_df = pd.concat(dummies_list, axis=1)\n\n    # Print summary statistics\n    print(\"\\nSummary:\")\n    print(f\"Original shape: {df.shape}\")\n    print(f\"Final shape: {final_df.shape}\")\n    print(f\"Known continuous variables: {len(known_continuous)}\")\n    print(f\"Total continuous variables: {len(continuous_columns)}\")\n    print(f\"Categorical variables: {len(df.columns) - len(continuous_columns)}\")\n\n    return final_df, continuous_columns\n\n# Create dummies with combined feature detection\nX_with_dummies, identified_continuous = create_dummies_with_base(X, KNOWN_CONTINUOUS)\n\n# Standardize continuous features\nscaler = StandardScaler()\nfor column in identified_continuous:\n    if column in X_with_dummies.columns:\n        col_data = X_with_dummies[[column]].fillna(X_with_dummies[column].median())\n        X_with_dummies[column] = scaler.fit_transform(col_data)\n\nprint(f\"\\nFinal shape after standardization: {X_with_dummies.shape}\")\n\n\nAnalyzing and creating dummy variables...\n\nContinuous columns:\nKnown continuous columns:\n- HINCP: 5493 unique values\n- AGEP: 76 unique values\n- HHLDRAGEP: 76 unique values\n\nAutomatically identified continuous columns:\n\nProcessing categorical variable: ACCESSINET\n\nProcessing continuous variable: AGEP\n\nProcessing categorical variable: BROADBND\n- Dropped base category for BROADBND: 1.0\n- Created 1 dummy variables\n\nProcessing categorical variable: DDRS\n- Dropped base category for DDRS: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: DEAR\n- Dropped base category for DEAR: 2\n- Created 1 dummy variables\n\nProcessing categorical variable: DEYE\n- Dropped base category for DEYE: 2\n- Created 1 dummy variables\n\nProcessing categorical variable: DIALUP\n- Dropped base category for DIALUP: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: DOUT\n- Dropped base category for DOUT: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: DPHY\n- Dropped base category for DPHY: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: DREM\n- Dropped base category for DREM: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: ESR\n- Dropped base category for ESR: 1.0\n- Created 5 dummy variables\n\nProcessing categorical variable: HFL\n- Dropped base category for HFL: 1.0\n- Created 8 dummy variables\n\nProcessing categorical variable: HHL\n- Dropped base category for HHL: 3.0\n- Created 4 dummy variables\n\nProcessing continuous variable: HHLDRAGEP\n\nProcessing categorical variable: HHLDRHISP\n- Dropped base category for HHLDRHISP: 1.0\n- Created 23 dummy variables\n\nProcessing categorical variable: HHT\n- Dropped base category for HHT: 1.0\n- Created 2 dummy variables\n\nProcessing categorical variable: HICOV\n- Dropped base category for HICOV: 1\n- Created 1 dummy variables\n\nProcessing continuous variable: HINCP\n\nProcessing categorical variable: HISP\n- Dropped base category for HISP: 1\n- Created 23 dummy variables\n\nProcessing categorical variable: HISPEED\n- Dropped base category for HISPEED: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: HUGCL\n- Dropped base category for HUGCL: 0.0\n- Created 1 dummy variables\n\nProcessing categorical variable: HUPAC\n- Dropped base category for HUPAC: 4.0\n- Created 3 dummy variables\n\nProcessing categorical variable: HUPAOC\n- Dropped base category for HUPAOC: 4.0\n- Created 3 dummy variables\n\nProcessing categorical variable: HUPARC\n- Dropped base category for HUPARC: 4.0\n- Created 3 dummy variables\n\nProcessing categorical variable: LANX\n- Dropped base category for LANX: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: LAPTOP\n- Dropped base category for LAPTOP: 1.0\n- Created 1 dummy variables\n\nProcessing categorical variable: MAR\n- Dropped base category for MAR: 1\n- Created 4 dummy variables\n\nProcessing categorical variable: MIG\n- Dropped base category for MIG: 1.0\n- Created 2 dummy variables\n\nProcessing categorical variable: PRIVCOV\n- Dropped base category for PRIVCOV: 1\n- Created 1 dummy variables\n\nProcessing categorical variable: PUBCOV\n- Dropped base category for PUBCOV: 2\n- Created 1 dummy variables\n\nProcessing categorical variable: R18\n- Dropped base category for R18: 0.0\n- Created 1 dummy variables\n\nProcessing categorical variable: R60\n- Dropped base category for R60: 1.0\n- Created 2 dummy variables\n\nProcessing categorical variable: R65\n- Dropped base category for R65: 0.0\n- Created 2 dummy variables\n\nProcessing categorical variable: RAC1P\n- Dropped base category for RAC1P: 1\n- Created 8 dummy variables\n\nProcessing categorical variable: SATELLITE\n- Dropped base category for SATELLITE: 2.0\n- Created 1 dummy variables\n\nProcessing categorical variable: SCHL\n- Dropped base category for SCHL: 22.0\n- Created 23 dummy variables\n\nProcessing categorical variable: SMARTPHONE\n- Dropped base category for SMARTPHONE: 1.0\n- Created 1 dummy variables\n\nProcessing categorical variable: TABLET\n- Dropped base category for TABLET: 1.0\n- Created 1 dummy variables\n\nProcessing categorical variable: TEL\n- Dropped base category for TEL: 1.0\n- Created 1 dummy variables\n\nProcessing categorical variable: WIF\n- Dropped base category for WIF: 3.0\n- Created 3 dummy variables\n\nProcessing categorical variable: WKEXREL\n- Dropped base category for WKEXREL: 5.0\n- Created 14 dummy variables\n\nProcessing categorical variable: WKL\n- Dropped base category for WKL: 1.0\n- Created 2 dummy variables\n\nProcessing categorical variable: WORKSTAT\n- Dropped base category for WORKSTAT: 1.0\n- Created 14 dummy variables\n\nSummary:\nOriginal shape: (59590, 43)\nFinal shape: (59590, 171)\nKnown continuous variables: 3\nTotal continuous variables: 3\nCategorical variables: 40\n\nFinal shape after standardization: (59590, 171)\n\n\n\n# First, let's convert any boolean columns to numeric in X_with_dummies\nfor col in X_with_dummies.columns:\n    if X_with_dummies[col].dtype == bool:\n        X_with_dummies[col] = X_with_dummies[col].astype(int)\n    elif X_with_dummies[col].dtype == 'object':\n        X_with_dummies[col] = pd.to_numeric(X_with_dummies[col], errors='coerce')\n\n\nX_with_dummies\n\n\n  \n    \n\n\n\n\n\n\nAGEP\nBROADBND_dummy_2.0\nDDRS_dummy_2.0\nDEAR_dummy_2\nDEYE_dummy_2\nDIALUP_dummy_2.0\nDOUT_dummy_2.0\nDPHY_dummy_2.0\nDREM_dummy_2.0\nESR_dummy_2.0\n...\nWORKSTAT_dummy_14.0\nWORKSTAT_dummy_15.0\nWORKSTAT_dummy_2.0\nWORKSTAT_dummy_3.0\nWORKSTAT_dummy_4.0\nWORKSTAT_dummy_5.0\nWORKSTAT_dummy_6.0\nWORKSTAT_dummy_7.0\nWORKSTAT_dummy_8.0\nWORKSTAT_dummy_9.0\n\n\n\n\n3350\n0.665865\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3351\n0.449724\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3352\n-1.279404\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3353\n-1.441509\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3355\n-0.522910\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n94305\n0.395689\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n94306\n0.449724\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n94307\n-1.387474\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n94308\n0.936041\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n94309\n0.611830\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n59590 rows × 171 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_with_dummies, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nFinal shapes after VIF and split:\")\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"X_test: {X_test.shape}\")\n\n\nFinal shapes after VIF and split:\nX_train: (47672, 171)\nX_test: (11918, 171)\n\n\n\nimport statsmodels.api as sm\nimport numpy as np\n\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# 1. Remove low variance columns\nvariance_threshold = 0.01  # Adjust this value as needed\nvariances = X_train.var()\nlow_variance_columns = variances[variances &lt; variance_threshold].index\nX_train_filtered = X_train.drop(columns=low_variance_columns)\nprint(f\"Removed {len(low_variance_columns)} low variance columns\")\n\n# 2. Handle perfect multicollinearity\n# Drop highly correlated features\ndef remove_highly_correlated_features(df, threshold=0.95):\n    corr_matrix = df.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] &gt; threshold)]\n    return df.drop(columns=to_drop), to_drop\n\nX_train_filtered, dropped_corr = remove_highly_correlated_features(X_train_filtered)\nprint(f\"Removed {len(dropped_corr)} highly correlated features\")\n\n# 3. Add constant and fit model\nX_train_probit = sm.add_constant(X_train_filtered)\n\n# 4. Ensure all data is numeric\nX_train_probit = X_train_probit.astype(float)\n\n# 5. Fit Probit model with robust covariance\ntry:\n    probit_model = sm.Probit(y_train, X_train_probit)\n    probit_results = probit_model.fit(method='newton', cov_type='HC0')\n\n    print(\"\\nProbit Model Summary:\")\n    print(probit_results.summary())\n\n    # Calculate marginal effects\n    print(\"\\nMarginal Effects (Probit):\")\n    marginal_effects = probit_results.get_margeff(at='overall')\n    print(marginal_effects.summary())\n\n    # Print top significant variables\n    significant_vars = pd.DataFrame({\n        'Variable': X_train_probit.columns,\n        'Coefficient': probit_results.params,\n        'Std Error': probit_results.bse,\n        'Z-Score': probit_results.tvalues,\n        'P-Value': probit_results.pvalues\n    }).sort_values('P-Value')\n\n    print(\"\\nTop 10 Most Significant Variables:\")\n    print(significant_vars.head(10))\n\nexcept Exception as e:\n    print(f\"Error in model fitting: {str(e)}\")\n    print(\"\\nData shape:\", X_train_probit.shape)\n    print(\"\\nNumber of NaN values:\", X_train_probit.isna().sum().sum())\n\n\nFitting Probit model for effect sizes and p-values...\nRemoved 64 low variance columns\nRemoved 4 highly correlated features\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.253002\n         Iterations: 35\n\nProbit Model Summary:\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 JWTRNS   No. Observations:                47672\nModel:                         Probit   Df Residuals:                    47570\nMethod:                           MLE   Df Model:                          101\nDate:                Thu, 06 Feb 2025   Pseudo R-squ.:                  0.2057\nTime:                        08:32:57   Log-Likelihood:                -12061.\nconverged:                      False   LL-Null:                       -15185.\nCovariance Type:                  HC0   LLR p-value:                     0.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -1.7055      0.374     -4.562      0.000      -2.438      -0.973\nAGEP                     0.0670      0.024      2.738      0.006       0.019       0.115\nBROADBND_dummy_2.0       0.0175      0.061      0.284      0.776      -0.103       0.138\nDDRS_dummy_2.0           0.1390      0.173      0.801      0.423      -0.201       0.479\nDEAR_dummy_2             0.0425      0.078      0.547      0.585      -0.110       0.195\nDEYE_dummy_2             0.1593      0.093      1.717      0.086      -0.023       0.341\nDIALUP_dummy_2.0         0.1160      0.070      1.662      0.097      -0.021       0.253\nDOUT_dummy_2.0          -0.3907      0.114     -3.441      0.001      -0.613      -0.168\nDPHY_dummy_2.0           0.0479      0.085      0.561      0.575      -0.119       0.215\nDREM_dummy_2.0          -0.0076      0.091     -0.084      0.933      -0.187       0.171\nESR_dummy_2.0           -5.2626   2.35e+06  -2.24e-06      1.000   -4.61e+06    4.61e+06\nESR_dummy_3.0           -6.0468      0.102    -59.231      0.000      -6.247      -5.847\nESR_dummy_6.0           -6.9979      0.119    -58.696      0.000      -7.232      -6.764\nHFL_dummy_2.0           -0.0533      0.056     -0.955      0.339      -0.163       0.056\nHFL_dummy_3.0           -0.0272      0.031     -0.888      0.375      -0.087       0.033\nHFL_dummy_4.0           -0.0713      0.036     -1.961      0.050      -0.143    -4.5e-05\nHHL_dummy_2.0           -0.0691      0.050     -1.373      0.170      -0.168       0.030\nHHL_dummy_3.0            0.0639      0.044      1.451      0.147      -0.022       0.150\nHHL_dummy_4.0           -0.0389      0.054     -0.718      0.473      -0.145       0.067\nHHL_dummy_5.0           -0.0476      0.071     -0.670      0.503      -0.187       0.092\nHHLDRAGEP                0.0182      0.022      0.845      0.398      -0.024       0.060\nHHLDRHISP_dummy_16.0     0.0560      0.126      0.443      0.658      -0.192       0.304\nHHLDRHISP_dummy_17.0     0.0600      0.158      0.379      0.705      -0.250       0.370\nHHLDRHISP_dummy_19.0    -0.3912      0.194     -2.013      0.044      -0.772      -0.010\nHHLDRHISP_dummy_2.0     -0.2452      0.161     -1.524      0.128      -0.561       0.070\nHHLDRHISP_dummy_3.0      0.0085      0.078      0.109      0.913      -0.144       0.161\nHHLDRHISP_dummy_5.0     -0.0056      0.139     -0.040      0.968      -0.277       0.266\nHHT_dummy_2.0            0.1672   2.16e+10   7.75e-12      1.000   -4.23e+10    4.23e+10\nHHT_dummy_3.0           -0.3132   5.31e+06   -5.9e-08      1.000   -1.04e+07    1.04e+07\nHICOV_dummy_2           -0.0794      0.071     -1.111      0.266      -0.219       0.061\nHINCP                    0.0935      0.009     10.519      0.000       0.076       0.111\nHISP_dummy_16           -0.0519      0.124     -0.418      0.676      -0.295       0.191\nHISP_dummy_17           -0.2335      0.161     -1.455      0.146      -0.548       0.081\nHISP_dummy_19            0.1699      0.172      0.987      0.323      -0.167       0.507\nHISP_dummy_2             0.1094      0.144      0.763      0.446      -0.172       0.391\nHISP_dummy_3            -0.0504      0.077     -0.655      0.513      -0.201       0.100\nHISP_dummy_5             0.0718      0.136      0.529      0.597      -0.194       0.338\nHISPEED_dummy_2.0       -0.1167      0.035     -3.346      0.001      -0.185      -0.048\nHUGCL_dummy_1.0          0.0097      0.056      0.175      0.861      -0.099       0.119\nHUPAC_dummy_2.0         -0.0800      0.104     -0.771      0.441      -0.283       0.123\nHUPAC_dummy_3.0         -0.2080      0.134     -1.557      0.119      -0.470       0.054\nHUPAC_dummy_4.0         -0.0577      0.094     -0.613      0.540      -0.242       0.127\nHUPAOC_dummy_2.0         0.0061      0.109      0.056      0.955      -0.207       0.220\nHUPAOC_dummy_3.0         0.1279      0.140      0.914      0.361      -0.146       0.402\nHUPAOC_dummy_4.0        -0.0343      0.097     -0.352      0.725      -0.225       0.157\nLANX_dummy_2.0           0.0587      0.042      1.412      0.158      -0.023       0.140\nLAPTOP_dummy_2.0        -0.2561      0.052     -4.925      0.000      -0.358      -0.154\nMAR_dummy_2             -0.0736      0.094     -0.786      0.432      -0.257       0.110\nMAR_dummy_3             -0.0375      0.055     -0.687      0.492      -0.145       0.070\nMAR_dummy_4              0.1311      0.094      1.392      0.164      -0.053       0.316\nMAR_dummy_5             -0.0893      0.046     -1.960      0.050      -0.179     1.3e-05\nMIG_dummy_3.0            0.1084      0.036      2.988      0.003       0.037       0.180\nPRIVCOV_dummy_2          0.0136      0.051      0.266      0.790      -0.087       0.114\nPUBCOV_dummy_2           0.0873      0.044      1.989      0.047       0.001       0.173\nR60_dummy_1.0           -0.0303      0.035     -0.858      0.391      -0.099       0.039\nR60_dummy_2.0           -0.0048      0.042     -0.113      0.910      -0.088       0.078\nR65_dummy_1.0            0.0077      0.037      0.204      0.838      -0.066       0.081\nR65_dummy_2.0            0.1076      0.049      2.193      0.028       0.011       0.204\nRAC1P_dummy_2           -0.0499      0.038     -1.325      0.185      -0.124       0.024\nRAC1P_dummy_6            0.1099      0.038      2.866      0.004       0.035       0.185\nRAC1P_dummy_8           -0.0905      0.053     -1.698      0.090      -0.195       0.014\nRAC1P_dummy_9            0.0315      0.038      0.829      0.407      -0.043       0.106\nSATELLITE_dummy_2.0      0.0478      0.052      0.912      0.362      -0.055       0.150\nSCHL_dummy_13.0          0.1440      0.156      0.926      0.355      -0.161       0.449\nSCHL_dummy_14.0         -0.0403      0.138     -0.292      0.770      -0.311       0.230\nSCHL_dummy_15.0          0.1638      0.119      1.373      0.170      -0.070       0.398\nSCHL_dummy_16.0          0.0871      0.084      1.039      0.299      -0.077       0.251\nSCHL_dummy_17.0          0.2057      0.107      1.915      0.055      -0.005       0.416\nSCHL_dummy_18.0          0.1795      0.092      1.947      0.052      -0.001       0.360\nSCHL_dummy_19.0          0.3121      0.086      3.646      0.000       0.144       0.480\nSCHL_dummy_20.0          0.2563      0.089      2.893      0.004       0.083       0.430\nSCHL_dummy_21.0          0.6000      0.082      7.299      0.000       0.439       0.761\nSCHL_dummy_22.0          0.5716      0.084      6.816      0.000       0.407       0.736\nSCHL_dummy_23.0          0.3064      0.093      3.290      0.001       0.124       0.489\nSCHL_dummy_24.0          0.3978      0.098      4.069      0.000       0.206       0.589\nSMARTPHONE_dummy_2.0    -0.0765      0.071     -1.077      0.282      -0.216       0.063\nTABLET_dummy_2.0         0.0026      0.026      0.100      0.921      -0.048       0.053\nWIF_dummy_1.0            0.2502      0.292      0.857      0.391      -0.322       0.822\nWIF_dummy_2.0            0.1825      0.293      0.623      0.533      -0.392       0.757\nWIF_dummy_3.0            0.0286      0.294      0.097      0.922      -0.548       0.605\nWKEXREL_dummy_10.0       0.2867   2.16e+10   1.33e-11      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_11.0       0.0967   2.16e+10   4.48e-12      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_12.0      -0.2163   2.16e+10     -1e-11      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_13.0       0.0062   8.22e+06   7.56e-10      1.000   -1.61e+07    1.61e+07\nWKEXREL_dummy_14.0      -0.0729   8.14e+06  -8.95e-09      1.000    -1.6e+07     1.6e+07\nWKEXREL_dummy_15.0      -0.2465   9.21e+06  -2.68e-08      1.000   -1.81e+07    1.81e+07\nWKEXREL_dummy_2.0       -0.0407      0.031     -1.320      0.187      -0.101       0.020\nWKEXREL_dummy_3.0       -0.1801      0.072     -2.495      0.013      -0.322      -0.039\nWKEXREL_dummy_4.0       -0.0240      0.034     -0.711      0.477      -0.090       0.042\nWKEXREL_dummy_5.0       -0.0269      0.049     -0.547      0.585      -0.123       0.070\nWKEXREL_dummy_6.0       -0.0786      0.087     -0.899      0.369      -0.250       0.093\nWKEXREL_dummy_7.0       -0.1291      0.076     -1.693      0.090      -0.279       0.020\nWKEXREL_dummy_8.0       -0.1110      0.096     -1.162      0.245      -0.298       0.076\nWKEXREL_dummy_9.0       -0.3651      0.232     -1.575      0.115      -0.819       0.089\nWKL_dummy_2.0           -0.8893      0.094     -9.413      0.000      -1.074      -0.704\nWKL_dummy_3.0            1.5457      0.097     15.886      0.000       1.355       1.736\nWORKSTAT_dummy_10.0     -0.5093      0.450     -1.131      0.258      -1.392       0.373\nWORKSTAT_dummy_12.0     -0.1287      0.488     -0.264      0.792      -1.085       0.828\nWORKSTAT_dummy_13.0      0.3465      0.314      1.103      0.270      -0.269       0.962\nWORKSTAT_dummy_15.0      0.4656      0.401      1.160      0.246      -0.321       1.252\nWORKSTAT_dummy_2.0       0.1987      0.096      2.078      0.038       0.011       0.386\nWORKSTAT_dummy_3.0       0.0993      0.063      1.581      0.114      -0.024       0.222\nWORKSTAT_dummy_7.0       0.0596      0.068      0.872      0.383      -0.074       0.194\nWORKSTAT_dummy_9.0       0.1506      0.221      0.681      0.496      -0.283       0.584\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.37 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 JWTRNS\nMethod:                          dydx\nAt:                           overall\n========================================================================================\n                          dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nAGEP                     0.0094        nan        nan        nan         nan         nan\nBROADBND_dummy_2.0       0.0024        nan        nan        nan         nan         nan\nDDRS_dummy_2.0           0.0195        nan        nan        nan         nan         nan\nDEAR_dummy_2             0.0059        nan        nan        nan         nan         nan\nDEYE_dummy_2             0.0223        nan        nan        nan         nan         nan\nDIALUP_dummy_2.0         0.0162        nan        nan        nan         nan         nan\nDOUT_dummy_2.0          -0.0547        nan        nan        nan         nan         nan\nDPHY_dummy_2.0           0.0067        nan        nan        nan         nan         nan\nDREM_dummy_2.0          -0.0011        nan        nan        nan         nan         nan\nESR_dummy_2.0           -0.7365   3.29e+05  -2.24e-06      1.000   -6.45e+05    6.45e+05\nESR_dummy_3.0           -0.8463        nan        nan        nan         nan         nan\nESR_dummy_6.0           -0.9794        nan        nan        nan         nan         nan\nHFL_dummy_2.0           -0.0075        nan        nan        nan         nan         nan\nHFL_dummy_3.0           -0.0038        nan        nan        nan         nan         nan\nHFL_dummy_4.0           -0.0100        nan        nan        nan         nan         nan\nHHL_dummy_2.0           -0.0097        nan        nan        nan         nan         nan\nHHL_dummy_3.0            0.0089        nan        nan        nan         nan         nan\nHHL_dummy_4.0           -0.0054        nan        nan        nan         nan         nan\nHHL_dummy_5.0           -0.0067        nan        nan        nan         nan         nan\nHHLDRAGEP                0.0025        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_16.0     0.0078        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_17.0     0.0084        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_19.0    -0.0548        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_2.0     -0.0343        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_3.0      0.0012        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_5.0     -0.0008        nan        nan        nan         nan         nan\nHHT_dummy_2.0            0.0234   3.02e+09   7.75e-12      1.000   -5.92e+09    5.92e+09\nHHT_dummy_3.0           -0.0438   7.65e+05  -5.73e-08      1.000    -1.5e+06     1.5e+06\nHICOV_dummy_2           -0.0111        nan        nan        nan         nan         nan\nHINCP                    0.0131        nan        nan        nan         nan         nan\nHISP_dummy_16           -0.0073        nan        nan        nan         nan         nan\nHISP_dummy_17           -0.0327        nan        nan        nan         nan         nan\nHISP_dummy_19            0.0238        nan        nan        nan         nan         nan\nHISP_dummy_2             0.0153        nan        nan        nan         nan         nan\nHISP_dummy_3            -0.0071        nan        nan        nan         nan         nan\nHISP_dummy_5             0.0100        nan        nan        nan         nan         nan\nHISPEED_dummy_2.0       -0.0163        nan        nan        nan         nan         nan\nHUGCL_dummy_1.0          0.0014        nan        nan        nan         nan         nan\nHUPAC_dummy_2.0         -0.0112        nan        nan        nan         nan         nan\nHUPAC_dummy_3.0         -0.0291        nan        nan        nan         nan         nan\nHUPAC_dummy_4.0         -0.0081        nan        nan        nan         nan         nan\nHUPAOC_dummy_2.0         0.0009        nan        nan        nan         nan         nan\nHUPAOC_dummy_3.0         0.0179        nan        nan        nan         nan         nan\nHUPAOC_dummy_4.0        -0.0048        nan        nan        nan         nan         nan\nLANX_dummy_2.0           0.0082        nan        nan        nan         nan         nan\nLAPTOP_dummy_2.0        -0.0358        nan        nan        nan         nan         nan\nMAR_dummy_2             -0.0103        nan        nan        nan         nan         nan\nMAR_dummy_3             -0.0052        nan        nan        nan         nan         nan\nMAR_dummy_4              0.0183        nan        nan        nan         nan         nan\nMAR_dummy_5             -0.0125        nan        nan        nan         nan         nan\nMIG_dummy_3.0            0.0152        nan        nan        nan         nan         nan\nPRIVCOV_dummy_2          0.0019        nan        nan        nan         nan         nan\nPUBCOV_dummy_2           0.0122        nan        nan        nan         nan         nan\nR60_dummy_1.0           -0.0042        nan        nan        nan         nan         nan\nR60_dummy_2.0           -0.0007        nan        nan        nan         nan         nan\nR65_dummy_1.0            0.0011        nan        nan        nan         nan         nan\nR65_dummy_2.0            0.0151        nan        nan        nan         nan         nan\nRAC1P_dummy_2           -0.0070        nan        nan        nan         nan         nan\nRAC1P_dummy_6            0.0154        nan        nan        nan         nan         nan\nRAC1P_dummy_8           -0.0127        nan        nan        nan         nan         nan\nRAC1P_dummy_9            0.0044        nan        nan        nan         nan         nan\nSATELLITE_dummy_2.0      0.0067        nan        nan        nan         nan         nan\nSCHL_dummy_13.0          0.0202        nan        nan        nan         nan         nan\nSCHL_dummy_14.0         -0.0056        nan        nan        nan         nan         nan\nSCHL_dummy_15.0          0.0229        nan        nan        nan         nan         nan\nSCHL_dummy_16.0          0.0122        nan        nan        nan         nan         nan\nSCHL_dummy_17.0          0.0288        nan        nan        nan         nan         nan\nSCHL_dummy_18.0          0.0251        nan        nan        nan         nan         nan\nSCHL_dummy_19.0          0.0437        nan        nan        nan         nan         nan\nSCHL_dummy_20.0          0.0359        nan        nan        nan         nan         nan\nSCHL_dummy_21.0          0.0840        nan        nan        nan         nan         nan\nSCHL_dummy_22.0          0.0800        nan        nan        nan         nan         nan\nSCHL_dummy_23.0          0.0429        nan        nan        nan         nan         nan\nSCHL_dummy_24.0          0.0557        nan        nan        nan         nan         nan\nSMARTPHONE_dummy_2.0    -0.0107        nan        nan        nan         nan         nan\nTABLET_dummy_2.0         0.0004        nan        nan        nan         nan         nan\nWIF_dummy_1.0            0.0350        nan        nan        nan         nan         nan\nWIF_dummy_2.0            0.0255        nan        nan        nan         nan         nan\nWIF_dummy_3.0            0.0040        nan        nan        nan         nan         nan\nWKEXREL_dummy_10.0       0.0401   3.02e+09   1.33e-11      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_11.0       0.0135   3.02e+09   4.48e-12      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_12.0      -0.0303   3.02e+09     -1e-11      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_13.0       0.0009   1.15e+06   7.56e-10      1.000   -2.26e+06    2.26e+06\nWKEXREL_dummy_14.0      -0.0102   1.14e+06  -8.98e-09      1.000   -2.23e+06    2.23e+06\nWKEXREL_dummy_15.0      -0.0345   1.28e+06   -2.7e-08      1.000   -2.51e+06    2.51e+06\nWKEXREL_dummy_2.0       -0.0057        nan        nan        nan         nan         nan\nWKEXREL_dummy_3.0       -0.0252        nan        nan        nan         nan         nan\nWKEXREL_dummy_4.0       -0.0034        nan        nan        nan         nan         nan\nWKEXREL_dummy_5.0       -0.0038        nan        nan        nan         nan         nan\nWKEXREL_dummy_6.0       -0.0110        nan        nan        nan         nan         nan\nWKEXREL_dummy_7.0       -0.0181        nan        nan        nan         nan         nan\nWKEXREL_dummy_8.0       -0.0155        nan        nan        nan         nan         nan\nWKEXREL_dummy_9.0       -0.0511        nan        nan        nan         nan         nan\nWKL_dummy_2.0           -0.1245        nan        nan        nan         nan         nan\nWKL_dummy_3.0            0.2163        nan        nan        nan         nan         nan\nWORKSTAT_dummy_10.0     -0.0713        nan        nan        nan         nan         nan\nWORKSTAT_dummy_12.0     -0.0180        nan        nan        nan         nan         nan\nWORKSTAT_dummy_13.0      0.0485        nan        nan        nan         nan         nan\nWORKSTAT_dummy_15.0      0.0652        nan        nan        nan         nan         nan\nWORKSTAT_dummy_2.0       0.0278        nan        nan        nan         nan         nan\nWORKSTAT_dummy_3.0       0.0139        nan        nan        nan         nan         nan\nWORKSTAT_dummy_7.0       0.0083        nan        nan        nan         nan         nan\nWORKSTAT_dummy_9.0       0.0211        nan        nan        nan         nan         nan\n========================================================================================\n\nTop 10 Most Significant Variables:\n                          Variable  Coefficient  Std Error    Z-Score  \\\nESR_dummy_3.0        ESR_dummy_3.0    -6.046823   0.102090 -59.230522   \nESR_dummy_6.0        ESR_dummy_6.0    -6.997910   0.119223 -58.696022   \nWKL_dummy_3.0        WKL_dummy_3.0     1.545731   0.097304  15.885554   \nHINCP                        HINCP     0.093459   0.008884  10.519390   \nWKL_dummy_2.0        WKL_dummy_2.0    -0.889296   0.094473  -9.413236   \nSCHL_dummy_21.0    SCHL_dummy_21.0     0.599952   0.082196   7.299062   \nSCHL_dummy_22.0    SCHL_dummy_22.0     0.571636   0.083865   6.816102   \nLAPTOP_dummy_2.0  LAPTOP_dummy_2.0    -0.256125   0.052010  -4.924543   \nconst                        const    -1.705480   0.373832  -4.562156   \nSCHL_dummy_24.0    SCHL_dummy_24.0     0.397794   0.097751   4.069468   \n\n                       P-Value  \nESR_dummy_3.0     0.000000e+00  \nESR_dummy_6.0     0.000000e+00  \nWKL_dummy_3.0     7.978972e-57  \nHINCP             7.032777e-26  \nWKL_dummy_2.0     4.810921e-21  \nSCHL_dummy_21.0   2.897805e-13  \nSCHL_dummy_22.0   9.354363e-12  \nLAPTOP_dummy_2.0  8.455767e-07  \nconst             5.063111e-06  \nSCHL_dummy_24.0   4.712057e-05  \n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/discrete/discrete_margins.py:343: RuntimeWarning: invalid value encountered in sqrt\n  return cov_me, np.sqrt(np.diag(cov_me))\n\n\n\n# Convert to DMatrix format\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': ['logloss', 'auc'],\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42\n}\n\n# Perform cross-validation\nprint(\"\\nPerforming cross-validation...\")\nnum_round = 1000\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Basic metrics\nprint(\"\\nCalculating performance metrics...\")\naccuracy = accuracy_score(y_test, y_pred_binary)\nroc_auc = roc_auc_score(y_test, y_pred)\n\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\n\nPerforming cross-validation...\n[0] train-auc:0.78588+0.00093   train-error:0.09704+0.00097 test-auc:0.77966+0.00318    test-error:0.09704+0.00386\n[1] train-auc:0.79851+0.00262   train-error:0.09704+0.00097 test-auc:0.79316+0.00286    test-error:0.09704+0.00386\n[2] train-auc:0.80167+0.00155   train-error:0.09704+0.00097 test-auc:0.79622+0.00320    test-error:0.09704+0.00386\n[3] train-auc:0.80285+0.00109   train-error:0.09704+0.00097 test-auc:0.79788+0.00407    test-error:0.09704+0.00386\n[4] train-auc:0.80430+0.00117   train-error:0.09704+0.00097 test-auc:0.79927+0.00456    test-error:0.09704+0.00386\n[5] train-auc:0.80595+0.00114   train-error:0.09704+0.00097 test-auc:0.80082+0.00469    test-error:0.09704+0.00386\n[6] train-auc:0.80759+0.00080   train-error:0.09704+0.00097 test-auc:0.80237+0.00459    test-error:0.09704+0.00386\n[7] train-auc:0.80834+0.00094   train-error:0.09704+0.00097 test-auc:0.80290+0.00421    test-error:0.09704+0.00386\n[8] train-auc:0.80917+0.00134   train-error:0.09704+0.00097 test-auc:0.80309+0.00406    test-error:0.09704+0.00386\n[9] train-auc:0.81059+0.00129   train-error:0.09704+0.00097 test-auc:0.80463+0.00368    test-error:0.09704+0.00386\n[10]    train-auc:0.81155+0.00114   train-error:0.09704+0.00097 test-auc:0.80503+0.00366    test-error:0.09704+0.00386\n[11]    train-auc:0.81215+0.00121   train-error:0.09704+0.00097 test-auc:0.80557+0.00378    test-error:0.09704+0.00386\n[12]    train-auc:0.81305+0.00112   train-error:0.09704+0.00097 test-auc:0.80636+0.00380    test-error:0.09704+0.00386\n[13]    train-auc:0.81349+0.00121   train-error:0.09704+0.00097 test-auc:0.80649+0.00372    test-error:0.09704+0.00386\n[14]    train-auc:0.81401+0.00120   train-error:0.09704+0.00097 test-auc:0.80736+0.00359    test-error:0.09704+0.00386\n[15]    train-auc:0.81451+0.00126   train-error:0.09704+0.00097 test-auc:0.80777+0.00366    test-error:0.09704+0.00386\n[16]    train-auc:0.81519+0.00128   train-error:0.09704+0.00097 test-auc:0.80810+0.00361    test-error:0.09704+0.00386\n[17]    train-auc:0.81555+0.00139   train-error:0.09704+0.00097 test-auc:0.80836+0.00358    test-error:0.09704+0.00386\n[18]    train-auc:0.81585+0.00148   train-error:0.09704+0.00097 test-auc:0.80857+0.00358    test-error:0.09704+0.00386\n[19]    train-auc:0.81630+0.00155   train-error:0.09704+0.00097 test-auc:0.80867+0.00346    test-error:0.09704+0.00386\n[20]    train-auc:0.81661+0.00150   train-error:0.09704+0.00097 test-auc:0.80900+0.00354    test-error:0.09704+0.00386\n\nCross-validation results:\nBest AUC: 0.7797 (+/- 0.0032)\nBest Error: 0.0970 (+/- 0.0039)\n\nTraining final model...\n\nCalculating performance metrics...\n\nAccuracy: 0.9001\nROC AUC: 0.8151\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.99      0.95     10762\n           1       0.36      0.04      0.07      1156\n\n    accuracy                           0.90     11918\n   macro avg       0.63      0.52      0.51     11918\nweighted avg       0.85      0.90      0.86     11918\n\n\nConfusion Matrix:\n[[10681    81]\n [ 1110    46]]\n\n\n\n# Additional Metrics\nprint(\"\\nCalculating additional performance metrics...\")\n\n# Calculate metrics\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Calculate detailed classification metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nspecificity = tn / (tn + fp)\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\nprevalence = (tp + fn) / total\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\n# Print comprehensive metrics\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\nprint(f\"\\nPrecision: {precision:.4f}\")\nprint(f\"Recall/Sensitivity: {recall:.4f}\")\nprint(f\"Specificity: {specificity:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\n# Create visualizations\nplt.figure(figsize=(20, 15))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, color='darkorange',\n         label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\nplt.plot(recall_curve, precision_curve,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()),\n                           columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\nplt.title('Top 10 Feature Importances')\n\n# Plot 4: Confusion Matrix Heatmap\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# SHAP Analysis\nprint(\"\\nGenerating SHAP values...\")\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test)\n\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\nplt.title('SHAP Feature Importance')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test)\nplt.title('SHAP Summary Plot')\nplt.tight_layout()\nplt.show()\n\n# Calibration plot\nplt.figure(figsize=(8, 6))\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nCalculating additional performance metrics...\n\nComprehensive Model Performance Metrics:\nRMSE: 0.2814\nMSE: 0.0792\nMAE: 0.1474\nR-squared: 0.0958\n\nPrecision: 0.3622\nRecall/Sensitivity: 0.0398\nSpecificity: 0.9925\nF1 Score: 0.0717\nPrevalence: 0.0970\nNegative Predictive Value: 0.9059\nPositive Likelihood Ratio: 5.2870\nNegative Likelihood Ratio: 0.9675\n\n\n\n\n\n\n\n\n\n\nGenerating SHAP values..."
  },
  {
    "objectID": "hps_cps_pums.html",
    "href": "hps_cps_pums.html",
    "title": "HPS Binary Classification Model",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\npd.set_option(\"display.max_columns\",None)\n\n\n# Load and prepare data\nfiltered_df = pd.read_csv(\"/content/combinedphase4hps.csv\")\nX = filtered_df.drop(['Unnamed: 0','PWEIGHT', 'HWEIGHT', 'SCRAM', 'TWDAYS','TWDAYS_RESP','AHHLD_NUMPER','AHHLD_NUMKID','ABIRTH_YEAR','AEDUC','ARACE'], axis=1)\ny = filtered_df['TWDAYS_RESP']\n\n\nX\n\n\n  \n    \n\n\n\n\n\n\nCYCLE\nEST_ST\nREGION\nRHISPANIC\nAHISPANIC\nRRACE\nEEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nTHHLD_NUMPER\nTHHLD_NUMKID\nTHHLD_NUMADLT\nACTVDUTY1\nRECVDVACC\nHADCOVIDRV\nWRKLOSSRV\nANYWORK\nKINDWORK\nEXPNS_DIF\nCURFOODSUF\nFREEFOOD\nANXIOUS\nWORRY\nINTEREST\nDOWN\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nLIVQTRRV\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nPRICECHNG\nPRICECONCRN\nND_DISPLACE\nVETERAN1\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nAGE\n\n\n\n\n0\n1.0\n36.0\n1.0\n1.0\n2.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n4.0\n3.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n6.0\n1.0\n3.0\n2.0\n1.0\n1.0\n4.0\n4.0\n2.0\n1.0\n55.0\n\n\n1\n1.0\n34.0\n1.0\n1.0\n2.0\n1.0\n6.0\n5.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n1.0\n1.0\n1.0\n3.0\n2.0\n2.0\n4.0\n4.0\n4.0\n4.0\n2.0\n2.0\n1.0\n2.0\n3.0\n3.0\n2.0\n1.0\n28.0\n\n\n2\n1.0\n36.0\n1.0\n2.0\n2.0\n1.0\n5.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n3.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n6.0\n1.0\n1.0\n2.0\n1.0\n5.0\n3.0\n2.0\n2.0\n2.0\n49.0\n\n\n3\n1.0\n34.0\n1.0\n1.0\n2.0\n1.0\n7.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n3.0\n2.0\n1.0\n2.0\n4.0\n4.0\n2.0\n1.0\n50.0\n\n\n4\n1.0\n36.0\n1.0\n1.0\n2.0\n1.0\n6.0\n1.0\n1.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n6.0\n4.0\n4.0\n4.0\n7.0\n1.0\n1.0\n2.0\n1.0\n2.0\n4.0\n3.0\n2.0\n2.0\n30.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5660\n9.0\n34.0\n1.0\n1.0\n2.0\n3.0\n5.0\n1.0\n1.0\n2.0\n4.0\n2.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n4.0\n2.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n1.0\n2.0\n1.0\n2.0\n3.0\n1.0\n2.0\n4.0\n49.0\n\n\n5661\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n7.0\n5.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n2.0\n3.0\n2.0\n1.0\n2.0\n4.0\n3.0\n2.0\n3.0\n42.0\n\n\n5662\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n7.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n2.0\n4.0\n4.0\n4.0\n8.0\n1.0\n2.0\n2.0\n1.0\n3.0\n4.0\n4.0\n1.0\n1.0\n33.0\n\n\n5663\n9.0\n34.0\n1.0\n1.0\n2.0\n1.0\n5.0\n1.0\n2.0\n2.0\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n2.0\n1.0\n2.0\n3.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n3.0\n2.0\n3.0\n4.0\n1.0\n1.0\n2.0\n1.0\n1.0\n4.0\n4.0\n2.0\n1.0\n55.0\n\n\n5664\n9.0\n36.0\n1.0\n1.0\n2.0\n1.0\n6.0\n5.0\n1.0\n2.0\n3.0\n0.0\n3.0\n1.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n5.0\n2.0\n4.0\n4.0\n5.0\n4.0\n2.0\n2.0\n1.0\n2.0\n4.0\n2.0\n3.0\n1.0\n27.0\n\n\n\n\n5665 rows × 50 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert labels from 1.0/2.0 to 0/1\ny = (y == 2.0).astype(int)\n\n\nprint(\"Performing initial feature selection...\")\nselector = SelectKBest(mutual_info_classif, k=50)  # Keep top 50 features\nX_reduced = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support(indices=True)]\nprint(\"Top 50 features selected.\")\n\nPerforming initial feature selection...\nTop 50 features selected.\n\n\n\n# Create a DataFrame with reduced features for VIF and further steps\nX = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 15][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n  return 1 - self.ssr/self.centered_tss\n/usr/local/lib/python3.11/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n  vif = 1. / (1. - r_squared_i)\n\n\nVIF Results:\n          Feature       VIF\n0           CYCLE  1.017447\n1          EST_ST  1.177024\n2          REGION  0.000000\n3       RHISPANIC  1.042880\n4       AHISPANIC  1.004881\n5           RRACE  1.078979\n6           EEDUC  1.222808\n7              MS  1.753173\n8    EGENID_BIRTH  1.112458\n9    AGENID_BIRTH  1.011530\n10   THHLD_NUMPER       inf\n11   THHLD_NUMKID       inf\n12  THHLD_NUMADLT       inf\n13      ACTVDUTY1  0.000000\n14      RECVDVACC  1.068647\n15     HADCOVIDRV  1.041604\n16      WRKLOSSRV  1.048995\n17        ANYWORK  0.000000\n18       KINDWORK  1.110101\n19      EXPNS_DIF  2.112060\n20     CURFOODSUF  1.742856\n21       FREEFOOD  1.055199\n22        ANXIOUS  2.953812\n23          WORRY  2.912706\n24       INTEREST  2.490250\n25           DOWN  2.924415\n26       PRIVHLTH  1.254752\n27        PUBHLTH  1.224917\n28         SEEING  1.174728\n29        HEARING  1.139989\n30    REMEMBERING  1.370348\n31       MOBILITY  1.233866\n32       SELFCARE  1.274685\n33     UNDERSTAND  1.213315\n34         TENURE  1.668701\n35       LIVQTRRV  1.898722\n36         ENERGY  1.902153\n37       HSE_TEMP  1.230725\n38     ENRGY_BILL  1.621682\n39         INCOME  1.693776\n40      PRICECHNG  1.231449\n41    PRICECONCRN  1.629774\n42    ND_DISPLACE  1.017538\n43       VETERAN1  0.000000\n44        SOCIAL1  1.384256\n45        SOCIAL2  1.625487\n46       SUPPORT1  1.204116\n47       SUPPORT2  1.235698\n48       SUPPORT3  1.128506\n49            AGE  1.693441\nDropping features with high VIF: ['THHLD_NUMPER', 'THHLD_NUMKID', 'THHLD_NUMADLT']\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\nPerforming Recursive Feature Elimination...\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# X_train_final = X_train_final[non_outliers]\n# y_train = y_train[non_outliers]\n\n\nX_train_final\n\n\n  \n    \n\n\n\n\n\n\nCYCLE\nEST_ST\nRRACE\nEEDUC\nMS\nEGENID_BIRTH\nHADCOVIDRV\nKINDWORK\nEXPNS_DIF\nCURFOODSUF\nANXIOUS\nWORRY\nINTEREST\nDOWN\nPUBHLTH\nSEEING\nREMEMBERING\nTENURE\nLIVQTRRV\nENERGY\nINCOME\nPRICECHNG\nPRICECONCRN\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nAGE\n\n\n\n\n2128\n4.0\n34.0\n1.0\n6.0\n1.0\n1.0\n1.0\n4.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n4.0\n6.0\n2.0\n4.0\n1.0\n4.0\n4.0\n4.0\n1.0\n66.0\n\n\n691\n2.0\n36.0\n1.0\n6.0\n1.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n3.0\n1.0\n1.0\n1.0\n3.0\n4.0\n8.0\n2.0\n4.0\n2.0\n4.0\n1.0\n1.0\n1.0\n61.0\n\n\n730\n2.0\n36.0\n1.0\n7.0\n1.0\n2.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n4.0\n6.0\n4.0\n3.0\n3.0\n3.0\n2.0\n2.0\n1.0\n49.0\n\n\n5443\n9.0\n36.0\n1.0\n6.0\n1.0\n2.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n7.0\n1.0\n3.0\n3.0\n4.0\n1.0\n1.0\n1.0\n54.0\n\n\n1984\n3.0\n34.0\n3.0\n7.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n1.0\n2.0\n2.0\n3.0\n4.0\n7.0\n1.0\n1.0\n4.0\n3.0\n2.0\n1.0\n2.0\n48.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4296\n7.0\n36.0\n1.0\n6.0\n1.0\n1.0\n1.0\n2.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n8.0\n1.0\n2.0\n2.0\n4.0\n2.0\n1.0\n1.0\n25.0\n\n\n2763\n5.0\n34.0\n1.0\n6.0\n1.0\n2.0\n1.0\n2.0\n4.0\n2.0\n3.0\n3.0\n2.0\n2.0\n2.0\n2.0\n3.0\n3.0\n6.0\n4.0\n7.0\n1.0\n1.0\n2.0\n3.0\n2.0\n1.0\n1.0\n34.0\n\n\n3593\n6.0\n36.0\n3.0\n7.0\n1.0\n2.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n3.0\n6.0\n4.0\n8.0\n2.0\n4.0\n2.0\n4.0\n3.0\n3.0\n1.0\n58.0\n\n\n3277\n5.0\n34.0\n1.0\n7.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n2.0\n2.0\n4.0\n8.0\n1.0\n2.0\n1.0\n5.0\n3.0\n2.0\n1.0\n42.0\n\n\n5389\n9.0\n34.0\n1.0\n4.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n2.0\n4.0\n8.0\n2.0\n3.0\n1.0\n5.0\n4.0\n1.0\n1.0\n39.0\n\n\n\n\n4532 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\nPerforming cross-validation...\n[0] train-auc:0.72904+0.00717   train-error:0.13923+0.00246 test-auc:0.63914+0.03029    test-error:0.13923+0.00984\n[1] train-auc:0.74121+0.00650   train-error:0.13923+0.00246 test-auc:0.65050+0.03281    test-error:0.13923+0.00984\n[2] train-auc:0.74980+0.00536   train-error:0.13923+0.00246 test-auc:0.65789+0.03393    test-error:0.13923+0.00984\n[3] train-auc:0.75701+0.00727   train-error:0.13923+0.00246 test-auc:0.65717+0.03223    test-error:0.13923+0.00984\n[4] train-auc:0.76338+0.00684   train-error:0.13923+0.00246 test-auc:0.66330+0.03274    test-error:0.13923+0.00984\n[5] train-auc:0.76952+0.00707   train-error:0.13923+0.00246 test-auc:0.66412+0.03152    test-error:0.13923+0.00984\n[6] train-auc:0.77329+0.00713   train-error:0.13912+0.00241 test-auc:0.66570+0.02975    test-error:0.13923+0.00984\n[7] train-auc:0.77670+0.00816   train-error:0.13885+0.00223 test-auc:0.66564+0.02755    test-error:0.13923+0.00984\n[8] train-auc:0.78105+0.00848   train-error:0.13857+0.00220 test-auc:0.66675+0.02709    test-error:0.13945+0.00967\n[9] train-auc:0.78542+0.00873   train-error:0.13813+0.00239 test-auc:0.66726+0.02838    test-error:0.13945+0.00967\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:35] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n[10]    train-auc:0.79012+0.00617   train-error:0.13758+0.00225 test-auc:0.66581+0.02624    test-error:0.13945+0.00967\n[11]    train-auc:0.79638+0.00740   train-error:0.13692+0.00228 test-auc:0.66725+0.02750    test-error:0.13945+0.00967\n[12]    train-auc:0.79997+0.00864   train-error:0.13653+0.00261 test-auc:0.66868+0.02840    test-error:0.13967+0.00952\n[13]    train-auc:0.80394+0.00838   train-error:0.13609+0.00258 test-auc:0.66558+0.02804    test-error:0.13967+0.00918\n[14]    train-auc:0.80669+0.00852   train-error:0.13576+0.00253 test-auc:0.66478+0.02999    test-error:0.13967+0.00918\n[15]    train-auc:0.81104+0.00836   train-error:0.13537+0.00250 test-auc:0.66511+0.03062    test-error:0.13945+0.00936\n[16]    train-auc:0.81227+0.00863   train-error:0.13482+0.00277 test-auc:0.66637+0.03134    test-error:0.13923+0.00946\n[17]    train-auc:0.81605+0.00882   train-error:0.13438+0.00338 test-auc:0.66839+0.03072    test-error:0.13945+0.00931\n[18]    train-auc:0.81879+0.00957   train-error:0.13361+0.00334 test-auc:0.66744+0.02975    test-error:0.13967+0.00858\n[19]    train-auc:0.82114+0.01037   train-error:0.13305+0.00338 test-auc:0.66828+0.03086    test-error:0.13945+0.00933\n[20]    train-auc:0.82445+0.00874   train-error:0.13245+0.00387 test-auc:0.66960+0.02902    test-error:0.13901+0.00908\n[21]    train-auc:0.82624+0.00880   train-error:0.13162+0.00414 test-auc:0.66994+0.02746    test-error:0.13879+0.00901\n[22]    train-auc:0.82979+0.00980   train-error:0.13112+0.00404 test-auc:0.66941+0.02966    test-error:0.13879+0.00880\n[23]    train-auc:0.83073+0.00946   train-error:0.13041+0.00371 test-auc:0.67039+0.02904    test-error:0.13857+0.00841\n[24]    train-auc:0.83414+0.01029   train-error:0.12991+0.00358 test-auc:0.66796+0.03002    test-error:0.13857+0.00841\n[25]    train-auc:0.83735+0.01163   train-error:0.12925+0.00328 test-auc:0.66852+0.02923    test-error:0.13857+0.00841\n[26]    train-auc:0.84148+0.01130   train-error:0.12870+0.00330 test-auc:0.66800+0.03058    test-error:0.13812+0.00874\n[27]    train-auc:0.84529+0.01061   train-error:0.12836+0.00327 test-auc:0.66701+0.02953    test-error:0.13790+0.00887\n[28]    train-auc:0.84720+0.01062   train-error:0.12770+0.00327 test-auc:0.66731+0.02853    test-error:0.13812+0.00874\n[29]    train-auc:0.84962+0.01130   train-error:0.12715+0.00349 test-auc:0.66684+0.02826    test-error:0.13813+0.00861\n[30]    train-auc:0.85275+0.01050   train-error:0.12610+0.00355 test-auc:0.66614+0.02878    test-error:0.13813+0.00861\n[31]    train-auc:0.85416+0.01075   train-error:0.12572+0.00319 test-auc:0.66590+0.02810    test-error:0.13790+0.00871\n[32]    train-auc:0.85600+0.01087   train-error:0.12483+0.00317 test-auc:0.66428+0.02854    test-error:0.13813+0.00861\n[33]    train-auc:0.85837+0.01135   train-error:0.12450+0.00339 test-auc:0.66299+0.02943    test-error:0.13790+0.00871\n[34]    train-auc:0.86169+0.00901   train-error:0.12401+0.00302 test-auc:0.66171+0.02896    test-error:0.13790+0.00851\n[35]    train-auc:0.86323+0.00908   train-error:0.12346+0.00322 test-auc:0.66093+0.02910    test-error:0.13835+0.00821\n[36]    train-auc:0.86474+0.00881   train-error:0.12329+0.00319 test-auc:0.66113+0.02873    test-error:0.13857+0.00810\n[37]    train-auc:0.86716+0.00913   train-error:0.12257+0.00315 test-auc:0.66113+0.02934    test-error:0.13857+0.00810\n[38]    train-auc:0.86932+0.00907   train-error:0.12202+0.00332 test-auc:0.66081+0.02915    test-error:0.13835+0.00824\n[39]    train-auc:0.87094+0.00852   train-error:0.12180+0.00307 test-auc:0.66066+0.03035    test-error:0.13835+0.00824\n[40]    train-auc:0.87232+0.00831   train-error:0.12119+0.00273 test-auc:0.65967+0.02967    test-error:0.13857+0.00810\n[41]    train-auc:0.87502+0.00781   train-error:0.12048+0.00315 test-auc:0.65826+0.02838    test-error:0.13857+0.00810\n[42]    train-auc:0.87668+0.00808   train-error:0.11981+0.00271 test-auc:0.65678+0.02758    test-error:0.13879+0.00809\n[43]    train-auc:0.87866+0.00812   train-error:0.11970+0.00257 test-auc:0.65662+0.02838    test-error:0.13901+0.00810\n[44]    train-auc:0.88010+0.00810   train-error:0.11904+0.00251 test-auc:0.65730+0.02788    test-error:0.13901+0.00810\n[45]    train-auc:0.88106+0.00816   train-error:0.11860+0.00274 test-auc:0.65704+0.02792    test-error:0.13923+0.00852\n[46]    train-auc:0.88228+0.00824   train-error:0.11766+0.00298 test-auc:0.65611+0.02854    test-error:0.13945+0.00841\n\n\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate and print metrics\nprint(\"\\nModel Performance Metrics:\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plot feature importances\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.show()\n\n\nCross-validation results:\nBest AUC: 0.6704 (+/- 0.0262)\nBest Error: 0.1379 (+/- 0.0084)\n\nTraining final model...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:07:14] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nModel Performance Metrics:\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.99      0.92       975\n           1       0.31      0.03      0.06       158\n\n    accuracy                           0.86      1133\n   macro avg       0.59      0.51      0.49      1133\nweighted avg       0.79      0.86      0.80      1133\n\n\nConfusion Matrix:\n[[964  11]\n [153   5]]\n\nROC AUC Score: 0.6858\n\nTop 10 Most Important Features:\n         Feature  Importance\n28           AGE       332.0\n0          CYCLE       138.0\n7       KINDWORK       136.0\n20        INCOME       100.0\n3          EEDUC        96.0\n18      LIVQTRRV        82.0\n25      SUPPORT1        77.0\n27      SUPPORT3        76.0\n24       SOCIAL2        75.0\n8      EXPNS_DIF        72.0\n5   EGENID_BIRTH        71.0\n22   PRICECONCRN        63.0\n9     CURFOODSUF        57.0\n4             MS        56.0\n19        ENERGY        56.0\n23       SOCIAL1        55.0\n26      SUPPORT2        54.0\n10       ANXIOUS        53.0\n15        SEEING        50.0\n21     PRICECHNG        44.0\n1         EST_ST        38.0\n14       PUBHLTH        37.0\n6     HADCOVIDRV        36.0\n13          DOWN        35.0\n16   REMEMBERING        33.0\n2          RRACE        31.0\n17        TENURE        30.0\n11         WORRY        29.0\n12      INTEREST        17.0\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# Add constant for statsmodels\nX_train_probit = sm.add_constant(X_train_final)\n\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit(disp=0)  # disp=0 -&gt; no iteration details\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# If you want to see marginal effects (average partial effects), uncomment below:\nme = probit_results.get_margeff()\nprint(\"\\nMarginal Effects:\")\nprint(me.summary())\n\n\nFitting Probit model for effect sizes and p-values...\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:            TWDAYS_RESP   No. Observations:                 4532\nModel:                         Probit   Df Residuals:                     4502\nMethod:                           MLE   Df Model:                           29\nDate:                Fri, 31 Jan 2025   Pseudo R-squ.:                 0.05405\nTime:                        04:00:44   Log-Likelihood:                -1730.1\nconverged:                       True   LL-Null:                       -1829.0\nCovariance Type:            nonrobust   LLR p-value:                 4.968e-27\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           -1.6437      1.003     -1.639      0.101      -3.609       0.322\nCYCLE           -0.0102      0.010     -1.053      0.293      -0.029       0.009\nEST_ST           0.0590      0.026      2.302      0.021       0.009       0.109\nRRACE           -0.0138      0.029     -0.475      0.635      -0.071       0.043\nEEDUC           -0.1432      0.023     -6.185      0.000      -0.189      -0.098\nMS              -0.0579      0.017     -3.412      0.001      -0.091      -0.025\nEGENID_BIRTH     0.0153      0.050      0.307      0.759      -0.082       0.113\nHADCOVIDRV      -0.0206      0.057     -0.362      0.717      -0.132       0.091\nKINDWORK        -0.0680      0.029     -2.381      0.017      -0.124      -0.012\nEXPNS_DIF        0.0525      0.037      1.434      0.152      -0.019       0.124\nCURFOODSUF       0.0481      0.060      0.799      0.425      -0.070       0.166\nANXIOUS         -0.1246      0.050     -2.516      0.012      -0.222      -0.028\nWORRY            0.0650      0.053      1.220      0.222      -0.039       0.169\nINTEREST        -0.0053      0.055     -0.097      0.923      -0.114       0.103\nDOWN             0.1047      0.059      1.767      0.077      -0.011       0.221\nPUBHLTH         -0.0555      0.053     -1.043      0.297      -0.160       0.049\nSEEING           0.0497      0.055      0.901      0.368      -0.058       0.158\nREMEMBERING     -0.0178      0.058     -0.307      0.759      -0.131       0.096\nTENURE           0.0076      0.046      0.165      0.869      -0.083       0.098\nLIVQTRRV        -0.0896      0.018     -4.977      0.000      -0.125      -0.054\nENERGY           0.0316      0.039      0.808      0.419      -0.045       0.108\nINCOME          -0.0499      0.018     -2.846      0.004      -0.084      -0.016\nPRICECHNG       -0.0007      0.031     -0.021      0.983      -0.062       0.061\nPRICECONCRN      0.0165      0.030      0.553      0.580      -0.042       0.075\nSOCIAL1         -0.0213      0.027     -0.775      0.438      -0.075       0.032\nSOCIAL2          0.1105      0.033      3.343      0.001       0.046       0.175\nSUPPORT1        -0.0149      0.024     -0.616      0.538      -0.062       0.032\nSUPPORT2        -0.0592      0.032     -1.824      0.068      -0.123       0.004\nSUPPORT3         0.0615      0.022      2.771      0.006       0.018       0.105\nAGE             -0.0058      0.002     -2.494      0.013      -0.010      -0.001\n================================================================================\n\nMarginal Effects:\n       Probit Marginal Effects       \n=====================================\nDep. Variable:            TWDAYS_RESP\nMethod:                          dydx\nAt:                           overall\n================================================================================\n                  dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nCYCLE           -0.0021      0.002     -1.053      0.292      -0.006       0.002\nEST_ST           0.0124      0.005      2.303      0.021       0.002       0.023\nRRACE           -0.0029      0.006     -0.475      0.635      -0.015       0.009\nEEDUC           -0.0301      0.005     -6.217      0.000      -0.040      -0.021\nMS              -0.0122      0.004     -3.416      0.001      -0.019      -0.005\nEGENID_BIRTH     0.0032      0.010      0.307      0.759      -0.017       0.024\nHADCOVIDRV      -0.0043      0.012     -0.362      0.717      -0.028       0.019\nKINDWORK        -0.0143      0.006     -2.383      0.017      -0.026      -0.003\nEXPNS_DIF        0.0110      0.008      1.434      0.151      -0.004       0.026\nCURFOODSUF       0.0101      0.013      0.799      0.425      -0.015       0.035\nANXIOUS         -0.0262      0.010     -2.518      0.012      -0.047      -0.006\nWORRY            0.0136      0.011      1.221      0.222      -0.008       0.036\nINTEREST        -0.0011      0.012     -0.097      0.923      -0.024       0.022\nDOWN             0.0220      0.012      1.767      0.077      -0.002       0.046\nPUBHLTH         -0.0117      0.011     -1.043      0.297      -0.034       0.010\nSEEING           0.0104      0.012      0.901      0.368      -0.012       0.033\nREMEMBERING     -0.0037      0.012     -0.307      0.759      -0.028       0.020\nTENURE           0.0016      0.010      0.165      0.869      -0.017       0.021\nLIVQTRRV        -0.0188      0.004     -4.987      0.000      -0.026      -0.011\nENERGY           0.0066      0.008      0.808      0.419      -0.009       0.023\nINCOME          -0.0105      0.004     -2.847      0.004      -0.018      -0.003\nPRICECHNG       -0.0001      0.007     -0.021      0.983      -0.013       0.013\nPRICECONCRN      0.0035      0.006      0.553      0.580      -0.009       0.016\nSOCIAL1         -0.0045      0.006     -0.775      0.438      -0.016       0.007\nSOCIAL2          0.0232      0.007      3.348      0.001       0.010       0.037\nSUPPORT1        -0.0031      0.005     -0.616      0.538      -0.013       0.007\nSUPPORT2        -0.0124      0.007     -1.825      0.068      -0.026       0.001\nSUPPORT3         0.0129      0.005      2.773      0.006       0.004       0.022\nAGE             -0.0012      0.000     -2.495      0.013      -0.002      -0.000\n================================================================================\n\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )\n\n\nCalculating and plotting SHAP values for XGBoost model...\nGenerating SHAP Beeswarm plot...\n\n\n\n\n\n\n\n\n\nGenerating SHAP Bar plot...\n\n\n\n\n\n\n\n\n\n\nCPS Binary Classification Model\n\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Load and prepare data\nfiltered_df = pd.read_csv(\"filtered_df.csv\")\nfiltered_df = filtered_df[(filtered_df['gestfips']==34) | (filtered_df['gestfips']==36)]\n\n\nX = filtered_df[['hehousut','hetelhhd','hetelavl','hefaminc','hrnumhou','hrhtype','HUBUS','gereg','gediv','gestfips','gtcbsa','gtco','gtcbsast','gtmetsta','gtindvpc','gtcbsasz','gtcsa','perrp','prtage','pemaritl','pesex','peeduca','ptdtrace','prdthsp','PUCHINHH','prfamrel','prfamtyp','pehspnon','penatvty','pemntvty','pefntvty','prcitshp','prinuyer','PUWK','pemjot','pemjnum','pehruslt','pehractt','peio1cow','prdtind1','prdtocc1','pternwa','ptwk','prchld','prnmchld']]\ny = filtered_df['pttlwk']\n\n\nX\n\n\n  \n    \n\n\n\n\n\n\nhehousut\nhetelhhd\nhetelavl\nhefaminc\nhrnumhou\nhrhtype\nHUBUS\ngereg\ngediv\ngestfips\n...\npemjnum\npehruslt\npehractt\npeio1cow\nprdtind1\nprdtocc1\npternwa\nptwk\nprchld\nprnmchld\n\n\n\n\n651\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n34\n...\n-1.0\n40.0\n48.0\n4.0\n25.0\n21.0\n-1.0\n0\n0.0\n0.0\n\n\n652\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n2.0\n51.0\n14.0\n-1.0\n0\n0.0\n0.0\n\n\n653\n1\n1\n-1\n11\n2\n6\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n14.0\n22.0\n-1.0\n0\n0.0\n0.0\n\n\n654\n1\n1\n-1\n11\n2\n6\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n22.0\n17.0\n-1.0\n0\n0.0\n0.0\n\n\n655\n1\n2\n2\n12\n2\n4\n2\n1\n2\n34\n...\n-1.0\n20.0\n20.0\n7.0\n36.0\n7.0\n-1.0\n0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n318282\n1\n1\n2\n10\n2\n4\n2\n1\n2\n36\n...\n-1.0\n40.0\n40.0\n4.0\n42.0\n11.0\n-1.0\n0\n0.0\n0.0\n\n\n318283\n1\n1\n-1\n15\n1\n6\n2\n1\n2\n36\n...\n-1.0\n40.0\n40.0\n1.0\n51.0\n20.0\n-1.0\n0\n0.0\n0.0\n\n\n318284\n1\n1\n-1\n14\n2\n1\n1\n1\n2\n36\n...\n-1.0\n50.0\n50.0\n4.0\n40.0\n13.0\n-1.0\n0\n0.0\n0.0\n\n\n318399\n1\n1\n-1\n16\n3\n4\n2\n1\n2\n34\n...\n-1.0\n20.0\n20.0\n4.0\n22.0\n16.0\n-1.0\n0\n0.0\n0.0\n\n\n318400\n1\n1\n-1\n16\n3\n4\n2\n1\n2\n34\n...\n-1.0\n40.0\n40.0\n4.0\n38.0\n22.0\n-1.0\n0\n4.0\n1.0\n\n\n\n\n17819 rows × 45 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert labels from 1.0/2.0 to 0/1\ny = (y == 2.0).astype(int)\n\n\ny.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\npttlwk\n\n\n\n\n\n1\n13529\n\n\n0\n4290\n\n\n\n\ndtype: int64\n\n\n\n# # Initialize label encoders dictionary\n# encoders = {}\n\n# # Label encode all feature columns\n# for column in X.columns:\n#     encoders[column] = LabelEncoder()\n#     X[column] = encoders[column].fit_transform(X[column])\n\n# # Label encode target variable\n# target_encoder = LabelEncoder()\n# y = target_encoder.fit_transform(y)\n\n# print(\"Shape of X after encoding:\", X.shape)\n# print(\"Shape of y after encoding:\", y.shape)\n# print(\"\\nUnique values in target variable:\", np.unique(y))\n\n\n# print(\"Performing initial feature selection...\")\n# selector = SelectKBest(mutual_info_classif, k=100)  # Keep top 50 features\n# X_reduced = selector.fit_transform(X, y)\n# selected_features = X.columns[selector.get_support(indices=True)]\n# print(\"Top 50 features selected.\")\n\n\n# Create a DataFrame with reduced features for VIF and further steps\n# X = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n  return 1 - self.ssr/self.centered_tss\n\n\nVIF Results:\n     Feature        VIF\n0   hehousut   1.046027\n1   hetelhhd   2.254292\n2   hetelavl   2.282600\n3   hefaminc   1.434420\n4   hrnumhou   2.452866\n5    hrhtype   4.732866\n6      HUBUS   1.156651\n7      gereg   0.000000\n8      gediv   0.000000\n9   gestfips   2.652757\n10    gtcbsa   2.183617\n11      gtco   2.202980\n12  gtcbsast   4.391335\n13  gtmetsta   2.231556\n14  gtindvpc   2.651372\n15  gtcbsasz   9.794757\n16     gtcsa   5.458005\n17     perrp   5.387685\n18    prtage   1.678536\n19  pemaritl   3.464919\n20     pesex   1.148676\n21   peeduca   1.524835\n22  ptdtrace   1.271114\n23   prdthsp   5.773714\n24  PUCHINHH   1.015834\n25  prfamrel   6.719113\n26  prfamtyp   4.662360\n27  pehspnon   5.659180\n28  penatvty   7.903467\n29  pemntvty   8.318010\n30  pefntvty   7.946751\n31  prcitshp  12.204819\n32  prinuyer   8.446316\n33      PUWK   1.015886\n34    pemjot  83.410023\n35   pemjnum  83.510317\n36  pehruslt   2.022003\n37  pehractt   2.050188\n38  peio1cow   1.171097\n39  prdtind1   1.193875\n40  prdtocc1   1.359964\n41   pternwa   3.260463\n42      ptwk   3.171225\n43    prchld   4.825561\n44  prnmchld   5.667953\nDropping features with high VIF: ['prcitshp', 'pemjot', 'pemjnum']\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\nPerforming Recursive Feature Elimination...\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# # Align indices before filtering\n# X_train_final = X_train_final[non_outliers].reset_index(drop=True)\n# # Use the index from X_train_final before reset to filter y_train\n# y_train = y_train[X_train_final.index[non_outliers]].reset_index(drop=True)\n\n\nX_train_final\n\n\n  \n    \n\n\n\n\n\n\nhefaminc\nhrnumhou\nhrhtype\nHUBUS\ngestfips\ngtcbsa\ngtco\ngtcbsast\ngtindvpc\ngtcbsasz\n...\npefntvty\nprinuyer\npehruslt\npehractt\npeio1cow\nprdtind1\nprdtocc1\npternwa\nprchld\nprnmchld\n\n\n\n\n143322\n9\n2\n6\n2\n36\n15380\n0\n1\n1\n5\n...\n57.0\n0.0\n30.0\n30.0\n4.0\n46.0\n13.0\n-1.0\n0.0\n0.0\n\n\n318280\n15\n2\n1\n2\n36\n10580\n0\n1\n0\n4\n...\n117.0\n0.0\n40.0\n40.0\n4.0\n19.0\n22.0\n-1.0\n0.0\n0.0\n\n\n316593\n16\n9\n1\n2\n36\n35620\n81\n1\n1\n7\n...\n333.0\n0.0\n25.0\n25.0\n1.0\n51.0\n12.0\n-1.0\n0.0\n0.0\n\n\n78397\n11\n4\n1\n2\n34\n0\n0\n4\n0\n0\n...\n57.0\n0.0\n40.0\n40.0\n3.0\n40.0\n22.0\n-1.0\n0.0\n0.0\n\n\n92298\n15\n2\n1\n2\n34\n45940\n21\n4\n0\n3\n...\n57.0\n0.0\n40.0\n1.0\n4.0\n19.0\n17.0\n-1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n143188\n16\n4\n1\n2\n34\n35620\n13\n2\n0\n7\n...\n57.0\n0.0\n40.0\n40.0\n1.0\n51.0\n7.0\n-1.0\n5.0\n2.0\n\n\n164531\n11\n5\n3\n2\n36\n35620\n5\n1\n1\n7\n...\n328.0\n24.0\n25.0\n25.0\n3.0\n51.0\n12.0\n-1.0\n0.0\n0.0\n\n\n44316\n13\n1\n6\n2\n36\n0\n0\n3\n0\n0\n...\n57.0\n0.0\n40.0\n40.0\n4.0\n42.0\n21.0\n72000.0\n0.0\n0.0\n\n\n297427\n12\n3\n1\n2\n34\n35620\n13\n2\n0\n7\n...\n370.0\n28.0\n60.0\n60.0\n7.0\n36.0\n9.0\n-1.0\n3.0\n1.0\n\n\n36780\n16\n2\n1\n2\n36\n15380\n0\n2\n0\n5\n...\n57.0\n0.0\n20.0\n20.0\n4.0\n29.0\n20.0\n-1.0\n0.0\n0.0\n\n\n\n\n14255 rows × 30 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\nPerforming cross-validation...\n[0] train-auc:0.84550+0.00162   train-error:0.24076+0.00117 test-auc:0.83084+0.00405    test-error:0.24076+0.00468\n[1] train-auc:0.85108+0.00091   train-error:0.24076+0.00117 test-auc:0.83766+0.00555    test-error:0.24076+0.00468\n[2] train-auc:0.85450+0.00175   train-error:0.24076+0.00117 test-auc:0.84107+0.00493    test-error:0.24076+0.00468\n[3] train-auc:0.85600+0.00190   train-error:0.24076+0.00117 test-auc:0.84246+0.00456    test-error:0.24076+0.00468\n[4] train-auc:0.85822+0.00136   train-error:0.24076+0.00117 test-auc:0.84453+0.00454    test-error:0.24076+0.00468\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:33:21] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n[5] train-auc:0.86179+0.00331   train-error:0.24046+0.00104 test-auc:0.84622+0.00409    test-error:0.24062+0.00477\n[6] train-auc:0.86704+0.00245   train-error:0.22038+0.01036 test-auc:0.85064+0.00415    test-error:0.22448+0.00933\n[7] train-auc:0.86990+0.00301   train-error:0.20051+0.00366 test-auc:0.85308+0.00421    test-error:0.20744+0.00496\n[8] train-auc:0.87272+0.00163   train-error:0.18886+0.00164 test-auc:0.85495+0.00438    test-error:0.19551+0.00420\n[9] train-auc:0.87495+0.00115   train-error:0.18346+0.00179 test-auc:0.85689+0.00411    test-error:0.19074+0.00270\n[10]    train-auc:0.87717+0.00113   train-error:0.18143+0.00199 test-auc:0.85855+0.00410    test-error:0.18871+0.00339\n[11]    train-auc:0.87898+0.00138   train-error:0.17866+0.00137 test-auc:0.86038+0.00398    test-error:0.18702+0.00132\n[12]    train-auc:0.88076+0.00112   train-error:0.17608+0.00184 test-auc:0.86169+0.00396    test-error:0.18471+0.00357\n[13]    train-auc:0.88210+0.00097   train-error:0.17420+0.00198 test-auc:0.86243+0.00393    test-error:0.18302+0.00244\n[14]    train-auc:0.88343+0.00097   train-error:0.17220+0.00197 test-auc:0.86327+0.00391    test-error:0.18281+0.00254\n[15]    train-auc:0.88504+0.00105   train-error:0.17092+0.00188 test-auc:0.86466+0.00360    test-error:0.18141+0.00310\n[16]    train-auc:0.88660+0.00101   train-error:0.16984+0.00225 test-auc:0.86528+0.00350    test-error:0.18064+0.00284\n[17]    train-auc:0.88781+0.00117   train-error:0.16819+0.00211 test-auc:0.86620+0.00336    test-error:0.18085+0.00317\n[18]    train-auc:0.88933+0.00129   train-error:0.16654+0.00178 test-auc:0.86720+0.00352    test-error:0.17994+0.00236\n[19]    train-auc:0.89063+0.00128   train-error:0.16519+0.00214 test-auc:0.86812+0.00367    test-error:0.17952+0.00156\n[20]    train-auc:0.89191+0.00111   train-error:0.16373+0.00224 test-auc:0.86901+0.00388    test-error:0.17790+0.00179\n[21]    train-auc:0.89321+0.00127   train-error:0.16242+0.00249 test-auc:0.86994+0.00382    test-error:0.17608+0.00234\n[22]    train-auc:0.89441+0.00090   train-error:0.16122+0.00216 test-auc:0.87096+0.00396    test-error:0.17454+0.00127\n[23]    train-auc:0.89540+0.00084   train-error:0.15982+0.00222 test-auc:0.87171+0.00389    test-error:0.17355+0.00153\n[24]    train-auc:0.89649+0.00108   train-error:0.15805+0.00215 test-auc:0.87229+0.00390    test-error:0.17257+0.00222\n[25]    train-auc:0.89755+0.00107   train-error:0.15724+0.00207 test-auc:0.87280+0.00417    test-error:0.17264+0.00185\n[26]    train-auc:0.89841+0.00106   train-error:0.15617+0.00233 test-auc:0.87343+0.00418    test-error:0.17166+0.00228\n[27]    train-auc:0.89925+0.00103   train-error:0.15582+0.00247 test-auc:0.87383+0.00411    test-error:0.17089+0.00294\n[28]    train-auc:0.89986+0.00095   train-error:0.15498+0.00219 test-auc:0.87411+0.00405    test-error:0.17096+0.00324\n[29]    train-auc:0.90054+0.00077   train-error:0.15479+0.00231 test-auc:0.87449+0.00400    test-error:0.17054+0.00290\n[30]    train-auc:0.90109+0.00080   train-error:0.15423+0.00193 test-auc:0.87485+0.00391    test-error:0.16976+0.00328\n[31]    train-auc:0.90180+0.00098   train-error:0.15370+0.00153 test-auc:0.87537+0.00397    test-error:0.17005+0.00264\n[32]    train-auc:0.90244+0.00104   train-error:0.15303+0.00169 test-auc:0.87572+0.00379    test-error:0.17026+0.00178\n[33]    train-auc:0.90308+0.00094   train-error:0.15209+0.00183 test-auc:0.87614+0.00382    test-error:0.16955+0.00313\n[34]    train-auc:0.90397+0.00126   train-error:0.15137+0.00201 test-auc:0.87652+0.00370    test-error:0.16906+0.00203\n[35]    train-auc:0.90459+0.00148   train-error:0.15021+0.00207 test-auc:0.87698+0.00376    test-error:0.16794+0.00238\n[36]    train-auc:0.90521+0.00113   train-error:0.14951+0.00167 test-auc:0.87729+0.00403    test-error:0.16836+0.00251\n[37]    train-auc:0.90611+0.00129   train-error:0.14865+0.00175 test-auc:0.87794+0.00388    test-error:0.16829+0.00231\n[38]    train-auc:0.90679+0.00124   train-error:0.14793+0.00162 test-auc:0.87813+0.00384    test-error:0.16773+0.00213\n[39]    train-auc:0.90777+0.00116   train-error:0.14681+0.00206 test-auc:0.87855+0.00389    test-error:0.16689+0.00238\n[40]    train-auc:0.90814+0.00126   train-error:0.14646+0.00206 test-auc:0.87881+0.00389    test-error:0.16626+0.00218\n[41]    train-auc:0.90889+0.00155   train-error:0.14590+0.00229 test-auc:0.87915+0.00367    test-error:0.16619+0.00193\n[42]    train-auc:0.90923+0.00147   train-error:0.14581+0.00214 test-auc:0.87937+0.00376    test-error:0.16626+0.00290\n[43]    train-auc:0.91005+0.00140   train-error:0.14562+0.00221 test-auc:0.87985+0.00373    test-error:0.16535+0.00228\n[44]    train-auc:0.91060+0.00165   train-error:0.14507+0.00241 test-auc:0.88012+0.00353    test-error:0.16499+0.00316\n[45]    train-auc:0.91126+0.00124   train-error:0.14411+0.00181 test-auc:0.88032+0.00366    test-error:0.16478+0.00338\n[46]    train-auc:0.91179+0.00127   train-error:0.14318+0.00208 test-auc:0.88063+0.00373    test-error:0.16436+0.00293\n[47]    train-auc:0.91219+0.00126   train-error:0.14297+0.00178 test-auc:0.88077+0.00366    test-error:0.16415+0.00310\n[48]    train-auc:0.91255+0.00120   train-error:0.14274+0.00142 test-auc:0.88100+0.00367    test-error:0.16373+0.00295\n[49]    train-auc:0.91312+0.00143   train-error:0.14228+0.00146 test-auc:0.88124+0.00372    test-error:0.16352+0.00332\n[50]    train-auc:0.91370+0.00138   train-error:0.14179+0.00166 test-auc:0.88155+0.00392    test-error:0.16401+0.00376\n[51]    train-auc:0.91428+0.00138   train-error:0.14106+0.00132 test-auc:0.88190+0.00387    test-error:0.16345+0.00382\n[52]    train-auc:0.91454+0.00131   train-error:0.14081+0.00148 test-auc:0.88203+0.00388    test-error:0.16303+0.00330\n[53]    train-auc:0.91490+0.00138   train-error:0.14093+0.00146 test-auc:0.88219+0.00393    test-error:0.16310+0.00358\n[54]    train-auc:0.91560+0.00137   train-error:0.14025+0.00157 test-auc:0.88252+0.00398    test-error:0.16233+0.00383\n[55]    train-auc:0.91624+0.00121   train-error:0.13964+0.00160 test-auc:0.88279+0.00404    test-error:0.16247+0.00426\n[56]    train-auc:0.91667+0.00102   train-error:0.13934+0.00142 test-auc:0.88295+0.00409    test-error:0.16226+0.00444\n[57]    train-auc:0.91727+0.00089   train-error:0.13885+0.00166 test-auc:0.88305+0.00415    test-error:0.16212+0.00437\n[58]    train-auc:0.91782+0.00097   train-error:0.13837+0.00158 test-auc:0.88327+0.00409    test-error:0.16191+0.00414\n[59]    train-auc:0.91839+0.00102   train-error:0.13820+0.00180 test-auc:0.88357+0.00392    test-error:0.16135+0.00434\n[60]    train-auc:0.91892+0.00119   train-error:0.13799+0.00179 test-auc:0.88376+0.00386    test-error:0.16135+0.00373\n[61]    train-auc:0.91948+0.00100   train-error:0.13707+0.00138 test-auc:0.88398+0.00393    test-error:0.16170+0.00351\n[62]    train-auc:0.91976+0.00105   train-error:0.13686+0.00150 test-auc:0.88413+0.00391    test-error:0.16135+0.00371\n[63]    train-auc:0.92011+0.00101   train-error:0.13688+0.00147 test-auc:0.88425+0.00384    test-error:0.16107+0.00405\n[64]    train-auc:0.92049+0.00096   train-error:0.13625+0.00195 test-auc:0.88441+0.00384    test-error:0.16093+0.00393\n[65]    train-auc:0.92116+0.00111   train-error:0.13515+0.00195 test-auc:0.88482+0.00391    test-error:0.16043+0.00405\n[66]    train-auc:0.92142+0.00109   train-error:0.13515+0.00200 test-auc:0.88494+0.00388    test-error:0.16036+0.00346\n[67]    train-auc:0.92189+0.00098   train-error:0.13462+0.00145 test-auc:0.88500+0.00398    test-error:0.16058+0.00312\n[68]    train-auc:0.92222+0.00093   train-error:0.13441+0.00159 test-auc:0.88515+0.00404    test-error:0.15994+0.00343\n[69]    train-auc:0.92261+0.00099   train-error:0.13427+0.00156 test-auc:0.88534+0.00390    test-error:0.15952+0.00294\n[70]    train-auc:0.92307+0.00098   train-error:0.13369+0.00142 test-auc:0.88556+0.00374    test-error:0.15938+0.00370\n[71]    train-auc:0.92326+0.00094   train-error:0.13350+0.00150 test-auc:0.88563+0.00379    test-error:0.15910+0.00379\n[72]    train-auc:0.92360+0.00107   train-error:0.13336+0.00175 test-auc:0.88580+0.00368    test-error:0.15896+0.00353\n[73]    train-auc:0.92397+0.00106   train-error:0.13302+0.00189 test-auc:0.88590+0.00364    test-error:0.15875+0.00353\n[74]    train-auc:0.92440+0.00094   train-error:0.13269+0.00158 test-auc:0.88608+0.00381    test-error:0.15875+0.00372\n[75]    train-auc:0.92480+0.00085   train-error:0.13260+0.00128 test-auc:0.88610+0.00374    test-error:0.15854+0.00379\n[76]    train-auc:0.92521+0.00081   train-error:0.13178+0.00091 test-auc:0.88624+0.00354    test-error:0.15826+0.00320\n[77]    train-auc:0.92546+0.00085   train-error:0.13166+0.00107 test-auc:0.88640+0.00357    test-error:0.15854+0.00322\n[78]    train-auc:0.92572+0.00086   train-error:0.13153+0.00111 test-auc:0.88647+0.00350    test-error:0.15861+0.00319\n[79]    train-auc:0.92608+0.00096   train-error:0.13127+0.00140 test-auc:0.88659+0.00346    test-error:0.15861+0.00337\n[80]    train-auc:0.92632+0.00089   train-error:0.13129+0.00167 test-auc:0.88657+0.00355    test-error:0.15805+0.00395\n[81]    train-auc:0.92655+0.00102   train-error:0.13113+0.00173 test-auc:0.88667+0.00344    test-error:0.15812+0.00375\n[82]    train-auc:0.92680+0.00105   train-error:0.13064+0.00193 test-auc:0.88679+0.00341    test-error:0.15854+0.00440\n[83]    train-auc:0.92715+0.00113   train-error:0.13041+0.00216 test-auc:0.88689+0.00344    test-error:0.15833+0.00357\n[84]    train-auc:0.92752+0.00122   train-error:0.12966+0.00201 test-auc:0.88708+0.00349    test-error:0.15791+0.00325\n[85]    train-auc:0.92778+0.00119   train-error:0.12952+0.00188 test-auc:0.88720+0.00356    test-error:0.15833+0.00332\n[86]    train-auc:0.92810+0.00139   train-error:0.12918+0.00232 test-auc:0.88729+0.00366    test-error:0.15805+0.00358\n[87]    train-auc:0.92849+0.00158   train-error:0.12857+0.00272 test-auc:0.88746+0.00353    test-error:0.15805+0.00347\n[88]    train-auc:0.92877+0.00170   train-error:0.12848+0.00293 test-auc:0.88760+0.00348    test-error:0.15784+0.00336\n[89]    train-auc:0.92925+0.00168   train-error:0.12822+0.00280 test-auc:0.88770+0.00357    test-error:0.15805+0.00365\n[90]    train-auc:0.92955+0.00162   train-error:0.12764+0.00314 test-auc:0.88780+0.00361    test-error:0.15749+0.00369\n[91]    train-auc:0.92986+0.00176   train-error:0.12732+0.00306 test-auc:0.88789+0.00363    test-error:0.15728+0.00408\n[92]    train-auc:0.93029+0.00168   train-error:0.12692+0.00266 test-auc:0.88805+0.00364    test-error:0.15763+0.00388\n[93]    train-auc:0.93049+0.00166   train-error:0.12673+0.00269 test-auc:0.88812+0.00359    test-error:0.15784+0.00354\n[94]    train-auc:0.93089+0.00157   train-error:0.12632+0.00278 test-auc:0.88828+0.00360    test-error:0.15735+0.00387\n[95]    train-auc:0.93134+0.00150   train-error:0.12587+0.00270 test-auc:0.88841+0.00373    test-error:0.15756+0.00378\n[96]    train-auc:0.93165+0.00156   train-error:0.12555+0.00268 test-auc:0.88859+0.00370    test-error:0.15763+0.00388\n[97]    train-auc:0.93201+0.00156   train-error:0.12513+0.00286 test-auc:0.88865+0.00362    test-error:0.15756+0.00412\n[98]    train-auc:0.93224+0.00155   train-error:0.12508+0.00271 test-auc:0.88867+0.00359    test-error:0.15728+0.00424\n[99]    train-auc:0.93247+0.00148   train-error:0.12461+0.00257 test-auc:0.88874+0.00368    test-error:0.15679+0.00387\n\n\n\nimport numpy as np\nfrom sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate additional metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\n# Basic Metrics\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)  # Also known as True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Additional Metrics\nprevalence = (tp + fn) / total\npositive_predictive_value = tp / (tp + fp)  # Same as precision\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"Precision (Positive Predictive Value): {precision:.4f}\")\nprint(f\"Recall (Sensitivity/True Positive Rate): {recall:.4f}\")\nprint(f\"Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"False Positive Rate: {false_positive_rate:.4f}\")\nprint(f\"False Negative Rate: {false_negative_rate:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Calculate ROC and PR curves\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plotting\nplt.figure(figsize=(15, 10))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred):.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nplt.plot(recall_curve, precision_curve, color='blue', lw=2,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\n\n# Plot 4: Confusion Matrix\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Print calibration metrics\nfrom sklearn.calibration import calibration_curve\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n\n# Plot calibration curve\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nCross-validation results:\nBest AUC: 0.8887 (+/- 0.0034)\nBest Error: 0.1568 (+/- 0.0013)\n\nTraining final model...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:33:24] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nComprehensive Model Performance Metrics:\n\nAccuracy: 0.8443\nPrecision (Positive Predictive Value): 0.8764\nRecall (Sensitivity/True Positive Rate): 0.9254\nSpecificity (True Negative Rate): 0.5886\nFalse Positive Rate: 0.4114\nFalse Negative Rate: 0.0746\nF1 Score: 0.9002\nPrevalence: 0.7593\nNegative Predictive Value: 0.7143\nPositive Likelihood Ratio: 2.2492\nNegative Likelihood Ratio: 0.1268\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.59      0.65       858\n           1       0.88      0.93      0.90      2706\n\n    accuracy                           0.84      3564\n   macro avg       0.80      0.76      0.77      3564\nweighted avg       0.84      0.84      0.84      3564\n\n\nConfusion Matrix:\n[[ 505  353]\n [ 202 2504]]\n\nROC AUC Score: 0.8916\n\nTop 10 Most Important Features:\n     Feature  Importance\n26  prdtocc1       234.0\n25  prdtind1       223.0\n12    prtage       176.0\n6       gtco       134.0\n15   peeduca       127.0\n23  pehractt       126.0\n0   hefaminc       111.0\n24  peio1cow       108.0\n19  pemntvty        91.0\n3      HUBUS        77.0\n11     perrp        75.0\n1   hrnumhou        67.0\n22  pehruslt        66.0\n21  prinuyer        58.0\n20  pefntvty        55.0\n17  prfamrel        51.0\n5     gtcbsa        47.0\n27   pternwa        44.0\n28    prchld        43.0\n14     pesex        42.0\n10     gtcsa        36.0\n29  prnmchld        36.0\n16  ptdtrace        32.0\n18  penatvty        28.0\n2    hrhtype        27.0\n4   gestfips        26.0\n8   gtindvpc        24.0\n13  pemaritl        22.0\n9   gtcbsasz        14.0\n7   gtcbsast        12.0\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# We add a constant for statsmodels\nX_train_final.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train_probit = sm.add_constant(X_train_final)\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit()\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# Optionally compute marginal effects\nprint(\"\\nMarginal Effects (Probit):\")\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nFitting Probit model for effect sizes and p-values...\nOptimization terminated successfully.\n         Current function value: 0.427806\n         Iterations 6\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14224\nMethod:                           MLE   Df Model:                           30\nDate:                Sun, 02 Feb 2025   Pseudo R-squ.:                  0.2249\nTime:                        05:33:30   Log-Likelihood:                -6098.4\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2208      0.902      2.463      0.014       0.454       3.988\nhefaminc      -0.0550      0.006     -9.743      0.000      -0.066      -0.044\nhrnumhou       0.0550      0.014      3.858      0.000       0.027       0.083\nhrhtype        0.0036      0.014      0.266      0.790      -0.023       0.030\nHUBUS          0.2808      0.031      9.035      0.000       0.220       0.342\ngestfips       0.1019      0.022      4.544      0.000       0.058       0.146\ngtcbsa     -3.337e-06   1.93e-06     -1.729      0.084   -7.12e-06    4.46e-07\ngtco        -8.21e-05      0.001     -0.151      0.880      -0.001       0.001\ngtcbsast       0.1007      0.032      3.175      0.001       0.039       0.163\ngtindvpc       0.0028      0.041      0.069      0.945      -0.077       0.083\ngtcbsasz       0.0894      0.020      4.542      0.000       0.051       0.128\ngtcsa         -0.0012      0.000     -6.856      0.000      -0.002      -0.001\nperrp          0.0007      0.005      0.137      0.891      -0.009       0.011\nprtage         0.0054      0.001      4.802      0.000       0.003       0.008\npemaritl       0.0251      0.011      2.260      0.024       0.003       0.047\npesex         -0.1291      0.027     -4.695      0.000      -0.183      -0.075\npeeduca       -0.1358      0.007    -19.551      0.000      -0.149      -0.122\nptdtrace      -0.0401      0.011     -3.645      0.000      -0.062      -0.019\nprfamrel       0.1337      0.022      5.951      0.000       0.090       0.178\npenatvty    7.458e-05      0.000      0.246      0.805      -0.001       0.001\npemntvty       0.0010      0.000      3.370      0.001       0.000       0.002\npefntvty      -0.0003      0.000     -1.054      0.292      -0.001       0.000\nprinuyer       0.0044      0.003      1.526      0.127      -0.001       0.010\npehruslt       0.0014      0.001      1.061      0.289      -0.001       0.004\npehractt      -0.0056      0.002     -3.699      0.000      -0.009      -0.003\npeio1cow      -0.1141      0.012     -9.191      0.000      -0.138      -0.090\nprdtind1       0.0064      0.001      5.538      0.000       0.004       0.009\nprdtocc1       0.0539      0.002     24.860      0.000       0.050       0.058\npternwa    -1.354e-07   9.27e-08     -1.461      0.144   -3.17e-07    4.62e-08\nprchld         0.0042      0.010      0.407      0.684      -0.016       0.024\nprnmchld      -0.0282      0.033     -0.859      0.390      -0.092       0.036\n==============================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nhefaminc      -0.0132      0.001     -9.814      0.000      -0.016      -0.011\nhrnumhou       0.0133      0.003      3.862      0.000       0.007       0.020\nhrhtype        0.0009      0.003      0.266      0.790      -0.006       0.007\nHUBUS          0.0676      0.007      9.111      0.000       0.053       0.082\ngestfips       0.0245      0.005      4.554      0.000       0.014       0.035\ngtcbsa     -8.035e-07   4.65e-07     -1.729      0.084   -1.71e-06    1.07e-07\ngtco       -1.977e-05      0.000     -0.151      0.880      -0.000       0.000\ngtcbsast       0.0242      0.008      3.179      0.001       0.009       0.039\ngtindvpc       0.0007      0.010      0.069      0.945      -0.019       0.020\ngtcbsasz       0.0215      0.005      4.552      0.000       0.012       0.031\ngtcsa         -0.0003   4.27e-05     -6.884      0.000      -0.000      -0.000\nperrp          0.0002      0.001      0.137      0.891      -0.002       0.003\nprtage         0.0013      0.000      4.810      0.000       0.001       0.002\npemaritl       0.0060      0.003      2.261      0.024       0.001       0.011\npesex         -0.0311      0.007     -4.706      0.000      -0.044      -0.018\npeeduca       -0.0327      0.002    -20.201      0.000      -0.036      -0.030\nptdtrace      -0.0097      0.003     -3.649      0.000      -0.015      -0.004\nprfamrel       0.0322      0.005      5.969      0.000       0.022       0.043\npenatvty    1.796e-05   7.29e-05      0.246      0.805      -0.000       0.000\npemntvty       0.0002   7.18e-05      3.374      0.001       0.000       0.000\npefntvty   -7.247e-05   6.87e-05     -1.054      0.292      -0.000    6.23e-05\nprinuyer       0.0011      0.001      1.527      0.127      -0.000       0.002\npehruslt       0.0003      0.000      1.061      0.289      -0.000       0.001\npehractt      -0.0014      0.000     -3.705      0.000      -0.002      -0.001\npeio1cow      -0.0275      0.003     -9.272      0.000      -0.033      -0.022\nprdtind1       0.0015      0.000      5.555      0.000       0.001       0.002\nprdtocc1       0.0130      0.000     26.497      0.000       0.012       0.014\npternwa    -3.261e-08   2.23e-08     -1.462      0.144   -7.63e-08    1.11e-08\nprchld         0.0010      0.002      0.407      0.684      -0.004       0.006\nprnmchld      -0.0068      0.008     -0.860      0.390      -0.022       0.009\n==============================================================================\n\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )\n\n\nCalculating and plotting SHAP values for XGBoost model...\nGenerating SHAP Beeswarm plot...\n\n\n\n\n\n\n\n\n\nGenerating SHAP Bar plot...\n\n\n\n\n\n\n\n\n\n\n# pip install bayesian-optimization\n\nCollecting bayesian-optimization\n  Downloading bayesian_optimization-2.0.3-py3-none-any.whl.metadata (9.0 kB)\nCollecting colorama&lt;0.5.0,&gt;=0.4.6 (from bayesian-optimization)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy&gt;=1.25 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.26.4)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.6.1)\nRequirement already satisfied: scipy&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.13.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.0-&gt;bayesian-optimization) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.0-&gt;bayesian-optimization) (3.5.0)\nDownloading bayesian_optimization-2.0.3-py3-none-any.whl (31 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: colorama, bayesian-optimization\nSuccessfully installed bayesian-optimization-2.0.3 colorama-0.4.6\n\n\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.feature_selection import SelectKBest, f_classif\n# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from imblearn.over_sampling import SMOTE\n# from bayes_opt import BayesianOptimization\n\n# # Load and prepare data\n# filtered_df = pd.read_csv(\"filtered_df.csv\")\n# X = filtered_df.drop(['pttlwk', 'pxtlwkhr', 'pttlwkhr', 'pxtlwk'], axis=1)\n# y = filtered_df['pttlwk']\n\n# # Convert labels from 1.0/2.0 to 0/1\n# y = (y == 2.0).astype(int)\n\n# # Split the data first\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# # Reset indices to ensure alignment\n# X_train = X_train.reset_index(drop=True)\n# X_test = X_test.reset_index(drop=True)\n# y_train = y_train.reset_index(drop=True)\n# y_test = y_test.reset_index(drop=True)\n\n# # Scale the features\n# print(\"Scaling features...\")\n# scaler = StandardScaler()\n# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n# X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n# # Feature selection\n# print(\"Performing feature selection...\")\n# selector = SelectKBest(f_classif, k=50)\n# X_train_reduced = selector.fit_transform(X_train_scaled, y_train)\n# X_test_reduced = selector.transform(X_test_scaled)\n\n# # Get selected feature names\n# selected_features = X_train.columns[selector.get_support()].tolist()\n# X_train_reduced = pd.DataFrame(X_train_reduced, columns=selected_features)\n# X_test_reduced = pd.DataFrame(X_test_reduced, columns=selected_features)\n\n# # Calculate VIF\n# print(\"Performing multicollinearity analysis...\")\n# vif_data = pd.DataFrame()\n# vif_data[\"Feature\"] = selected_features\n# vif_data[\"VIF\"] = [variance_inflation_factor(X_train_reduced.values, i)\n#                    for i in range(X_train_reduced.shape[1])]\n# print(\"\\nVIF Results:\")\n# print(vif_data)\n\n# # Drop high VIF features\n# high_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\n# print(f\"\\nDropping features with high VIF: {high_vif_features}\")\n# X_train_vif = X_train_reduced.drop(columns=high_vif_features)\n# X_test_vif = X_test_reduced.drop(columns=high_vif_features)\n\n# # Outlier removal\n# print(\"\\nPerforming outlier analysis...\")\n# z_scores = np.abs((X_train_vif - X_train_vif.mean()) / X_train_vif.std())\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n\n# # Make sure indices align before filtering\n# X_train_clean = X_train_vif[non_outliers].reset_index(drop=True)\n# y_train_clean = pd.Series(y_train.values[non_outliers]).reset_index(drop=True)\n\n# # Apply SMOTE for class imbalance\n# print(\"\\nApplying SMOTE for class balancing...\")\n# smote = SMOTE(random_state=42)\n# X_train_balanced, y_train_balanced = smote.fit_resample(X_train_clean, y_train_clean)\n\n# # Convert to numpy arrays for XGBoost\n# X_train_balanced = np.array(X_train_balanced)\n# y_train_balanced = np.array(y_train_balanced)\n# X_test_vif_array = np.array(X_test_vif)\n\n# # Function for Bayesian Optimization\n# def xgb_evaluate(max_depth, learning_rate, n_estimators, subsample, colsample_bytree):\n#     params = {\n#         'max_depth': int(max_depth),\n#         'learning_rate': learning_rate,\n#         'n_estimators': int(n_estimators),\n#         'subsample': subsample,\n#         'colsample_bytree': colsample_bytree,\n#         'objective': 'binary:logistic',\n#         'eval_metric': 'logloss',\n#         'seed': 42\n#     }\n\n#     # Convert to DMatrix format\n#     dtrain = xgb.DMatrix(X_train_balanced, label=y_train_balanced)\n#     dtest = xgb.DMatrix(X_test_vif_array)\n\n#     # Train model\n#     model = xgb.train(params, dtrain, num_boost_round=int(n_estimators))\n\n#     # Make predictions\n#     y_pred = (model.predict(dtest) &gt; 0.5).astype(int)\n\n#     return accuracy_score(y_test, y_pred)\n\n# # Initialize Bayesian Optimization\n# print(\"\\nStarting Bayesian Optimization...\")\n# optimizer = BayesianOptimization(\n#     f=xgb_evaluate,\n#     pbounds={\n#         'max_depth': (3, 10),\n#         'learning_rate': (0.01, 0.3),\n#         'n_estimators': (50, 200),\n#         'subsample': (0.5, 1.0),\n#         'colsample_bytree': (0.5, 1.0)\n#     },\n#     random_state=42\n# )\n\n# # Run optimization\n# optimizer.maximize(\n#     init_points=5,\n#     n_iter=15\n# )\n\n# # Get best parameters\n# best_params = optimizer.max['params']\n# print(\"\\nBest parameters found:\")\n# print(best_params)\n\n# # Train final model with best parameters\n# print(\"\\nTraining final model with best parameters...\")\n# final_params = {\n#     'max_depth': int(best_params['max_depth']),\n#     'learning_rate': best_params['learning_rate'],\n#     'n_estimators': int(best_params['n_estimators']),\n#     'subsample': best_params['subsample'],\n#     'colsample_bytree': best_params['colsample_bytree'],\n#     'objective': 'binary:logistic',\n#     'eval_metric': 'logloss'\n# }\n\n# # Create final DMatrix objects\n# dtrain = xgb.DMatrix(X_train_balanced, label=y_train_balanced)\n# dtest = xgb.DMatrix(X_test_vif_array)\n\n# # Train final model\n# final_model = xgb.train(final_params, dtrain)\n# y_pred = (final_model.predict(dtest) &gt; 0.5).astype(int)\n\n# # Print results\n# print(\"\\nModel Performance Metrics:\")\n# print(\"\\nClassification Report:\")\n# print(classification_report(y_test, y_pred))\n\n# # Plot confusion matrix\n# plt.figure(figsize=(8, 6))\n# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n# plt.title('XGBoost Confusion Matrix')\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.tight_layout()\n# plt.show()\n\n# # Plot optimization history\n# plt.figure(figsize=(10, 5))\n# plt.plot(range(len(optimizer.space.target)), optimizer.space.target, '-o')\n# plt.xlabel('Iteration')\n# plt.ylabel('Model Accuracy')\n# plt.title('Bayesian Optimization History')\n# plt.axvline(x=5, color='r', linestyle='--', label='End of Random Search')\n# plt.legend()\n# plt.show()\n\n# # Feature importance\n# importance = final_model.get_score(importance_type='weight')\n# importance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\n# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# plt.figure(figsize=(12, 6))\n# sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n# plt.title('Top 10 Feature Importances')\n# plt.xlabel('Importance')\n# plt.ylabel('Feature')\n# plt.tight_layout()\n# plt.show()\n\nScaling features...\nPerforming feature selection...\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [  7   8   9  16  17  24  26  32  42  53  66  69  71  72  76  77  78  79\n  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n 107 109 115 160 173 174 175 176 177 178 179 180 181 182 195 196 197 198\n 199 200 201 202 203 204 205 206 207 208 211 212 213 214 215 218 219 220\n 221 222 223 224 225 238 239 240 241 242 243 244 245 246 247 248 249 250\n 251 255 256 257 258 259 260 261 263 264 267 270 271 272 274 291] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\nPerforming multicollinearity analysis...\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n  vif = 1. / (1. - r_squared_i)\n\n\n\nVIF Results:\n      Feature         VIF\n0    OCCURNUM         inf\n1       HUBUS    3.104158\n2     HUBUSL1    3.040994\n3    hehousut    1.053932\n4    hrnumhou    1.613758\n5    hefaminc    1.366005\n6    hxfaminc    1.041178\n7    PULINENO         inf\n8    PUIO1MFG    1.200420\n9    PUSLFPRX    1.730368\n10      perrp    3.068334\n11   pemaritl  450.277952\n12    pxrace1    1.239154\n13   pehspnon    3.504174\n14    peeduca    1.865216\n15   pespouse   10.326498\n16   pemntvty    5.751234\n17   pefntvty    5.795244\n18   pedipged    1.620007\n19      pecyc    1.261176\n20     pepar1    6.813180\n21     pepar2    7.247503\n22  pepar1typ    8.514617\n23  pepar2typ    8.421113\n24   prmarsta  547.232596\n25    prdthsp    3.022894\n26   prfamrel    2.549348\n27     prtage    1.580120\n28    pemjnum    1.034042\n29   peio1icd   11.798984\n30   prmjind1   31.276246\n31   prmjocc1   92.513207\n32   prdtind1   41.036307\n33   prdtocc1   34.092564\n34    prcowpg    1.424787\n35   prmjocgr   48.168168\n36   peernwkp    1.834483\n37    peernrt    2.477255\n38   peernhro    6.011818\n39   prhernal    4.562487\n40   pternhly    5.746613\n41   pternh1o    7.337398\n42    pternwa    1.804345\n43   ptio1ocd   94.423709\n44     gtcbsa    2.323201\n45   gtcbsast    1.893200\n46   gtcbsasz    3.823655\n47      gtcsa    1.555382\n48   gtmetsta    2.227311\n49   gtindvpc    1.217917\n\nDropping features with high VIF: ['OCCURNUM', 'PULINENO', 'pemaritl', 'pespouse', 'prmarsta', 'peio1icd', 'prmjind1', 'prmjocc1', 'prdtind1', 'prdtocc1', 'prmjocgr', 'ptio1ocd']\n\nPerforming outlier analysis...\n\nApplying SMOTE for class balancing...\n\nStarting Bayesian Optimization...\n|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n-------------------------------------------------------------------------------------\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:48] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 1         | 0.7732    | 0.6873    | 0.2857    | 8.124     | 139.8     | 0.578     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:49] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 2         | 0.7872    | 0.578     | 0.02684   | 9.063     | 140.2     | 0.854     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:51] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 3         | 0.7812    | 0.5103    | 0.2913    | 8.827     | 81.85     | 0.5909    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:53] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 4         | 0.7909    | 0.5917    | 0.09823   | 6.673     | 114.8     | 0.6456    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:54] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 5         | 0.7862    | 0.8059    | 0.05045   | 5.045     | 105.0     | 0.728     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:54] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 6         | 0.7849    | 0.68      | 0.2875    | 6.503     | 114.9     | 0.7288    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:55] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 7         | 0.7828    | 0.7325    | 0.2368    | 6.817     | 114.4     | 0.7645    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:55] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 8         | 0.7914    | 0.8502    | 0.08643   | 6.483     | 114.6     | 0.742     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:56] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 9         | 0.7805    | 0.6778    | 0.02618   | 6.569     | 114.7     | 0.8147    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:57] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 10        | 0.786     | 0.9038    | 0.06098   | 5.217     | 90.33     | 0.8629    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:57] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 11        | 0.7864    | 0.7456    | 0.08416   | 8.146     | 139.4     | 0.899     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:58] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 12        | 0.785     | 0.5377    | 0.2455    | 6.267     | 95.87     | 0.6494    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:59] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 13        | 0.781     | 0.5413    | 0.2721    | 7.156     | 162.5     | 0.874     |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:06:59] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 14        | 0.7825    | 0.8641    | 0.2036    | 9.962     | 111.0     | 0.7694    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:00] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 15        | 0.7889    | 0.5668    | 0.2238    | 5.109     | 78.43     | 0.6134    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:01] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 16        | 0.7899    | 0.6357    | 0.199     | 3.435     | 162.1     | 0.9796    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:01] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 17        | 0.7862    | 0.5819    | 0.03932   | 4.665     | 186.8     | 0.8976    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:02] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 18        | 0.7827    | 0.8887    | 0.1137    | 8.863     | 190.2     | 0.5815    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:03] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 19        | 0.7818    | 0.7528    | 0.1584    | 8.54      | 99.56     | 0.9612    |\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:04] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n| 20        | 0.7884    | 0.9319    | 0.1759    | 5.914     | 77.49     | 0.6407    |\n=====================================================================================\n\nBest parameters found:\n{'colsample_bytree': 0.8502042604408235, 'learning_rate': 0.08643263337802379, 'max_depth': 6.482992630463139, 'n_estimators': 114.61319594670005, 'subsample': 0.7420110182828307}\n\nTraining final model with best parameters...\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:07:05] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nModel Performance Metrics:\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.48      0.55      0.51      1364\n           1       0.86      0.82      0.84      4618\n\n    accuracy                           0.76      5982\n   macro avg       0.67      0.69      0.68      5982\nweighted avg       0.77      0.76      0.77      5982\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\nPUMS Classification model\n\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.feature_selection import RFECV, SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom joblib import parallel_backend\nimport xgboost as xgb\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv(\"psam_p34.csv\")\ndf2 = pd.read_csv(\"psam_p36.csv\")\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# Concatenate dataframes\nmerged_df = pd.concat([df, df2], axis=0)\n\n# Drop weight columns\npwgtp_cols = [col for col in merged_df.columns if 'PWGTP' in col]\nfiltered_df = merged_df.drop(columns=pwgtp_cols)\n\n# Transform JWTRNS (0 for values 1-10, 1 for value 11)\nfiltered_df['JWTRNS'] = (filtered_df['JWTRNS'] == 11).astype(int)\n\nprint(f\"Original shapes: df {df.shape}, df2 {df2.shape}\")\nprint(f\"Combined shape: {merged_df.shape}\")\nprint(f\"Final shape: {filtered_df.shape}\")\nprint(\"\\nJWTRNS distribution:\")\nprint(filtered_df['JWTRNS'].value_counts())\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nfiltered_df\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\npd.set_option('display.max_columns', None)\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\ndef clean_dataframe(df, column_threshold=0.1):\n    \"\"\"\n    Removes columns with more than `column_threshold`% NaN values,\n    then removes remaining rows with NaNs.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame\n    column_threshold (float): Threshold for dropping columns (default 40%)\n\n    Returns:\n    pd.DataFrame: Cleaned DataFrame\n    \"\"\"\n    # Calculate percentage of NaNs per column\n    nan_percentage = df.isna().mean()\n\n    # Drop columns exceeding the threshold\n    cols_to_drop = nan_percentage[nan_percentage &gt; column_threshold].index\n    df_cleaned = df.drop(columns=cols_to_drop)\n\n    # Drop remaining rows with NaNs\n    df_cleaned = df_cleaned.dropna()\n\n    return df_cleaned\n\nfiltered_df_new = clean_dataframe(filtered_df, column_threshold=0.1)  # Set 40% threshold\nprint(f\"Shape before: {filtered_df_new.shape}, Shape after: {filtered_df.shape}\")\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nfiltered_df_new\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nX = filtered_df_new.drop(['RT','SERIALNO','JWTRNS'], axis=1)\ny = filtered_df_new['JWTRNS']\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nX\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nprint(\"Performing initial feature selection...\")\nselector = SelectKBest(mutual_info_classif, k=50)  # Keep top 50 features\nX_reduced = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support(indices=True)]\nprint(\"Top 50 features selected.\")\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# Create a DataFrame with reduced features for VIF and further steps\nX = pd.DataFrame(X_reduced, columns=selected_features)\n\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data\n\n# Calculate VIF for all features\nprint(\"Performing multicollinearity analysis...\")\nvif_data = calculate_vif(X)\nprint(\"VIF Results:\")\nprint(vif_data)\n\n# Drop features with high VIF (&gt;10)\nhigh_vif_features = vif_data[vif_data[\"VIF\"] &gt; 10][\"Feature\"].tolist()\nprint(f\"Dropping features with high VIF: {high_vif_features}\")\nX = X.drop(columns=high_vif_features)\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# Recursive Feature Elimination\nprint(\"Performing Recursive Feature Elimination...\")\nwith parallel_backend('threading'):\n    rf_model = RandomForestClassifier(random_state=42)\n    rfecv = RFECV(estimator=rf_model, step=1, cv=5, scoring='accuracy', n_jobs=-1)\n    rfecv.fit(X_train, y_train)\n\n# Get selected features from RFE\nrfe_selected_features = rfecv.support_\nX_train_final = X_train.iloc[:, rfe_selected_features]\nX_test_final = X_test.iloc[:, rfe_selected_features]\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# # Outlier Analysis\n# print(\"Performing outlier analysis...\")\n# z_scores = np.abs((X_train_final - np.mean(X_train_final, axis=0)) / np.std(X_train_final, axis=0))\n# threshold = 3\n# non_outliers = (z_scores &lt; threshold).all(axis=1)\n# outlier_percentage = 100 * (1 - np.sum(non_outliers) / len(non_outliers))\n# print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n\n# # Align indices before filtering\n# X_train_final = X_train_final[non_outliers].reset_index(drop=True)\n# y_train = y_train.iloc[X_train.index[non_outliers]].reset_index(drop=True) # Use iloc and original X_train index to align\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nX_train_final\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# Convert to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train_final, label=y_train)\ndtest = xgb.DMatrix(X_test_final, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'seed': 42\n}\n\n# Cross-validation\nprint(\"Performing cross-validation...\")\nnum_round = 100\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nimport numpy as np\nfrom sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Calculate additional metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\n# Basic Metrics\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)  # Also known as True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Additional Metrics\nprevalence = (tp + fn) / total\npositive_predictive_value = tp / (tp + fp)  # Same as precision\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"Precision (Positive Predictive Value): {precision:.4f}\")\nprint(f\"Recall (Sensitivity/True Positive Rate): {recall:.4f}\")\nprint(f\"Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"False Positive Rate: {false_positive_rate:.4f}\")\nprint(f\"False Negative Rate: {false_negative_rate:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n\n# Calculate ROC and PR curves\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\n\n# Feature importance analysis\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df)\n\n# Plotting\nplt.figure(figsize=(15, 10))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred):.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nplt.plot(recall_curve, precision_curve, color='blue', lw=2,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\n\n# Plot 4: Confusion Matrix\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Print calibration metrics\nfrom sklearn.calibration import calibration_curve\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n\n# Plot calibration curve\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nimport statsmodels.api as sm\nimport shap\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# We add a constant for statsmodels\nX_train_final.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train_probit = sm.add_constant(X_train_final)\nprobit_model = sm.Probit(y_train, X_train_probit)\nprobit_results = probit_model.fit()\nprint(\"\\nProbit Model Summary:\")\nprint(probit_results.summary())\n\n# Optionally compute marginal effects\nprint(\"\\nMarginal Effects (Probit):\")\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nprint(\"\\nCalculating and plotting SHAP values for XGBoost model...\")\n\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test_final)\n\n# --- SHAP Beeswarm (summary) Plot ---\nprint(\"Generating SHAP Beeswarm plot...\")\nshap.summary_plot(shap_values, X_test_final)\n\n# --- SHAP Bar Plot of Mean Absolute SHAP Values ---\nprint(\"Generating SHAP Bar plot...\")\nshap.summary_plot(shap_values, X_test_final, plot_type=\"bar\")\n\n# (Optional) If you want to examine a single observation or more advanced plots:\n# idx_to_explain = 0  # index in X_test_final\n# shap.force_plot(\n#     explainer.expected_value,\n#     shap_values[idx_to_explain,:],\n#     X_test_final.iloc[idx_to_explain,:],\n#     matplotlib=True\n# )\n\n\nFailed to start the Kernel. \n\nUnable to start Kernel 'rasa (Python 3.10.8)' due to a timeout waiting for the ports to get used. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details."
  },
  {
    "objectID": "datadict.html",
    "href": "datadict.html",
    "title": "Data Dictionary (Data from CDC PLACES, EPA SLD, USGS Building Footprint)",
    "section": "",
    "text": "CDC PLACES\nGo to metadata source\n\nimport pandas as pd\nfrom itables import show\ndf1 = pd.read_csv(\"CDC_metadata.csv\")\nshow(df1)\n\n\n\n    \n      \n      Column Name\n      Description\n      Type\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\nEPA SLD\nGo to metadata source\n\ndf2 = pd.read_csv(\"EPA_metadata.csv\")\nshow(df2)\n\n\n\n    \n      \n      Field Name\n      Description\n      Data Source\n      Geographic Coverage*\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\nUSGS Building Footprint\nGo to metadata source\n\ndf3 = pd.read_csv(\"USGSBFP_metadata.csv\")\nshow(df3)\n\n\n\n    \n      \n      Attribute Label\n      Attribute Definition\n      Attribute Definition Source\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "baselinemodels.html",
    "href": "baselinemodels.html",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "",
    "text": ":::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-results”}\n\n\n\nCalculating additional performance metrics...\n\nComprehensive Model Performance Metrics:\nRMSE: 0.2913\nMSE: 0.0849\nMAE: 0.1675\nR-squared: 0.5357\n\nPrecision: 0.7888\nRecall/Sensitivity: 0.7051\nSpecificity: 0.9401\nF1 Score: 0.7446\nPrevalence: 0.2407\nNegative Predictive Value: 0.9095\nPositive Likelihood Ratio: 11.7783\nNegative Likelihood Ratio: 0.3136\n\n\n\n\n\n\n\n\n\n\nGenerating SHAP values...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n:::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-probit”}\n\n\n\nFitting Probit model for effect sizes and p-values...\nRemoved 471 low variance columns\nRemoved 10 highly correlated features\nOptimization terminated successfully.\n         Current function value: 0.356300\n         Iterations 9\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14055\nMethod:                           MLE   Df Model:                          199\nDate:                Thu, 06 Feb 2025   Pseudo R-squ.:                  0.3545\nTime:                        08:33:29   Log-Likelihood:                -5079.1\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:                  HC0   LLR p-value:                     0.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -0.4438        nan        nan        nan         nan         nan\nhetelhhd_dummy_2        -0.3904      0.125     -3.120      0.002      -0.636      -0.145\nhetelavl_dummy_2         0.0665      0.167      0.399      0.690      -0.260       0.393\nhefaminc_dummy_10        0.0909      0.160      0.568      0.570      -0.223       0.405\nhefaminc_dummy_11       -0.1193      0.144     -0.826      0.409      -0.402       0.164\nhefaminc_dummy_12        0.0291      0.188      0.155      0.877      -0.339       0.397\nhefaminc_dummy_13        0.1481      0.108      1.368      0.171      -0.064       0.360\nhefaminc_dummy_14        0.1722      0.113      1.525      0.127      -0.049       0.394\nhefaminc_dummy_15        0.2896      0.101      2.859      0.004       0.091       0.488\nhefaminc_dummy_16        0.4533      0.122      3.720      0.000       0.214       0.692\nhefaminc_dummy_7        -0.1080      0.226     -0.477      0.633      -0.552       0.336\nhefaminc_dummy_8         0.2058      0.158      1.299      0.194      -0.105       0.516\nhefaminc_dummy_9        -0.2087      0.181     -1.151      0.250      -0.564       0.147\nhrnumhou                -0.0828      0.028     -3.000      0.003      -0.137      -0.029\nhrhtype_dummy_3         -0.0427      0.107     -0.398      0.690      -0.253       0.168\nhrhtype_dummy_4          0.0338      0.104      0.326      0.744      -0.169       0.237\nhrhtype_dummy_6         -0.0910      0.177     -0.513      0.608      -0.438       0.256\nhrhtype_dummy_7         -0.1437      0.182     -0.791      0.429      -0.500       0.212\nHUBUS_dummy_2           -0.3604      0.080     -4.485      0.000      -0.518      -0.203\ngestfips_dummy_36        0.0539      0.188      0.287      0.774      -0.315       0.422\ngtcbsa_dummy_10580       0.1101        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.1873      0.351     -0.534      0.594      -0.875       0.500\ngtcbsa_dummy_15380      -0.1131   5.19e+07  -2.18e-09      1.000   -1.02e+08    1.02e+08\ngtcbsa_dummy_35620       0.1921    1.6e+06    1.2e-07      1.000   -3.13e+06    3.13e+06\ngtcbsa_dummy_37980      -0.2836        nan        nan        nan         nan         nan\ngtcbsa_dummy_40380      -0.0566   5.96e+07   -9.5e-10      1.000   -1.17e+08    1.17e+08\ngtcbsa_dummy_45060      -0.1722        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.4605      0.248      1.857      0.063      -0.026       0.947\ngtco_dummy_103          -0.1515      0.125     -1.210      0.226      -0.397       0.094\ngtco_dummy_119          -0.2373      0.131     -1.813      0.070      -0.494       0.019\ngtco_dummy_13            0.0841      0.097      0.871      0.384      -0.105       0.273\ngtco_dummy_17            0.0980      0.115      0.849      0.396      -0.128       0.324\ngtco_dummy_23           -0.0961      0.100     -0.959      0.338      -0.293       0.100\ngtco_dummy_27            0.3721      0.096      3.857      0.000       0.183       0.561\ngtco_dummy_3             0.1696      0.094      1.813      0.070      -0.014       0.353\ngtco_dummy_31            0.2710      0.132      2.055      0.040       0.013       0.530\ngtco_dummy_35            0.0497      0.129      0.385      0.700      -0.203       0.302\ngtco_dummy_39           -0.0034      0.136     -0.025      0.980      -0.270       0.263\ngtco_dummy_47            0.7798      0.156      4.989      0.000       0.473       1.086\ngtco_dummy_5             0.5346      0.146      3.660      0.000       0.248       0.821\ngtco_dummy_55           -0.1912      0.170     -1.122      0.262      -0.525       0.143\ngtco_dummy_59           -0.3640      0.131     -2.783      0.005      -0.620      -0.108\ngtco_dummy_61            0.8676      0.159      5.457      0.000       0.556       1.179\ngtco_dummy_67           -0.1184      0.411     -0.288      0.773      -0.924       0.687\ngtco_dummy_7            -0.0162   4.53e+06  -3.57e-09      1.000   -8.89e+06    8.89e+06\ngtco_dummy_71            0.0603      0.148      0.409      0.683      -0.229       0.349\ngtco_dummy_81            0.7921      0.161      4.911      0.000       0.476       1.108\ngtco_dummy_85            0.1173      0.232      0.507      0.612      -0.336       0.571\ngtco_dummy_87           -1.0649      0.195     -5.458      0.000      -1.447      -0.683\ngtcbsast_dummy_2        -0.1585      0.101     -1.570      0.116      -0.356       0.039\ngtcbsast_dummy_3        -0.1579        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0213        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.8703      0.179     -4.871      0.000      -1.220      -0.520\ngtindvpc_dummy_2        -0.2450      0.176     -1.394      0.163      -0.589       0.099\ngtcbsasz_dummy_2        -0.3653      0.219     -1.665      0.096      -0.795       0.065\ngtcbsasz_dummy_3        -0.3957      0.240     -1.650      0.099      -0.866       0.074\ngtcbsasz_dummy_4        -0.0621        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.1698        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0915   8.05e+05  -1.14e-07      1.000   -1.58e+06    1.58e+06\ngtcsa_dummy_104         -0.2497      0.207     -1.208      0.227      -0.655       0.155\ngtcsa_dummy_428          0.2514      0.228      1.104      0.270      -0.195       0.698\nperrp_dummy_41.0         0.1192      0.213      0.560      0.576      -0.298       0.537\nperrp_dummy_42.0        -0.2038      0.145     -1.410      0.159      -0.487       0.080\nperrp_dummy_44.0         0.3822      0.169      2.267      0.023       0.052       0.713\nperrp_dummy_48.0        -0.3012      0.164     -1.836      0.066      -0.623       0.020\nperrp_dummy_50.0        -0.1561      0.267     -0.585      0.559      -0.680       0.367\nperrp_dummy_51.0        -0.0459      0.280     -0.164      0.870      -0.594       0.502\nperrp_dummy_52.0        -0.4274      0.318     -1.346      0.178      -1.050       0.195\nperrp_dummy_55.0         0.6969      0.180      3.872      0.000       0.344       1.050\nprtage                  -0.0426      0.021     -2.026      0.043      -0.084      -0.001\npemaritl_dummy_2.0      -0.1861      0.146     -1.273      0.203      -0.473       0.100\npemaritl_dummy_3.0       0.0187      0.137      0.136      0.891      -0.250       0.288\npemaritl_dummy_4.0      -0.0314      0.105     -0.298      0.766      -0.238       0.175\npemaritl_dummy_5.0      -0.1715      0.139     -1.238      0.216      -0.443       0.100\npemaritl_dummy_6.0      -0.0865      0.105     -0.826      0.409      -0.292       0.119\npesex_dummy_2.0          0.2232      0.035      6.335      0.000       0.154       0.292\npeeduca_dummy_33.0       0.3720      0.378      0.984      0.325      -0.369       1.113\npeeduca_dummy_37.0       0.1358      0.325      0.418      0.676      -0.500       0.772\npeeduca_dummy_38.0       0.0823      0.326      0.252      0.801      -0.557       0.721\npeeduca_dummy_39.0       0.3780      0.200      1.890      0.059      -0.014       0.770\npeeduca_dummy_40.0       0.6360      0.202      3.145      0.002       0.240       1.032\npeeduca_dummy_41.0       0.5762      0.222      2.600      0.009       0.142       1.010\npeeduca_dummy_42.0       0.6776      0.206      3.290      0.001       0.274       1.081\npeeduca_dummy_43.0       0.8655      0.200      4.330      0.000       0.474       1.257\npeeduca_dummy_44.0       1.0688      0.202      5.287      0.000       0.673       1.465\npeeduca_dummy_45.0       1.1099      0.219      5.058      0.000       0.680       1.540\npeeduca_dummy_46.0       1.2331      0.216      5.711      0.000       0.810       1.656\nptdtrace_dummy_2.0      -0.0249      0.055     -0.455      0.649      -0.132       0.082\nptdtrace_dummy_4.0      -0.0388      0.074     -0.523      0.601      -0.184       0.106\nprdthsp_dummy_2.0        0.1004      0.147      0.685      0.493      -0.187       0.388\nprdthsp_dummy_4.0       -0.3129      0.209     -1.497      0.134      -0.723       0.097\nprdthsp_dummy_6.0       -0.2805      0.202     -1.391      0.164      -0.676       0.115\nprdthsp_dummy_7.0        0.0820      0.159      0.516      0.606      -0.230       0.394\nPUCHINHH_dummy_3.0      -0.2696      0.195     -1.381      0.167      -0.652       0.113\nPUCHINHH_dummy_9.0      -0.0507      0.138     -0.367      0.714      -0.321       0.220\nprfamrel_dummy_1.0      -0.0493      0.142     -0.348      0.728      -0.327       0.228\nprfamrel_dummy_4.0      -0.2085      0.245     -0.850      0.396      -0.689       0.272\nprfamtyp_dummy_3.0      -0.0399      0.255     -0.156      0.876      -0.539       0.460\nprfamtyp_dummy_5.0      -0.3361      0.213     -1.580      0.114      -0.753       0.081\npehspnon_dummy_2.0       0.1137      0.104      1.091      0.275      -0.091       0.318\npenatvty_dummy_207.0     0.0231      0.166      0.139      0.889      -0.302       0.348\npenatvty_dummy_210.0     0.0226      0.165      0.137      0.891      -0.301       0.346\npenatvty_dummy_303.0     0.1393      0.304      0.458      0.647      -0.457       0.736\npenatvty_dummy_329.0     0.1313      0.222      0.591      0.555      -0.304       0.567\npenatvty_dummy_333.0    -0.3091      0.254     -1.215      0.224      -0.808       0.189\npenatvty_dummy_365.0     0.1572      0.327      0.481      0.631      -0.484       0.798\npenatvty_dummy_57.0      0.3278      0.143      2.288      0.022       0.047       0.609\npemntvty_dummy_207.0    -0.1621      0.231     -0.703      0.482      -0.614       0.290\npemntvty_dummy_210.0     0.3692      0.150      2.465      0.014       0.076       0.663\npemntvty_dummy_233.0     0.3973      0.362      1.098      0.272      -0.312       1.106\npemntvty_dummy_303.0    -0.4136      0.335     -1.235      0.217      -1.070       0.243\npemntvty_dummy_329.0    -0.3135      0.326     -0.962      0.336      -0.952       0.325\npemntvty_dummy_332.0    -0.4948      0.165     -2.991      0.003      -0.819      -0.171\npemntvty_dummy_333.0     0.4170      0.216      1.934      0.053      -0.006       0.840\npemntvty_dummy_364.0    -0.6624      0.368     -1.802      0.071      -1.383       0.058\npemntvty_dummy_365.0     1.2720      0.230      5.533      0.000       0.821       1.723\npemntvty_dummy_57.0     -0.0017      0.069     -0.025      0.980      -0.137       0.134\npemntvty_dummy_73.0     -0.1555      0.232     -0.670      0.503      -0.610       0.299\npefntvty_dummy_120.0    -0.1514      0.134     -1.133      0.257      -0.413       0.110\npefntvty_dummy_207.0     0.2339      0.227      1.031      0.303      -0.211       0.679\npefntvty_dummy_233.0    -0.4991      0.378     -1.321      0.186      -1.239       0.241\npefntvty_dummy_303.0    -0.0906      0.288     -0.314      0.753      -0.656       0.475\npefntvty_dummy_329.0     0.5558      0.298      1.868      0.062      -0.027       1.139\npefntvty_dummy_364.0     0.4193      0.371      1.129      0.259      -0.309       1.147\npefntvty_dummy_365.0    -1.0838      0.290     -3.732      0.000      -1.653      -0.515\npefntvty_dummy_57.0      0.0957      0.069      1.388      0.165      -0.039       0.231\npefntvty_dummy_73.0      0.3499      0.231      1.512      0.130      -0.104       0.803\nprcitshp_dummy_4.0       0.4262      0.148      2.876      0.004       0.136       0.717\nprcitshp_dummy_5.0       0.4360      0.161      2.706      0.007       0.120       0.752\nprinuyer_dummy_11.0     -0.0443      0.139     -0.319      0.750      -0.316       0.228\nprinuyer_dummy_12.0     -0.1863      0.148     -1.256      0.209      -0.477       0.104\nprinuyer_dummy_14.0     -0.0306      0.146     -0.209      0.834      -0.317       0.256\nprinuyer_dummy_15.0     -0.3445      0.178     -1.933      0.053      -0.694       0.005\nprinuyer_dummy_16.0     -0.1977      0.128     -1.541      0.123      -0.449       0.054\nprinuyer_dummy_17.0     -0.2555      0.122     -2.094      0.036      -0.495      -0.016\nprinuyer_dummy_18.0     -0.0259      0.150     -0.172      0.863      -0.320       0.268\nprinuyer_dummy_19.0     -0.0810      0.153     -0.528      0.597      -0.381       0.219\nprinuyer_dummy_20.0     -0.4099      0.164     -2.504      0.012      -0.731      -0.089\nprinuyer_dummy_21.0     -0.0698      0.150     -0.466      0.641      -0.363       0.224\nprinuyer_dummy_22.0     -0.0406      0.142     -0.287      0.774      -0.318       0.237\nprinuyer_dummy_23.0     -0.1634      0.161     -1.017      0.309      -0.478       0.152\nprinuyer_dummy_24.0     -0.4663      0.160     -2.914      0.004      -0.780      -0.153\nprinuyer_dummy_25.0     -0.4639      0.148     -3.136      0.002      -0.754      -0.174\nprinuyer_dummy_26.0     -0.2901      0.161     -1.803      0.071      -0.606       0.025\nprinuyer_dummy_27.0     -0.5785      0.189     -3.068      0.002      -0.948      -0.209\nprinuyer_dummy_28.0     -0.1560      0.170     -0.919      0.358      -0.489       0.177\npemjot_dummy_2.0        -0.4350      0.064     -6.818      0.000      -0.560      -0.310\npehruslt                -0.0201      0.021     -0.977      0.329      -0.060       0.020\npehractt                 0.0488      0.021      2.346      0.019       0.008       0.090\npeio1cow_dummy_2.0      -0.6072      0.114     -5.311      0.000      -0.831      -0.383\npeio1cow_dummy_3.0      -0.9695      0.117     -8.289      0.000      -1.199      -0.740\npeio1cow_dummy_4.0      -0.4798      0.108     -4.428      0.000      -0.692      -0.267\npeio1cow_dummy_5.0      -0.3845      0.119     -3.242      0.001      -0.617      -0.152\npeio1cow_dummy_6.0      -0.1254      0.126     -0.997      0.319      -0.372       0.121\npeio1cow_dummy_7.0      -0.2443      0.125     -1.951      0.051      -0.490       0.001\nprdtind1_dummy_19.0      0.2093      0.117      1.792      0.073      -0.020       0.438\nprdtind1_dummy_21.0     -0.0161      0.103     -0.156      0.876      -0.218       0.186\nprdtind1_dummy_22.0     -0.4119      0.072     -5.711      0.000      -0.553      -0.271\nprdtind1_dummy_23.0     -0.8310      0.130     -6.409      0.000      -1.085      -0.577\nprdtind1_dummy_32.0      0.2446      0.069      3.548      0.000       0.109       0.380\nprdtind1_dummy_33.0      0.5550      0.103      5.365      0.000       0.352       0.758\nprdtind1_dummy_34.0     -0.0839      0.108     -0.774      0.439      -0.296       0.129\nprdtind1_dummy_36.0      0.3042      0.058      5.250      0.000       0.191       0.418\nprdtind1_dummy_38.0     -0.0279      0.095     -0.292      0.770      -0.215       0.159\nprdtind1_dummy_4.0      -0.3925      0.097     -4.053      0.000      -0.582      -0.203\nprdtind1_dummy_40.0     -0.3013      0.080     -3.743      0.000      -0.459      -0.144\nprdtind1_dummy_41.0     -0.3179      0.096     -3.320      0.001      -0.506      -0.130\nprdtind1_dummy_42.0     -0.1392      0.077     -1.810      0.070      -0.290       0.012\nprdtind1_dummy_43.0     -0.1846      0.098     -1.880      0.060      -0.377       0.008\nprdtind1_dummy_44.0     -0.4101      0.103     -3.998      0.000      -0.611      -0.209\nprdtind1_dummy_46.0     -0.7857      0.114     -6.917      0.000      -1.008      -0.563\nprdtind1_dummy_48.0     -0.6080      0.178     -3.407      0.001      -0.958      -0.258\nprdtind1_dummy_49.0     -0.0754      0.119     -0.633      0.527      -0.309       0.158\nprdtind1_dummy_51.0     -0.1874      0.104     -1.808      0.071      -0.391       0.016\nprdtocc1_dummy_10.0     -1.0416      0.087    -11.907      0.000      -1.213      -0.870\nprdtocc1_dummy_11.0     -0.7223      0.098     -7.346      0.000      -0.915      -0.530\nprdtocc1_dummy_12.0     -1.0094      0.172     -5.859      0.000      -1.347      -0.672\nprdtocc1_dummy_13.0     -0.6446      0.129     -4.994      0.000      -0.898      -0.392\nprdtocc1_dummy_14.0     -0.8193      0.114     -7.172      0.000      -1.043      -0.595\nprdtocc1_dummy_15.0     -0.5606      0.132     -4.255      0.000      -0.819      -0.302\nprdtocc1_dummy_16.0     -0.1442      0.066     -2.201      0.028      -0.273      -0.016\nprdtocc1_dummy_17.0     -0.1676      0.057     -2.914      0.004      -0.280      -0.055\nprdtocc1_dummy_19.0     -0.8087      0.130     -6.209      0.000      -1.064      -0.553\nprdtocc1_dummy_2.0       0.2930      0.055      5.292      0.000       0.185       0.402\nprdtocc1_dummy_20.0     -0.8588      0.137     -6.246      0.000      -1.128      -0.589\nprdtocc1_dummy_21.0     -0.9260      0.117     -7.929      0.000      -1.155      -0.697\nprdtocc1_dummy_22.0     -1.6501      0.205     -8.037      0.000      -2.053      -1.248\nprdtocc1_dummy_3.0       0.4683      0.071      6.626      0.000       0.330       0.607\nprdtocc1_dummy_4.0      -0.0518      0.095     -0.544      0.587      -0.239       0.135\nprdtocc1_dummy_5.0      -0.0389      0.115     -0.337      0.736      -0.265       0.187\nprdtocc1_dummy_6.0      -0.2713      0.092     -2.962      0.003      -0.451      -0.092\nprdtocc1_dummy_7.0      -0.0872      0.101     -0.866      0.386      -0.284       0.110\nprdtocc1_dummy_8.0      -0.7237      0.081     -8.986      0.000      -0.882      -0.566\nprdtocc1_dummy_9.0      -0.0342      0.081     -0.422      0.673      -0.193       0.125\npternwa                  0.0320      0.023      1.386      0.166      -0.013       0.077\nptwk_dummy_1            -0.0838      0.207     -0.404      0.686      -0.490       0.322\nprchld_dummy_1.0        -0.0076      0.085     -0.090      0.929      -0.174       0.159\nprchld_dummy_10.0       -0.1151      0.099     -1.159      0.246      -0.310       0.079\nprchld_dummy_2.0        -0.1091      0.108     -1.012      0.312      -0.321       0.102\nprchld_dummy_3.0        -0.0836      0.067     -1.251      0.211      -0.215       0.047\nprchld_dummy_4.0        -0.1046      0.068     -1.530      0.126      -0.239       0.029\nprchld_dummy_5.0        -0.0709      0.107     -0.661      0.509      -0.281       0.139\nprchld_dummy_8.0        -0.1087      0.122     -0.887      0.375      -0.349       0.131\nprnmchld                 0.0549      0.028      1.977      0.048       0.000       0.109\n========================================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n========================================================================================\n                          dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nhetelhhd_dummy_2        -0.0778        nan        nan        nan         nan         nan\nhetelavl_dummy_2         0.0133      0.032      0.409      0.682      -0.050       0.077\nhefaminc_dummy_10        0.0181      0.026      0.687      0.492      -0.034       0.070\nhefaminc_dummy_11       -0.0238      0.033     -0.714      0.475      -0.089       0.041\nhefaminc_dummy_12        0.0058      0.036      0.163      0.871      -0.064       0.076\nhefaminc_dummy_13        0.0295        nan        nan        nan         nan         nan\nhefaminc_dummy_14        0.0343        nan        nan        nan         nan         nan\nhefaminc_dummy_15        0.0577        nan        nan        nan         nan         nan\nhefaminc_dummy_16        0.0903        nan        nan        nan         nan         nan\nhefaminc_dummy_7        -0.0215      0.049     -0.438      0.661      -0.118       0.075\nhefaminc_dummy_8         0.0410        nan        nan        nan         nan         nan\nhefaminc_dummy_9        -0.0416      0.038     -1.084      0.278      -0.117       0.034\nhrnumhou                -0.0165        nan        nan        nan         nan         nan\nhrhtype_dummy_3         -0.0085      0.021     -0.399      0.690      -0.050       0.033\nhrhtype_dummy_4          0.0067      0.020      0.336      0.737      -0.033       0.046\nhrhtype_dummy_6         -0.0181      0.034     -0.538      0.590      -0.084       0.048\nhrhtype_dummy_7         -0.0286      0.032     -0.900      0.368      -0.091       0.034\nHUBUS_dummy_2           -0.0718        nan        nan        nan         nan         nan\ngestfips_dummy_36        0.0107      0.038      0.281      0.779      -0.064       0.086\ngtcbsa_dummy_10580       0.0219        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.0373      0.068     -0.549      0.583      -0.171       0.096\ngtcbsa_dummy_15380      -0.0226   1.03e+07  -2.19e-09      1.000   -2.02e+07    2.02e+07\ngtcbsa_dummy_35620       0.0383   2.78e+05   1.38e-07      1.000   -5.45e+05    5.45e+05\ngtcbsa_dummy_37980      -0.0565   9.01e+04  -6.28e-07      1.000   -1.77e+05    1.77e+05\ngtcbsa_dummy_40380      -0.0113   1.19e+07  -9.51e-10      1.000   -2.33e+07    2.33e+07\ngtcbsa_dummy_45060      -0.0343        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.0918        nan        nan        nan         nan         nan\ngtco_dummy_103          -0.0302      0.024     -1.278      0.201      -0.077       0.016\ngtco_dummy_119          -0.0473      0.020     -2.383      0.017      -0.086      -0.008\ngtco_dummy_13            0.0168      0.020      0.854      0.393      -0.022       0.055\ngtco_dummy_17            0.0195      0.021      0.935      0.350      -0.021       0.060\ngtco_dummy_23           -0.0192      0.017     -1.129      0.259      -0.052       0.014\ngtco_dummy_27            0.0742        nan        nan        nan         nan         nan\ngtco_dummy_3             0.0338      0.006      5.359      0.000       0.021       0.046\ngtco_dummy_31            0.0540        nan        nan        nan         nan         nan\ngtco_dummy_35            0.0099      0.026      0.381      0.704      -0.041       0.061\ngtco_dummy_39           -0.0007      0.027     -0.025      0.980      -0.054       0.052\ngtco_dummy_47            0.1554        nan        nan        nan         nan         nan\ngtco_dummy_5             0.1066        nan        nan        nan         nan         nan\ngtco_dummy_55           -0.0381      0.011     -3.559      0.000      -0.059      -0.017\ngtco_dummy_59           -0.0726        nan        nan        nan         nan         nan\ngtco_dummy_61            0.1729        nan        nan        nan         nan         nan\ngtco_dummy_67           -0.0236      0.081     -0.292      0.771      -0.182       0.135\ngtco_dummy_7            -0.0032   9.03e+05  -3.58e-09      1.000   -1.77e+06    1.77e+06\ngtco_dummy_71            0.0120      0.027      0.445      0.656      -0.041       0.065\ngtco_dummy_81            0.1579        nan        nan        nan         nan         nan\ngtco_dummy_85            0.0234      0.049      0.480      0.631      -0.072       0.119\ngtco_dummy_87           -0.2123        nan        nan        nan         nan         nan\ngtcbsast_dummy_2        -0.0316      0.018     -1.771      0.077      -0.067       0.003\ngtcbsast_dummy_3        -0.0315        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0042        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.1735        nan        nan        nan         nan         nan\ngtindvpc_dummy_2        -0.0488      0.022     -2.212      0.027      -0.092      -0.006\ngtcbsasz_dummy_2        -0.0728      0.021     -3.396      0.001      -0.115      -0.031\ngtcbsasz_dummy_3        -0.0789      0.017     -4.561      0.000      -0.113      -0.045\ngtcbsasz_dummy_4        -0.0124        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.0338        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0182   1.56e+05  -1.17e-07      1.000   -3.05e+05    3.05e+05\ngtcsa_dummy_104         -0.0498      0.037     -1.359      0.174      -0.122       0.022\ngtcsa_dummy_428          0.0501      0.034      1.478      0.139      -0.016       0.117\nperrp_dummy_41.0         0.0238      0.017      1.410      0.159      -0.009       0.057\nperrp_dummy_42.0        -0.0406      0.048     -0.851      0.395      -0.134       0.053\nperrp_dummy_44.0         0.0762        nan        nan        nan         nan         nan\nperrp_dummy_48.0        -0.0600      0.056     -1.081      0.280      -0.169       0.049\nperrp_dummy_50.0        -0.0311      0.064     -0.487      0.626      -0.156       0.094\nperrp_dummy_51.0        -0.0091      0.059     -0.155      0.877      -0.125       0.107\nperrp_dummy_52.0        -0.0852      0.080     -1.061      0.288      -0.243       0.072\nperrp_dummy_55.0         0.1389        nan        nan        nan         nan         nan\nprtage                  -0.0085        nan        nan        nan         nan         nan\npemaritl_dummy_2.0      -0.0371      0.012     -3.160      0.002      -0.060      -0.014\npemaritl_dummy_3.0       0.0037      0.027      0.136      0.892      -0.050       0.058\npemaritl_dummy_4.0      -0.0063      0.020     -0.312      0.755      -0.046       0.033\npemaritl_dummy_5.0      -0.0342      0.019     -1.768      0.077      -0.072       0.004\npemaritl_dummy_6.0      -0.0172      0.016     -1.053      0.292      -0.049       0.015\npesex_dummy_2.0          0.0445        nan        nan        nan         nan         nan\npeeduca_dummy_33.0       0.0741      0.088      0.838      0.402      -0.099       0.248\npeeduca_dummy_37.0       0.0271      0.071      0.382      0.702      -0.112       0.166\npeeduca_dummy_38.0       0.0164      0.069      0.237      0.813      -0.119       0.152\npeeduca_dummy_39.0       0.0753      0.050      1.498      0.134      -0.023       0.174\npeeduca_dummy_40.0       0.1268      0.042      3.015      0.003       0.044       0.209\npeeduca_dummy_41.0       0.1148      0.049      2.341      0.019       0.019       0.211\npeeduca_dummy_42.0       0.1351      0.038      3.541      0.000       0.060       0.210\npeeduca_dummy_43.0       0.1725        nan        nan        nan         nan         nan\npeeduca_dummy_44.0       0.2130        nan        nan        nan         nan         nan\npeeduca_dummy_45.0       0.2212        nan        nan        nan         nan         nan\npeeduca_dummy_46.0       0.2458        nan        nan        nan         nan         nan\nptdtrace_dummy_2.0      -0.0050      0.010     -0.480      0.632      -0.025       0.015\nptdtrace_dummy_4.0      -0.0077      0.014     -0.549      0.583      -0.035       0.020\nprdthsp_dummy_2.0        0.0200      0.033      0.600      0.548      -0.045       0.085\nprdthsp_dummy_4.0       -0.0624        nan        nan        nan         nan         nan\nprdthsp_dummy_6.0       -0.0559        nan        nan        nan         nan         nan\nprdthsp_dummy_7.0        0.0163      0.034      0.482      0.630      -0.050       0.083\nPUCHINHH_dummy_3.0      -0.0537        nan        nan        nan         nan         nan\nPUCHINHH_dummy_9.0      -0.0101      0.023     -0.436      0.663      -0.056       0.035\nprfamrel_dummy_1.0      -0.0098      0.035     -0.277      0.782      -0.079       0.060\nprfamrel_dummy_4.0      -0.0416      0.047     -0.884      0.376      -0.134       0.051\nprfamtyp_dummy_3.0      -0.0079      0.048     -0.165      0.869      -0.102       0.086\nprfamtyp_dummy_5.0      -0.0670      0.063     -1.058      0.290      -0.191       0.057\npehspnon_dummy_2.0       0.0227      0.024      0.926      0.355      -0.025       0.071\npenatvty_dummy_207.0     0.0046      0.035      0.133      0.894      -0.063       0.072\npenatvty_dummy_210.0     0.0045      0.034      0.134      0.893      -0.061       0.070\npenatvty_dummy_303.0     0.0278      0.060      0.461      0.645      -0.090       0.146\npenatvty_dummy_329.0     0.0262      0.045      0.577      0.564      -0.063       0.115\npenatvty_dummy_333.0    -0.0616      0.026     -2.405      0.016      -0.112      -0.011\npenatvty_dummy_365.0     0.0313      0.064      0.490      0.624      -0.094       0.157\npenatvty_dummy_57.0      0.0653      0.101      0.644      0.520      -0.134       0.264\npemntvty_dummy_207.0    -0.0323      0.044     -0.731      0.465      -0.119       0.054\npemntvty_dummy_210.0     0.0736        nan        nan        nan         nan         nan\npemntvty_dummy_233.0     0.0792      0.034      2.296      0.022       0.012       0.147\npemntvty_dummy_303.0    -0.0824      0.053     -1.553      0.120      -0.186       0.022\npemntvty_dummy_329.0    -0.0625      0.063     -0.988      0.323      -0.186       0.062\npemntvty_dummy_332.0    -0.0986        nan        nan        nan         nan         nan\npemntvty_dummy_333.0     0.0831        nan        nan        nan         nan         nan\npemntvty_dummy_364.0    -0.1320      0.051     -2.587      0.010      -0.232      -0.032\npemntvty_dummy_365.0     0.2535        nan        nan        nan         nan         nan\npemntvty_dummy_57.0     -0.0003      0.014     -0.024      0.981      -0.029       0.028\npemntvty_dummy_73.0     -0.0310      0.043     -0.725      0.469      -0.115       0.053\npefntvty_dummy_120.0    -0.0302        nan        nan        nan         nan         nan\npefntvty_dummy_207.0     0.0466      0.036      1.304      0.192      -0.023       0.117\npefntvty_dummy_233.0    -0.0995      0.004    -24.044      0.000      -0.108      -0.091\npefntvty_dummy_303.0    -0.0181      0.054     -0.332      0.740      -0.125       0.089\npefntvty_dummy_329.0     0.1108      0.019      5.941      0.000       0.074       0.147\npefntvty_dummy_364.0     0.0836      0.072      1.154      0.248      -0.058       0.225\npefntvty_dummy_365.0    -0.2160        nan        nan        nan         nan         nan\npefntvty_dummy_57.0      0.0191      0.030      0.643      0.520      -0.039       0.077\npefntvty_dummy_73.0      0.0697      0.058      1.196      0.232      -0.045       0.184\nprcitshp_dummy_4.0       0.0849      0.105      0.810      0.418      -0.120       0.290\nprcitshp_dummy_5.0       0.0869      0.105      0.825      0.410      -0.120       0.293\nprinuyer_dummy_11.0     -0.0088      0.024     -0.372      0.710      -0.055       0.038\nprinuyer_dummy_12.0     -0.0371        nan        nan        nan         nan         nan\nprinuyer_dummy_14.0     -0.0061      0.028     -0.220      0.826      -0.060       0.048\nprinuyer_dummy_15.0     -0.0687        nan        nan        nan         nan         nan\nprinuyer_dummy_16.0     -0.0394        nan        nan        nan         nan         nan\nprinuyer_dummy_17.0     -0.0509        nan        nan        nan         nan         nan\nprinuyer_dummy_18.0     -0.0052      0.029     -0.180      0.857      -0.061       0.051\nprinuyer_dummy_19.0     -0.0161      0.026     -0.628      0.530      -0.066       0.034\nprinuyer_dummy_20.0     -0.0817        nan        nan        nan         nan         nan\nprinuyer_dummy_21.0     -0.0139      0.024     -0.584      0.559      -0.061       0.033\nprinuyer_dummy_22.0     -0.0081      0.026     -0.312      0.755      -0.059       0.043\nprinuyer_dummy_23.0     -0.0326      0.017     -1.917      0.055      -0.066       0.001\nprinuyer_dummy_24.0     -0.0930        nan        nan        nan         nan         nan\nprinuyer_dummy_25.0     -0.0925        nan        nan        nan         nan         nan\nprinuyer_dummy_26.0     -0.0578        nan        nan        nan         nan         nan\nprinuyer_dummy_27.0     -0.1153        nan        nan        nan         nan         nan\nprinuyer_dummy_28.0     -0.0311      0.020     -1.559      0.119      -0.070       0.008\npemjot_dummy_2.0        -0.0867        nan        nan        nan         nan         nan\npehruslt                -0.0040      0.003     -1.313      0.189      -0.010       0.002\npehractt                 0.0097        nan        nan        nan         nan         nan\npeio1cow_dummy_2.0      -0.1210        nan        nan        nan         nan         nan\npeio1cow_dummy_3.0      -0.1932        nan        nan        nan         nan         nan\npeio1cow_dummy_4.0      -0.0956        nan        nan        nan         nan         nan\npeio1cow_dummy_5.0      -0.0766        nan        nan        nan         nan         nan\npeio1cow_dummy_6.0      -0.0250      0.029     -0.858      0.391      -0.082       0.032\npeio1cow_dummy_7.0      -0.0487      0.026     -1.909      0.056      -0.099       0.001\nprdtind1_dummy_19.0      0.0417      0.014      2.981      0.003       0.014       0.069\nprdtind1_dummy_21.0     -0.0032      0.020     -0.160      0.873      -0.042       0.036\nprdtind1_dummy_22.0     -0.0821        nan        nan        nan         nan         nan\nprdtind1_dummy_23.0     -0.1656        nan        nan        nan         nan         nan\nprdtind1_dummy_32.0      0.0488        nan        nan        nan         nan         nan\nprdtind1_dummy_33.0      0.1106        nan        nan        nan         nan         nan\nprdtind1_dummy_34.0     -0.0167      0.020     -0.838      0.402      -0.056       0.022\nprdtind1_dummy_36.0      0.0606        nan        nan        nan         nan         nan\nprdtind1_dummy_38.0     -0.0056      0.018     -0.306      0.759      -0.041       0.030\nprdtind1_dummy_4.0      -0.0782        nan        nan        nan         nan         nan\nprdtind1_dummy_40.0     -0.0600        nan        nan        nan         nan         nan\nprdtind1_dummy_41.0     -0.0634        nan        nan        nan         nan         nan\nprdtind1_dummy_42.0     -0.0277        nan        nan        nan         nan         nan\nprdtind1_dummy_43.0     -0.0368        nan        nan        nan         nan         nan\nprdtind1_dummy_44.0     -0.0817        nan        nan        nan         nan         nan\nprdtind1_dummy_46.0     -0.1566        nan        nan        nan         nan         nan\nprdtind1_dummy_48.0     -0.1212        nan        nan        nan         nan         nan\nprdtind1_dummy_49.0     -0.0150      0.019     -0.788      0.431      -0.052       0.022\nprdtind1_dummy_51.0     -0.0374      0.012     -3.125      0.002      -0.061      -0.014\nprdtocc1_dummy_10.0     -0.2076        nan        nan        nan         nan         nan\nprdtocc1_dummy_11.0     -0.1440        nan        nan        nan         nan         nan\nprdtocc1_dummy_12.0     -0.2012        nan        nan        nan         nan         nan\nprdtocc1_dummy_13.0     -0.1285        nan        nan        nan         nan         nan\nprdtocc1_dummy_14.0     -0.1633        nan        nan        nan         nan         nan\nprdtocc1_dummy_15.0     -0.1117        nan        nan        nan         nan         nan\nprdtocc1_dummy_16.0     -0.0287        nan        nan        nan         nan         nan\nprdtocc1_dummy_17.0     -0.0334        nan        nan        nan         nan         nan\nprdtocc1_dummy_19.0     -0.1612        nan        nan        nan         nan         nan\nprdtocc1_dummy_2.0       0.0584        nan        nan        nan         nan         nan\nprdtocc1_dummy_20.0     -0.1712        nan        nan        nan         nan         nan\nprdtocc1_dummy_21.0     -0.1846        nan        nan        nan         nan         nan\nprdtocc1_dummy_22.0     -0.3289        nan        nan        nan         nan         nan\nprdtocc1_dummy_3.0       0.0933        nan        nan        nan         nan         nan\nprdtocc1_dummy_4.0      -0.0103      0.018     -0.583      0.560      -0.045       0.024\nprdtocc1_dummy_5.0      -0.0078      0.022     -0.350      0.726      -0.051       0.036\nprdtocc1_dummy_6.0      -0.0541        nan        nan        nan         nan         nan\nprdtocc1_dummy_7.0      -0.0174      0.018     -0.960      0.337      -0.053       0.018\nprdtocc1_dummy_8.0      -0.1443        nan        nan        nan         nan         nan\nprdtocc1_dummy_9.0      -0.0068      0.016     -0.424      0.672      -0.038       0.025\npternwa                  0.0064      0.003      2.006      0.045       0.000       0.013\nptwk_dummy_1            -0.0167      0.040     -0.415      0.678      -0.096       0.062\nprchld_dummy_1.0        -0.0015      0.017     -0.090      0.929      -0.035       0.032\nprchld_dummy_10.0       -0.0229      0.016     -1.435      0.151      -0.054       0.008\nprchld_dummy_2.0        -0.0218      0.019     -1.159      0.246      -0.059       0.015\nprchld_dummy_3.0        -0.0167      0.011     -1.459      0.145      -0.039       0.006\nprchld_dummy_4.0        -0.0209      0.007     -2.911      0.004      -0.035      -0.007\nprchld_dummy_5.0        -0.0141      0.019     -0.753      0.451      -0.051       0.023\nprchld_dummy_8.0        -0.0217      0.021     -1.020      0.308      -0.063       0.020\nprnmchld                 0.0109        nan        nan        nan         nan         nan\n========================================================================================\n\nTop 10 Most Significant Variables:\n                                Variable  Coefficient  Std Error    Z-Score  \\\nprdtocc1_dummy_10.0  prdtocc1_dummy_10.0    -1.041564   0.087472 -11.907428   \nprdtocc1_dummy_8.0    prdtocc1_dummy_8.0    -0.723746   0.080538  -8.986380   \npeio1cow_dummy_3.0    peio1cow_dummy_3.0    -0.969504   0.116961  -8.289127   \nprdtocc1_dummy_22.0  prdtocc1_dummy_22.0    -1.650114   0.205311  -8.037127   \nprdtocc1_dummy_21.0  prdtocc1_dummy_21.0    -0.925981   0.116783  -7.929049   \nprdtocc1_dummy_11.0  prdtocc1_dummy_11.0    -0.722348   0.098332  -7.345973   \nprdtocc1_dummy_14.0  prdtocc1_dummy_14.0    -0.819323   0.114242  -7.171822   \nprdtind1_dummy_46.0  prdtind1_dummy_46.0    -0.785719   0.113591  -6.917108   \npemjot_dummy_2.0        pemjot_dummy_2.0    -0.434968   0.063793  -6.818380   \nprdtocc1_dummy_3.0    prdtocc1_dummy_3.0     0.468295   0.070678   6.625764   \n\n                          P-Value  \nprdtocc1_dummy_10.0  1.082656e-32  \nprdtocc1_dummy_8.0   2.555069e-19  \npeio1cow_dummy_3.0   1.140827e-16  \nprdtocc1_dummy_22.0  9.196906e-16  \nprdtocc1_dummy_21.0  2.208309e-15  \nprdtocc1_dummy_11.0  2.042670e-13  \nprdtocc1_dummy_14.0  7.400639e-13  \nprdtind1_dummy_46.0  4.609557e-12  \npemjot_dummy_2.0     9.207254e-12  \nprdtocc1_dummy_3.0   3.454563e-11  \n\n\n:::"
  },
  {
    "objectID": "baselinemodels.html#model-peformance",
    "href": "baselinemodels.html#model-peformance",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "",
    "text": ":::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-results”}\n\n\n\nCalculating additional performance metrics...\n\nComprehensive Model Performance Metrics:\nRMSE: 0.2913\nMSE: 0.0849\nMAE: 0.1675\nR-squared: 0.5357\n\nPrecision: 0.7888\nRecall/Sensitivity: 0.7051\nSpecificity: 0.9401\nF1 Score: 0.7446\nPrevalence: 0.2407\nNegative Predictive Value: 0.9095\nPositive Likelihood Ratio: 11.7783\nNegative Likelihood Ratio: 0.3136\n\n\n\n\n\n\n\n\n\n\nGenerating SHAP values...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "baselinemodels.html#probit-model",
    "href": "baselinemodels.html#probit-model",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "",
    "text": ":::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-probit”}\n\n\n\nFitting Probit model for effect sizes and p-values...\nRemoved 471 low variance columns\nRemoved 10 highly correlated features\nOptimization terminated successfully.\n         Current function value: 0.356300\n         Iterations 9\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14055\nMethod:                           MLE   Df Model:                          199\nDate:                Thu, 06 Feb 2025   Pseudo R-squ.:                  0.3545\nTime:                        08:33:29   Log-Likelihood:                -5079.1\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:                  HC0   LLR p-value:                     0.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -0.4438        nan        nan        nan         nan         nan\nhetelhhd_dummy_2        -0.3904      0.125     -3.120      0.002      -0.636      -0.145\nhetelavl_dummy_2         0.0665      0.167      0.399      0.690      -0.260       0.393\nhefaminc_dummy_10        0.0909      0.160      0.568      0.570      -0.223       0.405\nhefaminc_dummy_11       -0.1193      0.144     -0.826      0.409      -0.402       0.164\nhefaminc_dummy_12        0.0291      0.188      0.155      0.877      -0.339       0.397\nhefaminc_dummy_13        0.1481      0.108      1.368      0.171      -0.064       0.360\nhefaminc_dummy_14        0.1722      0.113      1.525      0.127      -0.049       0.394\nhefaminc_dummy_15        0.2896      0.101      2.859      0.004       0.091       0.488\nhefaminc_dummy_16        0.4533      0.122      3.720      0.000       0.214       0.692\nhefaminc_dummy_7        -0.1080      0.226     -0.477      0.633      -0.552       0.336\nhefaminc_dummy_8         0.2058      0.158      1.299      0.194      -0.105       0.516\nhefaminc_dummy_9        -0.2087      0.181     -1.151      0.250      -0.564       0.147\nhrnumhou                -0.0828      0.028     -3.000      0.003      -0.137      -0.029\nhrhtype_dummy_3         -0.0427      0.107     -0.398      0.690      -0.253       0.168\nhrhtype_dummy_4          0.0338      0.104      0.326      0.744      -0.169       0.237\nhrhtype_dummy_6         -0.0910      0.177     -0.513      0.608      -0.438       0.256\nhrhtype_dummy_7         -0.1437      0.182     -0.791      0.429      -0.500       0.212\nHUBUS_dummy_2           -0.3604      0.080     -4.485      0.000      -0.518      -0.203\ngestfips_dummy_36        0.0539      0.188      0.287      0.774      -0.315       0.422\ngtcbsa_dummy_10580       0.1101        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.1873      0.351     -0.534      0.594      -0.875       0.500\ngtcbsa_dummy_15380      -0.1131   5.19e+07  -2.18e-09      1.000   -1.02e+08    1.02e+08\ngtcbsa_dummy_35620       0.1921    1.6e+06    1.2e-07      1.000   -3.13e+06    3.13e+06\ngtcbsa_dummy_37980      -0.2836        nan        nan        nan         nan         nan\ngtcbsa_dummy_40380      -0.0566   5.96e+07   -9.5e-10      1.000   -1.17e+08    1.17e+08\ngtcbsa_dummy_45060      -0.1722        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.4605      0.248      1.857      0.063      -0.026       0.947\ngtco_dummy_103          -0.1515      0.125     -1.210      0.226      -0.397       0.094\ngtco_dummy_119          -0.2373      0.131     -1.813      0.070      -0.494       0.019\ngtco_dummy_13            0.0841      0.097      0.871      0.384      -0.105       0.273\ngtco_dummy_17            0.0980      0.115      0.849      0.396      -0.128       0.324\ngtco_dummy_23           -0.0961      0.100     -0.959      0.338      -0.293       0.100\ngtco_dummy_27            0.3721      0.096      3.857      0.000       0.183       0.561\ngtco_dummy_3             0.1696      0.094      1.813      0.070      -0.014       0.353\ngtco_dummy_31            0.2710      0.132      2.055      0.040       0.013       0.530\ngtco_dummy_35            0.0497      0.129      0.385      0.700      -0.203       0.302\ngtco_dummy_39           -0.0034      0.136     -0.025      0.980      -0.270       0.263\ngtco_dummy_47            0.7798      0.156      4.989      0.000       0.473       1.086\ngtco_dummy_5             0.5346      0.146      3.660      0.000       0.248       0.821\ngtco_dummy_55           -0.1912      0.170     -1.122      0.262      -0.525       0.143\ngtco_dummy_59           -0.3640      0.131     -2.783      0.005      -0.620      -0.108\ngtco_dummy_61            0.8676      0.159      5.457      0.000       0.556       1.179\ngtco_dummy_67           -0.1184      0.411     -0.288      0.773      -0.924       0.687\ngtco_dummy_7            -0.0162   4.53e+06  -3.57e-09      1.000   -8.89e+06    8.89e+06\ngtco_dummy_71            0.0603      0.148      0.409      0.683      -0.229       0.349\ngtco_dummy_81            0.7921      0.161      4.911      0.000       0.476       1.108\ngtco_dummy_85            0.1173      0.232      0.507      0.612      -0.336       0.571\ngtco_dummy_87           -1.0649      0.195     -5.458      0.000      -1.447      -0.683\ngtcbsast_dummy_2        -0.1585      0.101     -1.570      0.116      -0.356       0.039\ngtcbsast_dummy_3        -0.1579        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0213        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.8703      0.179     -4.871      0.000      -1.220      -0.520\ngtindvpc_dummy_2        -0.2450      0.176     -1.394      0.163      -0.589       0.099\ngtcbsasz_dummy_2        -0.3653      0.219     -1.665      0.096      -0.795       0.065\ngtcbsasz_dummy_3        -0.3957      0.240     -1.650      0.099      -0.866       0.074\ngtcbsasz_dummy_4        -0.0621        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.1698        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0915   8.05e+05  -1.14e-07      1.000   -1.58e+06    1.58e+06\ngtcsa_dummy_104         -0.2497      0.207     -1.208      0.227      -0.655       0.155\ngtcsa_dummy_428          0.2514      0.228      1.104      0.270      -0.195       0.698\nperrp_dummy_41.0         0.1192      0.213      0.560      0.576      -0.298       0.537\nperrp_dummy_42.0        -0.2038      0.145     -1.410      0.159      -0.487       0.080\nperrp_dummy_44.0         0.3822      0.169      2.267      0.023       0.052       0.713\nperrp_dummy_48.0        -0.3012      0.164     -1.836      0.066      -0.623       0.020\nperrp_dummy_50.0        -0.1561      0.267     -0.585      0.559      -0.680       0.367\nperrp_dummy_51.0        -0.0459      0.280     -0.164      0.870      -0.594       0.502\nperrp_dummy_52.0        -0.4274      0.318     -1.346      0.178      -1.050       0.195\nperrp_dummy_55.0         0.6969      0.180      3.872      0.000       0.344       1.050\nprtage                  -0.0426      0.021     -2.026      0.043      -0.084      -0.001\npemaritl_dummy_2.0      -0.1861      0.146     -1.273      0.203      -0.473       0.100\npemaritl_dummy_3.0       0.0187      0.137      0.136      0.891      -0.250       0.288\npemaritl_dummy_4.0      -0.0314      0.105     -0.298      0.766      -0.238       0.175\npemaritl_dummy_5.0      -0.1715      0.139     -1.238      0.216      -0.443       0.100\npemaritl_dummy_6.0      -0.0865      0.105     -0.826      0.409      -0.292       0.119\npesex_dummy_2.0          0.2232      0.035      6.335      0.000       0.154       0.292\npeeduca_dummy_33.0       0.3720      0.378      0.984      0.325      -0.369       1.113\npeeduca_dummy_37.0       0.1358      0.325      0.418      0.676      -0.500       0.772\npeeduca_dummy_38.0       0.0823      0.326      0.252      0.801      -0.557       0.721\npeeduca_dummy_39.0       0.3780      0.200      1.890      0.059      -0.014       0.770\npeeduca_dummy_40.0       0.6360      0.202      3.145      0.002       0.240       1.032\npeeduca_dummy_41.0       0.5762      0.222      2.600      0.009       0.142       1.010\npeeduca_dummy_42.0       0.6776      0.206      3.290      0.001       0.274       1.081\npeeduca_dummy_43.0       0.8655      0.200      4.330      0.000       0.474       1.257\npeeduca_dummy_44.0       1.0688      0.202      5.287      0.000       0.673       1.465\npeeduca_dummy_45.0       1.1099      0.219      5.058      0.000       0.680       1.540\npeeduca_dummy_46.0       1.2331      0.216      5.711      0.000       0.810       1.656\nptdtrace_dummy_2.0      -0.0249      0.055     -0.455      0.649      -0.132       0.082\nptdtrace_dummy_4.0      -0.0388      0.074     -0.523      0.601      -0.184       0.106\nprdthsp_dummy_2.0        0.1004      0.147      0.685      0.493      -0.187       0.388\nprdthsp_dummy_4.0       -0.3129      0.209     -1.497      0.134      -0.723       0.097\nprdthsp_dummy_6.0       -0.2805      0.202     -1.391      0.164      -0.676       0.115\nprdthsp_dummy_7.0        0.0820      0.159      0.516      0.606      -0.230       0.394\nPUCHINHH_dummy_3.0      -0.2696      0.195     -1.381      0.167      -0.652       0.113\nPUCHINHH_dummy_9.0      -0.0507      0.138     -0.367      0.714      -0.321       0.220\nprfamrel_dummy_1.0      -0.0493      0.142     -0.348      0.728      -0.327       0.228\nprfamrel_dummy_4.0      -0.2085      0.245     -0.850      0.396      -0.689       0.272\nprfamtyp_dummy_3.0      -0.0399      0.255     -0.156      0.876      -0.539       0.460\nprfamtyp_dummy_5.0      -0.3361      0.213     -1.580      0.114      -0.753       0.081\npehspnon_dummy_2.0       0.1137      0.104      1.091      0.275      -0.091       0.318\npenatvty_dummy_207.0     0.0231      0.166      0.139      0.889      -0.302       0.348\npenatvty_dummy_210.0     0.0226      0.165      0.137      0.891      -0.301       0.346\npenatvty_dummy_303.0     0.1393      0.304      0.458      0.647      -0.457       0.736\npenatvty_dummy_329.0     0.1313      0.222      0.591      0.555      -0.304       0.567\npenatvty_dummy_333.0    -0.3091      0.254     -1.215      0.224      -0.808       0.189\npenatvty_dummy_365.0     0.1572      0.327      0.481      0.631      -0.484       0.798\npenatvty_dummy_57.0      0.3278      0.143      2.288      0.022       0.047       0.609\npemntvty_dummy_207.0    -0.1621      0.231     -0.703      0.482      -0.614       0.290\npemntvty_dummy_210.0     0.3692      0.150      2.465      0.014       0.076       0.663\npemntvty_dummy_233.0     0.3973      0.362      1.098      0.272      -0.312       1.106\npemntvty_dummy_303.0    -0.4136      0.335     -1.235      0.217      -1.070       0.243\npemntvty_dummy_329.0    -0.3135      0.326     -0.962      0.336      -0.952       0.325\npemntvty_dummy_332.0    -0.4948      0.165     -2.991      0.003      -0.819      -0.171\npemntvty_dummy_333.0     0.4170      0.216      1.934      0.053      -0.006       0.840\npemntvty_dummy_364.0    -0.6624      0.368     -1.802      0.071      -1.383       0.058\npemntvty_dummy_365.0     1.2720      0.230      5.533      0.000       0.821       1.723\npemntvty_dummy_57.0     -0.0017      0.069     -0.025      0.980      -0.137       0.134\npemntvty_dummy_73.0     -0.1555      0.232     -0.670      0.503      -0.610       0.299\npefntvty_dummy_120.0    -0.1514      0.134     -1.133      0.257      -0.413       0.110\npefntvty_dummy_207.0     0.2339      0.227      1.031      0.303      -0.211       0.679\npefntvty_dummy_233.0    -0.4991      0.378     -1.321      0.186      -1.239       0.241\npefntvty_dummy_303.0    -0.0906      0.288     -0.314      0.753      -0.656       0.475\npefntvty_dummy_329.0     0.5558      0.298      1.868      0.062      -0.027       1.139\npefntvty_dummy_364.0     0.4193      0.371      1.129      0.259      -0.309       1.147\npefntvty_dummy_365.0    -1.0838      0.290     -3.732      0.000      -1.653      -0.515\npefntvty_dummy_57.0      0.0957      0.069      1.388      0.165      -0.039       0.231\npefntvty_dummy_73.0      0.3499      0.231      1.512      0.130      -0.104       0.803\nprcitshp_dummy_4.0       0.4262      0.148      2.876      0.004       0.136       0.717\nprcitshp_dummy_5.0       0.4360      0.161      2.706      0.007       0.120       0.752\nprinuyer_dummy_11.0     -0.0443      0.139     -0.319      0.750      -0.316       0.228\nprinuyer_dummy_12.0     -0.1863      0.148     -1.256      0.209      -0.477       0.104\nprinuyer_dummy_14.0     -0.0306      0.146     -0.209      0.834      -0.317       0.256\nprinuyer_dummy_15.0     -0.3445      0.178     -1.933      0.053      -0.694       0.005\nprinuyer_dummy_16.0     -0.1977      0.128     -1.541      0.123      -0.449       0.054\nprinuyer_dummy_17.0     -0.2555      0.122     -2.094      0.036      -0.495      -0.016\nprinuyer_dummy_18.0     -0.0259      0.150     -0.172      0.863      -0.320       0.268\nprinuyer_dummy_19.0     -0.0810      0.153     -0.528      0.597      -0.381       0.219\nprinuyer_dummy_20.0     -0.4099      0.164     -2.504      0.012      -0.731      -0.089\nprinuyer_dummy_21.0     -0.0698      0.150     -0.466      0.641      -0.363       0.224\nprinuyer_dummy_22.0     -0.0406      0.142     -0.287      0.774      -0.318       0.237\nprinuyer_dummy_23.0     -0.1634      0.161     -1.017      0.309      -0.478       0.152\nprinuyer_dummy_24.0     -0.4663      0.160     -2.914      0.004      -0.780      -0.153\nprinuyer_dummy_25.0     -0.4639      0.148     -3.136      0.002      -0.754      -0.174\nprinuyer_dummy_26.0     -0.2901      0.161     -1.803      0.071      -0.606       0.025\nprinuyer_dummy_27.0     -0.5785      0.189     -3.068      0.002      -0.948      -0.209\nprinuyer_dummy_28.0     -0.1560      0.170     -0.919      0.358      -0.489       0.177\npemjot_dummy_2.0        -0.4350      0.064     -6.818      0.000      -0.560      -0.310\npehruslt                -0.0201      0.021     -0.977      0.329      -0.060       0.020\npehractt                 0.0488      0.021      2.346      0.019       0.008       0.090\npeio1cow_dummy_2.0      -0.6072      0.114     -5.311      0.000      -0.831      -0.383\npeio1cow_dummy_3.0      -0.9695      0.117     -8.289      0.000      -1.199      -0.740\npeio1cow_dummy_4.0      -0.4798      0.108     -4.428      0.000      -0.692      -0.267\npeio1cow_dummy_5.0      -0.3845      0.119     -3.242      0.001      -0.617      -0.152\npeio1cow_dummy_6.0      -0.1254      0.126     -0.997      0.319      -0.372       0.121\npeio1cow_dummy_7.0      -0.2443      0.125     -1.951      0.051      -0.490       0.001\nprdtind1_dummy_19.0      0.2093      0.117      1.792      0.073      -0.020       0.438\nprdtind1_dummy_21.0     -0.0161      0.103     -0.156      0.876      -0.218       0.186\nprdtind1_dummy_22.0     -0.4119      0.072     -5.711      0.000      -0.553      -0.271\nprdtind1_dummy_23.0     -0.8310      0.130     -6.409      0.000      -1.085      -0.577\nprdtind1_dummy_32.0      0.2446      0.069      3.548      0.000       0.109       0.380\nprdtind1_dummy_33.0      0.5550      0.103      5.365      0.000       0.352       0.758\nprdtind1_dummy_34.0     -0.0839      0.108     -0.774      0.439      -0.296       0.129\nprdtind1_dummy_36.0      0.3042      0.058      5.250      0.000       0.191       0.418\nprdtind1_dummy_38.0     -0.0279      0.095     -0.292      0.770      -0.215       0.159\nprdtind1_dummy_4.0      -0.3925      0.097     -4.053      0.000      -0.582      -0.203\nprdtind1_dummy_40.0     -0.3013      0.080     -3.743      0.000      -0.459      -0.144\nprdtind1_dummy_41.0     -0.3179      0.096     -3.320      0.001      -0.506      -0.130\nprdtind1_dummy_42.0     -0.1392      0.077     -1.810      0.070      -0.290       0.012\nprdtind1_dummy_43.0     -0.1846      0.098     -1.880      0.060      -0.377       0.008\nprdtind1_dummy_44.0     -0.4101      0.103     -3.998      0.000      -0.611      -0.209\nprdtind1_dummy_46.0     -0.7857      0.114     -6.917      0.000      -1.008      -0.563\nprdtind1_dummy_48.0     -0.6080      0.178     -3.407      0.001      -0.958      -0.258\nprdtind1_dummy_49.0     -0.0754      0.119     -0.633      0.527      -0.309       0.158\nprdtind1_dummy_51.0     -0.1874      0.104     -1.808      0.071      -0.391       0.016\nprdtocc1_dummy_10.0     -1.0416      0.087    -11.907      0.000      -1.213      -0.870\nprdtocc1_dummy_11.0     -0.7223      0.098     -7.346      0.000      -0.915      -0.530\nprdtocc1_dummy_12.0     -1.0094      0.172     -5.859      0.000      -1.347      -0.672\nprdtocc1_dummy_13.0     -0.6446      0.129     -4.994      0.000      -0.898      -0.392\nprdtocc1_dummy_14.0     -0.8193      0.114     -7.172      0.000      -1.043      -0.595\nprdtocc1_dummy_15.0     -0.5606      0.132     -4.255      0.000      -0.819      -0.302\nprdtocc1_dummy_16.0     -0.1442      0.066     -2.201      0.028      -0.273      -0.016\nprdtocc1_dummy_17.0     -0.1676      0.057     -2.914      0.004      -0.280      -0.055\nprdtocc1_dummy_19.0     -0.8087      0.130     -6.209      0.000      -1.064      -0.553\nprdtocc1_dummy_2.0       0.2930      0.055      5.292      0.000       0.185       0.402\nprdtocc1_dummy_20.0     -0.8588      0.137     -6.246      0.000      -1.128      -0.589\nprdtocc1_dummy_21.0     -0.9260      0.117     -7.929      0.000      -1.155      -0.697\nprdtocc1_dummy_22.0     -1.6501      0.205     -8.037      0.000      -2.053      -1.248\nprdtocc1_dummy_3.0       0.4683      0.071      6.626      0.000       0.330       0.607\nprdtocc1_dummy_4.0      -0.0518      0.095     -0.544      0.587      -0.239       0.135\nprdtocc1_dummy_5.0      -0.0389      0.115     -0.337      0.736      -0.265       0.187\nprdtocc1_dummy_6.0      -0.2713      0.092     -2.962      0.003      -0.451      -0.092\nprdtocc1_dummy_7.0      -0.0872      0.101     -0.866      0.386      -0.284       0.110\nprdtocc1_dummy_8.0      -0.7237      0.081     -8.986      0.000      -0.882      -0.566\nprdtocc1_dummy_9.0      -0.0342      0.081     -0.422      0.673      -0.193       0.125\npternwa                  0.0320      0.023      1.386      0.166      -0.013       0.077\nptwk_dummy_1            -0.0838      0.207     -0.404      0.686      -0.490       0.322\nprchld_dummy_1.0        -0.0076      0.085     -0.090      0.929      -0.174       0.159\nprchld_dummy_10.0       -0.1151      0.099     -1.159      0.246      -0.310       0.079\nprchld_dummy_2.0        -0.1091      0.108     -1.012      0.312      -0.321       0.102\nprchld_dummy_3.0        -0.0836      0.067     -1.251      0.211      -0.215       0.047\nprchld_dummy_4.0        -0.1046      0.068     -1.530      0.126      -0.239       0.029\nprchld_dummy_5.0        -0.0709      0.107     -0.661      0.509      -0.281       0.139\nprchld_dummy_8.0        -0.1087      0.122     -0.887      0.375      -0.349       0.131\nprnmchld                 0.0549      0.028      1.977      0.048       0.000       0.109\n========================================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n========================================================================================\n                          dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nhetelhhd_dummy_2        -0.0778        nan        nan        nan         nan         nan\nhetelavl_dummy_2         0.0133      0.032      0.409      0.682      -0.050       0.077\nhefaminc_dummy_10        0.0181      0.026      0.687      0.492      -0.034       0.070\nhefaminc_dummy_11       -0.0238      0.033     -0.714      0.475      -0.089       0.041\nhefaminc_dummy_12        0.0058      0.036      0.163      0.871      -0.064       0.076\nhefaminc_dummy_13        0.0295        nan        nan        nan         nan         nan\nhefaminc_dummy_14        0.0343        nan        nan        nan         nan         nan\nhefaminc_dummy_15        0.0577        nan        nan        nan         nan         nan\nhefaminc_dummy_16        0.0903        nan        nan        nan         nan         nan\nhefaminc_dummy_7        -0.0215      0.049     -0.438      0.661      -0.118       0.075\nhefaminc_dummy_8         0.0410        nan        nan        nan         nan         nan\nhefaminc_dummy_9        -0.0416      0.038     -1.084      0.278      -0.117       0.034\nhrnumhou                -0.0165        nan        nan        nan         nan         nan\nhrhtype_dummy_3         -0.0085      0.021     -0.399      0.690      -0.050       0.033\nhrhtype_dummy_4          0.0067      0.020      0.336      0.737      -0.033       0.046\nhrhtype_dummy_6         -0.0181      0.034     -0.538      0.590      -0.084       0.048\nhrhtype_dummy_7         -0.0286      0.032     -0.900      0.368      -0.091       0.034\nHUBUS_dummy_2           -0.0718        nan        nan        nan         nan         nan\ngestfips_dummy_36        0.0107      0.038      0.281      0.779      -0.064       0.086\ngtcbsa_dummy_10580       0.0219        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.0373      0.068     -0.549      0.583      -0.171       0.096\ngtcbsa_dummy_15380      -0.0226   1.03e+07  -2.19e-09      1.000   -2.02e+07    2.02e+07\ngtcbsa_dummy_35620       0.0383   2.78e+05   1.38e-07      1.000   -5.45e+05    5.45e+05\ngtcbsa_dummy_37980      -0.0565   9.01e+04  -6.28e-07      1.000   -1.77e+05    1.77e+05\ngtcbsa_dummy_40380      -0.0113   1.19e+07  -9.51e-10      1.000   -2.33e+07    2.33e+07\ngtcbsa_dummy_45060      -0.0343        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.0918        nan        nan        nan         nan         nan\ngtco_dummy_103          -0.0302      0.024     -1.278      0.201      -0.077       0.016\ngtco_dummy_119          -0.0473      0.020     -2.383      0.017      -0.086      -0.008\ngtco_dummy_13            0.0168      0.020      0.854      0.393      -0.022       0.055\ngtco_dummy_17            0.0195      0.021      0.935      0.350      -0.021       0.060\ngtco_dummy_23           -0.0192      0.017     -1.129      0.259      -0.052       0.014\ngtco_dummy_27            0.0742        nan        nan        nan         nan         nan\ngtco_dummy_3             0.0338      0.006      5.359      0.000       0.021       0.046\ngtco_dummy_31            0.0540        nan        nan        nan         nan         nan\ngtco_dummy_35            0.0099      0.026      0.381      0.704      -0.041       0.061\ngtco_dummy_39           -0.0007      0.027     -0.025      0.980      -0.054       0.052\ngtco_dummy_47            0.1554        nan        nan        nan         nan         nan\ngtco_dummy_5             0.1066        nan        nan        nan         nan         nan\ngtco_dummy_55           -0.0381      0.011     -3.559      0.000      -0.059      -0.017\ngtco_dummy_59           -0.0726        nan        nan        nan         nan         nan\ngtco_dummy_61            0.1729        nan        nan        nan         nan         nan\ngtco_dummy_67           -0.0236      0.081     -0.292      0.771      -0.182       0.135\ngtco_dummy_7            -0.0032   9.03e+05  -3.58e-09      1.000   -1.77e+06    1.77e+06\ngtco_dummy_71            0.0120      0.027      0.445      0.656      -0.041       0.065\ngtco_dummy_81            0.1579        nan        nan        nan         nan         nan\ngtco_dummy_85            0.0234      0.049      0.480      0.631      -0.072       0.119\ngtco_dummy_87           -0.2123        nan        nan        nan         nan         nan\ngtcbsast_dummy_2        -0.0316      0.018     -1.771      0.077      -0.067       0.003\ngtcbsast_dummy_3        -0.0315        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0042        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.1735        nan        nan        nan         nan         nan\ngtindvpc_dummy_2        -0.0488      0.022     -2.212      0.027      -0.092      -0.006\ngtcbsasz_dummy_2        -0.0728      0.021     -3.396      0.001      -0.115      -0.031\ngtcbsasz_dummy_3        -0.0789      0.017     -4.561      0.000      -0.113      -0.045\ngtcbsasz_dummy_4        -0.0124        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.0338        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0182   1.56e+05  -1.17e-07      1.000   -3.05e+05    3.05e+05\ngtcsa_dummy_104         -0.0498      0.037     -1.359      0.174      -0.122       0.022\ngtcsa_dummy_428          0.0501      0.034      1.478      0.139      -0.016       0.117\nperrp_dummy_41.0         0.0238      0.017      1.410      0.159      -0.009       0.057\nperrp_dummy_42.0        -0.0406      0.048     -0.851      0.395      -0.134       0.053\nperrp_dummy_44.0         0.0762        nan        nan        nan         nan         nan\nperrp_dummy_48.0        -0.0600      0.056     -1.081      0.280      -0.169       0.049\nperrp_dummy_50.0        -0.0311      0.064     -0.487      0.626      -0.156       0.094\nperrp_dummy_51.0        -0.0091      0.059     -0.155      0.877      -0.125       0.107\nperrp_dummy_52.0        -0.0852      0.080     -1.061      0.288      -0.243       0.072\nperrp_dummy_55.0         0.1389        nan        nan        nan         nan         nan\nprtage                  -0.0085        nan        nan        nan         nan         nan\npemaritl_dummy_2.0      -0.0371      0.012     -3.160      0.002      -0.060      -0.014\npemaritl_dummy_3.0       0.0037      0.027      0.136      0.892      -0.050       0.058\npemaritl_dummy_4.0      -0.0063      0.020     -0.312      0.755      -0.046       0.033\npemaritl_dummy_5.0      -0.0342      0.019     -1.768      0.077      -0.072       0.004\npemaritl_dummy_6.0      -0.0172      0.016     -1.053      0.292      -0.049       0.015\npesex_dummy_2.0          0.0445        nan        nan        nan         nan         nan\npeeduca_dummy_33.0       0.0741      0.088      0.838      0.402      -0.099       0.248\npeeduca_dummy_37.0       0.0271      0.071      0.382      0.702      -0.112       0.166\npeeduca_dummy_38.0       0.0164      0.069      0.237      0.813      -0.119       0.152\npeeduca_dummy_39.0       0.0753      0.050      1.498      0.134      -0.023       0.174\npeeduca_dummy_40.0       0.1268      0.042      3.015      0.003       0.044       0.209\npeeduca_dummy_41.0       0.1148      0.049      2.341      0.019       0.019       0.211\npeeduca_dummy_42.0       0.1351      0.038      3.541      0.000       0.060       0.210\npeeduca_dummy_43.0       0.1725        nan        nan        nan         nan         nan\npeeduca_dummy_44.0       0.2130        nan        nan        nan         nan         nan\npeeduca_dummy_45.0       0.2212        nan        nan        nan         nan         nan\npeeduca_dummy_46.0       0.2458        nan        nan        nan         nan         nan\nptdtrace_dummy_2.0      -0.0050      0.010     -0.480      0.632      -0.025       0.015\nptdtrace_dummy_4.0      -0.0077      0.014     -0.549      0.583      -0.035       0.020\nprdthsp_dummy_2.0        0.0200      0.033      0.600      0.548      -0.045       0.085\nprdthsp_dummy_4.0       -0.0624        nan        nan        nan         nan         nan\nprdthsp_dummy_6.0       -0.0559        nan        nan        nan         nan         nan\nprdthsp_dummy_7.0        0.0163      0.034      0.482      0.630      -0.050       0.083\nPUCHINHH_dummy_3.0      -0.0537        nan        nan        nan         nan         nan\nPUCHINHH_dummy_9.0      -0.0101      0.023     -0.436      0.663      -0.056       0.035\nprfamrel_dummy_1.0      -0.0098      0.035     -0.277      0.782      -0.079       0.060\nprfamrel_dummy_4.0      -0.0416      0.047     -0.884      0.376      -0.134       0.051\nprfamtyp_dummy_3.0      -0.0079      0.048     -0.165      0.869      -0.102       0.086\nprfamtyp_dummy_5.0      -0.0670      0.063     -1.058      0.290      -0.191       0.057\npehspnon_dummy_2.0       0.0227      0.024      0.926      0.355      -0.025       0.071\npenatvty_dummy_207.0     0.0046      0.035      0.133      0.894      -0.063       0.072\npenatvty_dummy_210.0     0.0045      0.034      0.134      0.893      -0.061       0.070\npenatvty_dummy_303.0     0.0278      0.060      0.461      0.645      -0.090       0.146\npenatvty_dummy_329.0     0.0262      0.045      0.577      0.564      -0.063       0.115\npenatvty_dummy_333.0    -0.0616      0.026     -2.405      0.016      -0.112      -0.011\npenatvty_dummy_365.0     0.0313      0.064      0.490      0.624      -0.094       0.157\npenatvty_dummy_57.0      0.0653      0.101      0.644      0.520      -0.134       0.264\npemntvty_dummy_207.0    -0.0323      0.044     -0.731      0.465      -0.119       0.054\npemntvty_dummy_210.0     0.0736        nan        nan        nan         nan         nan\npemntvty_dummy_233.0     0.0792      0.034      2.296      0.022       0.012       0.147\npemntvty_dummy_303.0    -0.0824      0.053     -1.553      0.120      -0.186       0.022\npemntvty_dummy_329.0    -0.0625      0.063     -0.988      0.323      -0.186       0.062\npemntvty_dummy_332.0    -0.0986        nan        nan        nan         nan         nan\npemntvty_dummy_333.0     0.0831        nan        nan        nan         nan         nan\npemntvty_dummy_364.0    -0.1320      0.051     -2.587      0.010      -0.232      -0.032\npemntvty_dummy_365.0     0.2535        nan        nan        nan         nan         nan\npemntvty_dummy_57.0     -0.0003      0.014     -0.024      0.981      -0.029       0.028\npemntvty_dummy_73.0     -0.0310      0.043     -0.725      0.469      -0.115       0.053\npefntvty_dummy_120.0    -0.0302        nan        nan        nan         nan         nan\npefntvty_dummy_207.0     0.0466      0.036      1.304      0.192      -0.023       0.117\npefntvty_dummy_233.0    -0.0995      0.004    -24.044      0.000      -0.108      -0.091\npefntvty_dummy_303.0    -0.0181      0.054     -0.332      0.740      -0.125       0.089\npefntvty_dummy_329.0     0.1108      0.019      5.941      0.000       0.074       0.147\npefntvty_dummy_364.0     0.0836      0.072      1.154      0.248      -0.058       0.225\npefntvty_dummy_365.0    -0.2160        nan        nan        nan         nan         nan\npefntvty_dummy_57.0      0.0191      0.030      0.643      0.520      -0.039       0.077\npefntvty_dummy_73.0      0.0697      0.058      1.196      0.232      -0.045       0.184\nprcitshp_dummy_4.0       0.0849      0.105      0.810      0.418      -0.120       0.290\nprcitshp_dummy_5.0       0.0869      0.105      0.825      0.410      -0.120       0.293\nprinuyer_dummy_11.0     -0.0088      0.024     -0.372      0.710      -0.055       0.038\nprinuyer_dummy_12.0     -0.0371        nan        nan        nan         nan         nan\nprinuyer_dummy_14.0     -0.0061      0.028     -0.220      0.826      -0.060       0.048\nprinuyer_dummy_15.0     -0.0687        nan        nan        nan         nan         nan\nprinuyer_dummy_16.0     -0.0394        nan        nan        nan         nan         nan\nprinuyer_dummy_17.0     -0.0509        nan        nan        nan         nan         nan\nprinuyer_dummy_18.0     -0.0052      0.029     -0.180      0.857      -0.061       0.051\nprinuyer_dummy_19.0     -0.0161      0.026     -0.628      0.530      -0.066       0.034\nprinuyer_dummy_20.0     -0.0817        nan        nan        nan         nan         nan\nprinuyer_dummy_21.0     -0.0139      0.024     -0.584      0.559      -0.061       0.033\nprinuyer_dummy_22.0     -0.0081      0.026     -0.312      0.755      -0.059       0.043\nprinuyer_dummy_23.0     -0.0326      0.017     -1.917      0.055      -0.066       0.001\nprinuyer_dummy_24.0     -0.0930        nan        nan        nan         nan         nan\nprinuyer_dummy_25.0     -0.0925        nan        nan        nan         nan         nan\nprinuyer_dummy_26.0     -0.0578        nan        nan        nan         nan         nan\nprinuyer_dummy_27.0     -0.1153        nan        nan        nan         nan         nan\nprinuyer_dummy_28.0     -0.0311      0.020     -1.559      0.119      -0.070       0.008\npemjot_dummy_2.0        -0.0867        nan        nan        nan         nan         nan\npehruslt                -0.0040      0.003     -1.313      0.189      -0.010       0.002\npehractt                 0.0097        nan        nan        nan         nan         nan\npeio1cow_dummy_2.0      -0.1210        nan        nan        nan         nan         nan\npeio1cow_dummy_3.0      -0.1932        nan        nan        nan         nan         nan\npeio1cow_dummy_4.0      -0.0956        nan        nan        nan         nan         nan\npeio1cow_dummy_5.0      -0.0766        nan        nan        nan         nan         nan\npeio1cow_dummy_6.0      -0.0250      0.029     -0.858      0.391      -0.082       0.032\npeio1cow_dummy_7.0      -0.0487      0.026     -1.909      0.056      -0.099       0.001\nprdtind1_dummy_19.0      0.0417      0.014      2.981      0.003       0.014       0.069\nprdtind1_dummy_21.0     -0.0032      0.020     -0.160      0.873      -0.042       0.036\nprdtind1_dummy_22.0     -0.0821        nan        nan        nan         nan         nan\nprdtind1_dummy_23.0     -0.1656        nan        nan        nan         nan         nan\nprdtind1_dummy_32.0      0.0488        nan        nan        nan         nan         nan\nprdtind1_dummy_33.0      0.1106        nan        nan        nan         nan         nan\nprdtind1_dummy_34.0     -0.0167      0.020     -0.838      0.402      -0.056       0.022\nprdtind1_dummy_36.0      0.0606        nan        nan        nan         nan         nan\nprdtind1_dummy_38.0     -0.0056      0.018     -0.306      0.759      -0.041       0.030\nprdtind1_dummy_4.0      -0.0782        nan        nan        nan         nan         nan\nprdtind1_dummy_40.0     -0.0600        nan        nan        nan         nan         nan\nprdtind1_dummy_41.0     -0.0634        nan        nan        nan         nan         nan\nprdtind1_dummy_42.0     -0.0277        nan        nan        nan         nan         nan\nprdtind1_dummy_43.0     -0.0368        nan        nan        nan         nan         nan\nprdtind1_dummy_44.0     -0.0817        nan        nan        nan         nan         nan\nprdtind1_dummy_46.0     -0.1566        nan        nan        nan         nan         nan\nprdtind1_dummy_48.0     -0.1212        nan        nan        nan         nan         nan\nprdtind1_dummy_49.0     -0.0150      0.019     -0.788      0.431      -0.052       0.022\nprdtind1_dummy_51.0     -0.0374      0.012     -3.125      0.002      -0.061      -0.014\nprdtocc1_dummy_10.0     -0.2076        nan        nan        nan         nan         nan\nprdtocc1_dummy_11.0     -0.1440        nan        nan        nan         nan         nan\nprdtocc1_dummy_12.0     -0.2012        nan        nan        nan         nan         nan\nprdtocc1_dummy_13.0     -0.1285        nan        nan        nan         nan         nan\nprdtocc1_dummy_14.0     -0.1633        nan        nan        nan         nan         nan\nprdtocc1_dummy_15.0     -0.1117        nan        nan        nan         nan         nan\nprdtocc1_dummy_16.0     -0.0287        nan        nan        nan         nan         nan\nprdtocc1_dummy_17.0     -0.0334        nan        nan        nan         nan         nan\nprdtocc1_dummy_19.0     -0.1612        nan        nan        nan         nan         nan\nprdtocc1_dummy_2.0       0.0584        nan        nan        nan         nan         nan\nprdtocc1_dummy_20.0     -0.1712        nan        nan        nan         nan         nan\nprdtocc1_dummy_21.0     -0.1846        nan        nan        nan         nan         nan\nprdtocc1_dummy_22.0     -0.3289        nan        nan        nan         nan         nan\nprdtocc1_dummy_3.0       0.0933        nan        nan        nan         nan         nan\nprdtocc1_dummy_4.0      -0.0103      0.018     -0.583      0.560      -0.045       0.024\nprdtocc1_dummy_5.0      -0.0078      0.022     -0.350      0.726      -0.051       0.036\nprdtocc1_dummy_6.0      -0.0541        nan        nan        nan         nan         nan\nprdtocc1_dummy_7.0      -0.0174      0.018     -0.960      0.337      -0.053       0.018\nprdtocc1_dummy_8.0      -0.1443        nan        nan        nan         nan         nan\nprdtocc1_dummy_9.0      -0.0068      0.016     -0.424      0.672      -0.038       0.025\npternwa                  0.0064      0.003      2.006      0.045       0.000       0.013\nptwk_dummy_1            -0.0167      0.040     -0.415      0.678      -0.096       0.062\nprchld_dummy_1.0        -0.0015      0.017     -0.090      0.929      -0.035       0.032\nprchld_dummy_10.0       -0.0229      0.016     -1.435      0.151      -0.054       0.008\nprchld_dummy_2.0        -0.0218      0.019     -1.159      0.246      -0.059       0.015\nprchld_dummy_3.0        -0.0167      0.011     -1.459      0.145      -0.039       0.006\nprchld_dummy_4.0        -0.0209      0.007     -2.911      0.004      -0.035      -0.007\nprchld_dummy_5.0        -0.0141      0.019     -0.753      0.451      -0.051       0.023\nprchld_dummy_8.0        -0.0217      0.021     -1.020      0.308      -0.063       0.020\nprnmchld                 0.0109        nan        nan        nan         nan         nan\n========================================================================================\n\nTop 10 Most Significant Variables:\n                                Variable  Coefficient  Std Error    Z-Score  \\\nprdtocc1_dummy_10.0  prdtocc1_dummy_10.0    -1.041564   0.087472 -11.907428   \nprdtocc1_dummy_8.0    prdtocc1_dummy_8.0    -0.723746   0.080538  -8.986380   \npeio1cow_dummy_3.0    peio1cow_dummy_3.0    -0.969504   0.116961  -8.289127   \nprdtocc1_dummy_22.0  prdtocc1_dummy_22.0    -1.650114   0.205311  -8.037127   \nprdtocc1_dummy_21.0  prdtocc1_dummy_21.0    -0.925981   0.116783  -7.929049   \nprdtocc1_dummy_11.0  prdtocc1_dummy_11.0    -0.722348   0.098332  -7.345973   \nprdtocc1_dummy_14.0  prdtocc1_dummy_14.0    -0.819323   0.114242  -7.171822   \nprdtind1_dummy_46.0  prdtind1_dummy_46.0    -0.785719   0.113591  -6.917108   \npemjot_dummy_2.0        pemjot_dummy_2.0    -0.434968   0.063793  -6.818380   \nprdtocc1_dummy_3.0    prdtocc1_dummy_3.0     0.468295   0.070678   6.625764   \n\n                          P-Value  \nprdtocc1_dummy_10.0  1.082656e-32  \nprdtocc1_dummy_8.0   2.555069e-19  \npeio1cow_dummy_3.0   1.140827e-16  \nprdtocc1_dummy_22.0  9.196906e-16  \nprdtocc1_dummy_21.0  2.208309e-15  \nprdtocc1_dummy_11.0  2.042670e-13  \nprdtocc1_dummy_14.0  7.400639e-13  \nprdtind1_dummy_46.0  4.609557e-12  \npemjot_dummy_2.0     9.207254e-12  \nprdtocc1_dummy_3.0   3.454563e-11  \n\n\n:::"
  },
  {
    "objectID": "baselinemodels.html#model-performance",
    "href": "baselinemodels.html#model-performance",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "Model Performance",
    "text": "Model Performance\n:::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-results”}\n\n\n\nCalculating additional performance metrics...\n\nComprehensive Model Performance Metrics:\nRMSE: 0.2814\nMSE: 0.0792\nMAE: 0.1474\nR-squared: 0.0958\n\nPrecision: 0.3622\nRecall/Sensitivity: 0.0398\nSpecificity: 0.9925\nF1 Score: 0.0717\nPrevalence: 0.0970\nNegative Predictive Value: 0.9059\nPositive Likelihood Ratio: 5.2870\nNegative Likelihood Ratio: 0.9675\n\n\n\n\n\n\n\n\n\n\nGenerating SHAP values...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "baselinemodels.html#probit-model-1",
    "href": "baselinemodels.html#probit-model-1",
    "title": "CPS Binary Classification Model Results & Explainability",
    "section": "Probit Model",
    "text": "Probit Model\n:::{.quarto-embed-nb-cell notebook=“D:_New_LAB_RUCI_with_dummies.ipynb” notebook-title=” Probit Regression Results ” notebook-cellId=“cell-probit”}\n\n\n\nFitting Probit model for effect sizes and p-values...\nRemoved 64 low variance columns\nRemoved 4 highly correlated features\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.253002\n         Iterations: 35\n\nProbit Model Summary:\n\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 JWTRNS   No. Observations:                47672\nModel:                         Probit   Df Residuals:                    47570\nMethod:                           MLE   Df Model:                          101\nDate:                Thu, 06 Feb 2025   Pseudo R-squ.:                  0.2057\nTime:                        08:32:57   Log-Likelihood:                -12061.\nconverged:                      False   LL-Null:                       -15185.\nCovariance Type:                  HC0   LLR p-value:                     0.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -1.7055      0.374     -4.562      0.000      -2.438      -0.973\nAGEP                     0.0670      0.024      2.738      0.006       0.019       0.115\nBROADBND_dummy_2.0       0.0175      0.061      0.284      0.776      -0.103       0.138\nDDRS_dummy_2.0           0.1390      0.173      0.801      0.423      -0.201       0.479\nDEAR_dummy_2             0.0425      0.078      0.547      0.585      -0.110       0.195\nDEYE_dummy_2             0.1593      0.093      1.717      0.086      -0.023       0.341\nDIALUP_dummy_2.0         0.1160      0.070      1.662      0.097      -0.021       0.253\nDOUT_dummy_2.0          -0.3907      0.114     -3.441      0.001      -0.613      -0.168\nDPHY_dummy_2.0           0.0479      0.085      0.561      0.575      -0.119       0.215\nDREM_dummy_2.0          -0.0076      0.091     -0.084      0.933      -0.187       0.171\nESR_dummy_2.0           -5.2626   2.35e+06  -2.24e-06      1.000   -4.61e+06    4.61e+06\nESR_dummy_3.0           -6.0468      0.102    -59.231      0.000      -6.247      -5.847\nESR_dummy_6.0           -6.9979      0.119    -58.696      0.000      -7.232      -6.764\nHFL_dummy_2.0           -0.0533      0.056     -0.955      0.339      -0.163       0.056\nHFL_dummy_3.0           -0.0272      0.031     -0.888      0.375      -0.087       0.033\nHFL_dummy_4.0           -0.0713      0.036     -1.961      0.050      -0.143    -4.5e-05\nHHL_dummy_2.0           -0.0691      0.050     -1.373      0.170      -0.168       0.030\nHHL_dummy_3.0            0.0639      0.044      1.451      0.147      -0.022       0.150\nHHL_dummy_4.0           -0.0389      0.054     -0.718      0.473      -0.145       0.067\nHHL_dummy_5.0           -0.0476      0.071     -0.670      0.503      -0.187       0.092\nHHLDRAGEP                0.0182      0.022      0.845      0.398      -0.024       0.060\nHHLDRHISP_dummy_16.0     0.0560      0.126      0.443      0.658      -0.192       0.304\nHHLDRHISP_dummy_17.0     0.0600      0.158      0.379      0.705      -0.250       0.370\nHHLDRHISP_dummy_19.0    -0.3912      0.194     -2.013      0.044      -0.772      -0.010\nHHLDRHISP_dummy_2.0     -0.2452      0.161     -1.524      0.128      -0.561       0.070\nHHLDRHISP_dummy_3.0      0.0085      0.078      0.109      0.913      -0.144       0.161\nHHLDRHISP_dummy_5.0     -0.0056      0.139     -0.040      0.968      -0.277       0.266\nHHT_dummy_2.0            0.1672   2.16e+10   7.75e-12      1.000   -4.23e+10    4.23e+10\nHHT_dummy_3.0           -0.3132   5.31e+06   -5.9e-08      1.000   -1.04e+07    1.04e+07\nHICOV_dummy_2           -0.0794      0.071     -1.111      0.266      -0.219       0.061\nHINCP                    0.0935      0.009     10.519      0.000       0.076       0.111\nHISP_dummy_16           -0.0519      0.124     -0.418      0.676      -0.295       0.191\nHISP_dummy_17           -0.2335      0.161     -1.455      0.146      -0.548       0.081\nHISP_dummy_19            0.1699      0.172      0.987      0.323      -0.167       0.507\nHISP_dummy_2             0.1094      0.144      0.763      0.446      -0.172       0.391\nHISP_dummy_3            -0.0504      0.077     -0.655      0.513      -0.201       0.100\nHISP_dummy_5             0.0718      0.136      0.529      0.597      -0.194       0.338\nHISPEED_dummy_2.0       -0.1167      0.035     -3.346      0.001      -0.185      -0.048\nHUGCL_dummy_1.0          0.0097      0.056      0.175      0.861      -0.099       0.119\nHUPAC_dummy_2.0         -0.0800      0.104     -0.771      0.441      -0.283       0.123\nHUPAC_dummy_3.0         -0.2080      0.134     -1.557      0.119      -0.470       0.054\nHUPAC_dummy_4.0         -0.0577      0.094     -0.613      0.540      -0.242       0.127\nHUPAOC_dummy_2.0         0.0061      0.109      0.056      0.955      -0.207       0.220\nHUPAOC_dummy_3.0         0.1279      0.140      0.914      0.361      -0.146       0.402\nHUPAOC_dummy_4.0        -0.0343      0.097     -0.352      0.725      -0.225       0.157\nLANX_dummy_2.0           0.0587      0.042      1.412      0.158      -0.023       0.140\nLAPTOP_dummy_2.0        -0.2561      0.052     -4.925      0.000      -0.358      -0.154\nMAR_dummy_2             -0.0736      0.094     -0.786      0.432      -0.257       0.110\nMAR_dummy_3             -0.0375      0.055     -0.687      0.492      -0.145       0.070\nMAR_dummy_4              0.1311      0.094      1.392      0.164      -0.053       0.316\nMAR_dummy_5             -0.0893      0.046     -1.960      0.050      -0.179     1.3e-05\nMIG_dummy_3.0            0.1084      0.036      2.988      0.003       0.037       0.180\nPRIVCOV_dummy_2          0.0136      0.051      0.266      0.790      -0.087       0.114\nPUBCOV_dummy_2           0.0873      0.044      1.989      0.047       0.001       0.173\nR60_dummy_1.0           -0.0303      0.035     -0.858      0.391      -0.099       0.039\nR60_dummy_2.0           -0.0048      0.042     -0.113      0.910      -0.088       0.078\nR65_dummy_1.0            0.0077      0.037      0.204      0.838      -0.066       0.081\nR65_dummy_2.0            0.1076      0.049      2.193      0.028       0.011       0.204\nRAC1P_dummy_2           -0.0499      0.038     -1.325      0.185      -0.124       0.024\nRAC1P_dummy_6            0.1099      0.038      2.866      0.004       0.035       0.185\nRAC1P_dummy_8           -0.0905      0.053     -1.698      0.090      -0.195       0.014\nRAC1P_dummy_9            0.0315      0.038      0.829      0.407      -0.043       0.106\nSATELLITE_dummy_2.0      0.0478      0.052      0.912      0.362      -0.055       0.150\nSCHL_dummy_13.0          0.1440      0.156      0.926      0.355      -0.161       0.449\nSCHL_dummy_14.0         -0.0403      0.138     -0.292      0.770      -0.311       0.230\nSCHL_dummy_15.0          0.1638      0.119      1.373      0.170      -0.070       0.398\nSCHL_dummy_16.0          0.0871      0.084      1.039      0.299      -0.077       0.251\nSCHL_dummy_17.0          0.2057      0.107      1.915      0.055      -0.005       0.416\nSCHL_dummy_18.0          0.1795      0.092      1.947      0.052      -0.001       0.360\nSCHL_dummy_19.0          0.3121      0.086      3.646      0.000       0.144       0.480\nSCHL_dummy_20.0          0.2563      0.089      2.893      0.004       0.083       0.430\nSCHL_dummy_21.0          0.6000      0.082      7.299      0.000       0.439       0.761\nSCHL_dummy_22.0          0.5716      0.084      6.816      0.000       0.407       0.736\nSCHL_dummy_23.0          0.3064      0.093      3.290      0.001       0.124       0.489\nSCHL_dummy_24.0          0.3978      0.098      4.069      0.000       0.206       0.589\nSMARTPHONE_dummy_2.0    -0.0765      0.071     -1.077      0.282      -0.216       0.063\nTABLET_dummy_2.0         0.0026      0.026      0.100      0.921      -0.048       0.053\nWIF_dummy_1.0            0.2502      0.292      0.857      0.391      -0.322       0.822\nWIF_dummy_2.0            0.1825      0.293      0.623      0.533      -0.392       0.757\nWIF_dummy_3.0            0.0286      0.294      0.097      0.922      -0.548       0.605\nWKEXREL_dummy_10.0       0.2867   2.16e+10   1.33e-11      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_11.0       0.0967   2.16e+10   4.48e-12      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_12.0      -0.2163   2.16e+10     -1e-11      1.000   -4.23e+10    4.23e+10\nWKEXREL_dummy_13.0       0.0062   8.22e+06   7.56e-10      1.000   -1.61e+07    1.61e+07\nWKEXREL_dummy_14.0      -0.0729   8.14e+06  -8.95e-09      1.000    -1.6e+07     1.6e+07\nWKEXREL_dummy_15.0      -0.2465   9.21e+06  -2.68e-08      1.000   -1.81e+07    1.81e+07\nWKEXREL_dummy_2.0       -0.0407      0.031     -1.320      0.187      -0.101       0.020\nWKEXREL_dummy_3.0       -0.1801      0.072     -2.495      0.013      -0.322      -0.039\nWKEXREL_dummy_4.0       -0.0240      0.034     -0.711      0.477      -0.090       0.042\nWKEXREL_dummy_5.0       -0.0269      0.049     -0.547      0.585      -0.123       0.070\nWKEXREL_dummy_6.0       -0.0786      0.087     -0.899      0.369      -0.250       0.093\nWKEXREL_dummy_7.0       -0.1291      0.076     -1.693      0.090      -0.279       0.020\nWKEXREL_dummy_8.0       -0.1110      0.096     -1.162      0.245      -0.298       0.076\nWKEXREL_dummy_9.0       -0.3651      0.232     -1.575      0.115      -0.819       0.089\nWKL_dummy_2.0           -0.8893      0.094     -9.413      0.000      -1.074      -0.704\nWKL_dummy_3.0            1.5457      0.097     15.886      0.000       1.355       1.736\nWORKSTAT_dummy_10.0     -0.5093      0.450     -1.131      0.258      -1.392       0.373\nWORKSTAT_dummy_12.0     -0.1287      0.488     -0.264      0.792      -1.085       0.828\nWORKSTAT_dummy_13.0      0.3465      0.314      1.103      0.270      -0.269       0.962\nWORKSTAT_dummy_15.0      0.4656      0.401      1.160      0.246      -0.321       1.252\nWORKSTAT_dummy_2.0       0.1987      0.096      2.078      0.038       0.011       0.386\nWORKSTAT_dummy_3.0       0.0993      0.063      1.581      0.114      -0.024       0.222\nWORKSTAT_dummy_7.0       0.0596      0.068      0.872      0.383      -0.074       0.194\nWORKSTAT_dummy_9.0       0.1506      0.221      0.681      0.496      -0.283       0.584\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.37 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 JWTRNS\nMethod:                          dydx\nAt:                           overall\n========================================================================================\n                          dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nAGEP                     0.0094        nan        nan        nan         nan         nan\nBROADBND_dummy_2.0       0.0024        nan        nan        nan         nan         nan\nDDRS_dummy_2.0           0.0195        nan        nan        nan         nan         nan\nDEAR_dummy_2             0.0059        nan        nan        nan         nan         nan\nDEYE_dummy_2             0.0223        nan        nan        nan         nan         nan\nDIALUP_dummy_2.0         0.0162        nan        nan        nan         nan         nan\nDOUT_dummy_2.0          -0.0547        nan        nan        nan         nan         nan\nDPHY_dummy_2.0           0.0067        nan        nan        nan         nan         nan\nDREM_dummy_2.0          -0.0011        nan        nan        nan         nan         nan\nESR_dummy_2.0           -0.7365   3.29e+05  -2.24e-06      1.000   -6.45e+05    6.45e+05\nESR_dummy_3.0           -0.8463        nan        nan        nan         nan         nan\nESR_dummy_6.0           -0.9794        nan        nan        nan         nan         nan\nHFL_dummy_2.0           -0.0075        nan        nan        nan         nan         nan\nHFL_dummy_3.0           -0.0038        nan        nan        nan         nan         nan\nHFL_dummy_4.0           -0.0100        nan        nan        nan         nan         nan\nHHL_dummy_2.0           -0.0097        nan        nan        nan         nan         nan\nHHL_dummy_3.0            0.0089        nan        nan        nan         nan         nan\nHHL_dummy_4.0           -0.0054        nan        nan        nan         nan         nan\nHHL_dummy_5.0           -0.0067        nan        nan        nan         nan         nan\nHHLDRAGEP                0.0025        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_16.0     0.0078        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_17.0     0.0084        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_19.0    -0.0548        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_2.0     -0.0343        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_3.0      0.0012        nan        nan        nan         nan         nan\nHHLDRHISP_dummy_5.0     -0.0008        nan        nan        nan         nan         nan\nHHT_dummy_2.0            0.0234   3.02e+09   7.75e-12      1.000   -5.92e+09    5.92e+09\nHHT_dummy_3.0           -0.0438   7.65e+05  -5.73e-08      1.000    -1.5e+06     1.5e+06\nHICOV_dummy_2           -0.0111        nan        nan        nan         nan         nan\nHINCP                    0.0131        nan        nan        nan         nan         nan\nHISP_dummy_16           -0.0073        nan        nan        nan         nan         nan\nHISP_dummy_17           -0.0327        nan        nan        nan         nan         nan\nHISP_dummy_19            0.0238        nan        nan        nan         nan         nan\nHISP_dummy_2             0.0153        nan        nan        nan         nan         nan\nHISP_dummy_3            -0.0071        nan        nan        nan         nan         nan\nHISP_dummy_5             0.0100        nan        nan        nan         nan         nan\nHISPEED_dummy_2.0       -0.0163        nan        nan        nan         nan         nan\nHUGCL_dummy_1.0          0.0014        nan        nan        nan         nan         nan\nHUPAC_dummy_2.0         -0.0112        nan        nan        nan         nan         nan\nHUPAC_dummy_3.0         -0.0291        nan        nan        nan         nan         nan\nHUPAC_dummy_4.0         -0.0081        nan        nan        nan         nan         nan\nHUPAOC_dummy_2.0         0.0009        nan        nan        nan         nan         nan\nHUPAOC_dummy_3.0         0.0179        nan        nan        nan         nan         nan\nHUPAOC_dummy_4.0        -0.0048        nan        nan        nan         nan         nan\nLANX_dummy_2.0           0.0082        nan        nan        nan         nan         nan\nLAPTOP_dummy_2.0        -0.0358        nan        nan        nan         nan         nan\nMAR_dummy_2             -0.0103        nan        nan        nan         nan         nan\nMAR_dummy_3             -0.0052        nan        nan        nan         nan         nan\nMAR_dummy_4              0.0183        nan        nan        nan         nan         nan\nMAR_dummy_5             -0.0125        nan        nan        nan         nan         nan\nMIG_dummy_3.0            0.0152        nan        nan        nan         nan         nan\nPRIVCOV_dummy_2          0.0019        nan        nan        nan         nan         nan\nPUBCOV_dummy_2           0.0122        nan        nan        nan         nan         nan\nR60_dummy_1.0           -0.0042        nan        nan        nan         nan         nan\nR60_dummy_2.0           -0.0007        nan        nan        nan         nan         nan\nR65_dummy_1.0            0.0011        nan        nan        nan         nan         nan\nR65_dummy_2.0            0.0151        nan        nan        nan         nan         nan\nRAC1P_dummy_2           -0.0070        nan        nan        nan         nan         nan\nRAC1P_dummy_6            0.0154        nan        nan        nan         nan         nan\nRAC1P_dummy_8           -0.0127        nan        nan        nan         nan         nan\nRAC1P_dummy_9            0.0044        nan        nan        nan         nan         nan\nSATELLITE_dummy_2.0      0.0067        nan        nan        nan         nan         nan\nSCHL_dummy_13.0          0.0202        nan        nan        nan         nan         nan\nSCHL_dummy_14.0         -0.0056        nan        nan        nan         nan         nan\nSCHL_dummy_15.0          0.0229        nan        nan        nan         nan         nan\nSCHL_dummy_16.0          0.0122        nan        nan        nan         nan         nan\nSCHL_dummy_17.0          0.0288        nan        nan        nan         nan         nan\nSCHL_dummy_18.0          0.0251        nan        nan        nan         nan         nan\nSCHL_dummy_19.0          0.0437        nan        nan        nan         nan         nan\nSCHL_dummy_20.0          0.0359        nan        nan        nan         nan         nan\nSCHL_dummy_21.0          0.0840        nan        nan        nan         nan         nan\nSCHL_dummy_22.0          0.0800        nan        nan        nan         nan         nan\nSCHL_dummy_23.0          0.0429        nan        nan        nan         nan         nan\nSCHL_dummy_24.0          0.0557        nan        nan        nan         nan         nan\nSMARTPHONE_dummy_2.0    -0.0107        nan        nan        nan         nan         nan\nTABLET_dummy_2.0         0.0004        nan        nan        nan         nan         nan\nWIF_dummy_1.0            0.0350        nan        nan        nan         nan         nan\nWIF_dummy_2.0            0.0255        nan        nan        nan         nan         nan\nWIF_dummy_3.0            0.0040        nan        nan        nan         nan         nan\nWKEXREL_dummy_10.0       0.0401   3.02e+09   1.33e-11      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_11.0       0.0135   3.02e+09   4.48e-12      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_12.0      -0.0303   3.02e+09     -1e-11      1.000   -5.92e+09    5.92e+09\nWKEXREL_dummy_13.0       0.0009   1.15e+06   7.56e-10      1.000   -2.26e+06    2.26e+06\nWKEXREL_dummy_14.0      -0.0102   1.14e+06  -8.98e-09      1.000   -2.23e+06    2.23e+06\nWKEXREL_dummy_15.0      -0.0345   1.28e+06   -2.7e-08      1.000   -2.51e+06    2.51e+06\nWKEXREL_dummy_2.0       -0.0057        nan        nan        nan         nan         nan\nWKEXREL_dummy_3.0       -0.0252        nan        nan        nan         nan         nan\nWKEXREL_dummy_4.0       -0.0034        nan        nan        nan         nan         nan\nWKEXREL_dummy_5.0       -0.0038        nan        nan        nan         nan         nan\nWKEXREL_dummy_6.0       -0.0110        nan        nan        nan         nan         nan\nWKEXREL_dummy_7.0       -0.0181        nan        nan        nan         nan         nan\nWKEXREL_dummy_8.0       -0.0155        nan        nan        nan         nan         nan\nWKEXREL_dummy_9.0       -0.0511        nan        nan        nan         nan         nan\nWKL_dummy_2.0           -0.1245        nan        nan        nan         nan         nan\nWKL_dummy_3.0            0.2163        nan        nan        nan         nan         nan\nWORKSTAT_dummy_10.0     -0.0713        nan        nan        nan         nan         nan\nWORKSTAT_dummy_12.0     -0.0180        nan        nan        nan         nan         nan\nWORKSTAT_dummy_13.0      0.0485        nan        nan        nan         nan         nan\nWORKSTAT_dummy_15.0      0.0652        nan        nan        nan         nan         nan\nWORKSTAT_dummy_2.0       0.0278        nan        nan        nan         nan         nan\nWORKSTAT_dummy_3.0       0.0139        nan        nan        nan         nan         nan\nWORKSTAT_dummy_7.0       0.0083        nan        nan        nan         nan         nan\nWORKSTAT_dummy_9.0       0.0211        nan        nan        nan         nan         nan\n========================================================================================\n\nTop 10 Most Significant Variables:\n                          Variable  Coefficient  Std Error    Z-Score  \\\nESR_dummy_3.0        ESR_dummy_3.0    -6.046823   0.102090 -59.230522   \nESR_dummy_6.0        ESR_dummy_6.0    -6.997910   0.119223 -58.696022   \nWKL_dummy_3.0        WKL_dummy_3.0     1.545731   0.097304  15.885554   \nHINCP                        HINCP     0.093459   0.008884  10.519390   \nWKL_dummy_2.0        WKL_dummy_2.0    -0.889296   0.094473  -9.413236   \nSCHL_dummy_21.0    SCHL_dummy_21.0     0.599952   0.082196   7.299062   \nSCHL_dummy_22.0    SCHL_dummy_22.0     0.571636   0.083865   6.816102   \nLAPTOP_dummy_2.0  LAPTOP_dummy_2.0    -0.256125   0.052010  -4.924543   \nconst                        const    -1.705480   0.373832  -4.562156   \nSCHL_dummy_24.0    SCHL_dummy_24.0     0.397794   0.097751   4.069468   \n\n                       P-Value  \nESR_dummy_3.0     0.000000e+00  \nESR_dummy_6.0     0.000000e+00  \nWKL_dummy_3.0     7.978972e-57  \nHINCP             7.032777e-26  \nWKL_dummy_2.0     4.810921e-21  \nSCHL_dummy_21.0   2.897805e-13  \nSCHL_dummy_22.0   9.354363e-12  \nLAPTOP_dummy_2.0  8.455767e-07  \nconst             5.063111e-06  \nSCHL_dummy_24.0   4.712057e-05  \n\n\n:::"
  },
  {
    "objectID": "cps_with_dummies.html",
    "href": "cps_with_dummies.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,roc_curve,precision_recall_curve, average_precision_score,\n                           mean_squared_error, mean_absolute_error, r2_score, accuracy_score)\nfrom sklearn.calibration import calibration_curve\nimport statsmodels.api as sm\nimport shap\n\n# Load data\nprint(\"Loading data...\")\nfiltered_df = pd.read_csv(\"filtered_df.csv\")\nfiltered_df = filtered_df[(filtered_df['gestfips']==34) | (filtered_df['gestfips']==36)]\n\n# Define features and target\nX = filtered_df[['hehousut','hetelhhd','hetelavl','hefaminc','hrnumhou','hrhtype',\n                'HUBUS','gereg','gediv','gestfips','gtcbsa','gtco','gtcbsast',\n                'gtmetsta','gtindvpc','gtcbsasz','gtcsa','perrp','prtage','pemaritl',\n                'pesex','peeduca','ptdtrace','prdthsp','PUCHINHH','prfamrel',\n                'prfamtyp','pehspnon','penatvty','pemntvty','pefntvty','prcitshp',\n                'prinuyer','PUWK','pemjot','pemjnum','pehruslt','pehractt',\n                'peio1cow','prdtind1','prdtocc1','pternwa','ptwk','prchld','prnmchld']]\ny = filtered_df['pttlwk']\ny = (y == 1.0).astype(int)\n\nprint(f\"Data shape: X = {X.shape}, y = {y.shape}\")\n\nLoading data...\nData shape: X = (17819, 45), y = (17819,)\n\n\n\n# Define continuous columns\ncontinuous_columns = [\n    'hrnumhou',   # Number of persons living in household\n    'prtage',     # Age\n    'pemjnum',    # Number of jobs\n    'pehruslt',   # Usual hours worked\n    'pehractt',   # Actual hours worked last week\n    'pternwa',    # Weekly earnings\n    'prnmchld'    # Number of own children\n]\ndef create_dummies_with_base(df, continuous_cols):\n    \"\"\"Create dummy variables while preserving continuous features and dropping base categories\"\"\"\n    print(\"\\nCreating dummy variables...\")\n    df_processed = df.copy()\n    dummies_list = []\n\n    for column in df_processed.columns:\n        if column in continuous_cols:\n            # Convert to numeric without filling missing values\n            df_processed[column] = pd.to_numeric(df_processed[column], errors='coerce')\n            dummies_list.append(df_processed[[column]])\n        else:\n            # Create dummies with drop_first=True to avoid dummy variable trap\n            df_temp = df_processed[column].copy()\n            df_temp = df_temp.map(lambda x: str(x) if pd.notnull(x) and\n                                (not isinstance(x, (int, float)) or x &gt;= 0) else np.nan)\n            dummies = pd.get_dummies(df_temp, prefix=f\"{column}_dummy\", drop_first=True)\n            dummies_list.append(dummies)\n\n            # Optionally, print the dropped base category\n            if len(dummies.columns) &gt; 0:\n                base_category = df_temp.dropna().unique()[0]\n                print(f\"Dropped base category for {column}: {base_category}\")\n\n    final_df = pd.concat(dummies_list, axis=1)\n    print(f\"\\nOriginal shape: {df.shape}\")\n    print(f\"Final shape after dummy creation: {final_df.shape}\")\n    return final_df\n\n# Create dummies with standardization\nX_with_dummies = create_dummies_with_base(X, continuous_columns)\n\n# Standardize continuous features\nscaler = StandardScaler()\nfor column in continuous_columns:\n    if column in X_with_dummies.columns:\n        col_data = X_with_dummies[[column]].fillna(X_with_dummies[column].median())\n        X_with_dummies[column] = scaler.fit_transform(col_data)\n\nprint(f\"\\nFinal shape after standardization: {X_with_dummies.shape}\")\n\n\nCreating dummy variables...\nDropped base category for hehousut: 1\nDropped base category for hetelhhd: 1\nDropped base category for hetelavl: 2\nDropped base category for hefaminc: 14\nDropped base category for hrhtype: 1\nDropped base category for HUBUS: 1\nDropped base category for gestfips: 34\nDropped base category for gtcbsa: 35620\nDropped base category for gtco: 3\nDropped base category for gtcbsast: 2\nDropped base category for gtmetsta: 1\nDropped base category for gtindvpc: 0\nDropped base category for gtcbsasz: 7\nDropped base category for gtcsa: 408\nDropped base category for perrp: 40.0\nDropped base category for pemaritl: 1.0\nDropped base category for pesex: 2.0\nDropped base category for peeduca: 44.0\nDropped base category for ptdtrace: 1.0\nDropped base category for prdthsp: 7.0\nDropped base category for PUCHINHH: 9.0\nDropped base category for prfamrel: 1.0\nDropped base category for prfamtyp: 1.0\nDropped base category for pehspnon: 2.0\nDropped base category for penatvty: 104.0\nDropped base category for pemntvty: 104.0\nDropped base category for pefntvty: 104.0\nDropped base category for prcitshp: 4.0\nDropped base category for prinuyer: 19.0\nDropped base category for PUWK: 1.0\nDropped base category for pemjot: 2.0\nDropped base category for peio1cow: 4.0\nDropped base category for prdtind1: 25.0\nDropped base category for prdtocc1: 21.0\nDropped base category for ptwk: 0\nDropped base category for prchld: 0.0\n\nOriginal shape: (17819, 45)\nFinal shape after dummy creation: (17819, 684)\n\nFinal shape after standardization: (17819, 684)\n\n\n\n# First, let's convert any boolean columns to numeric in X_with_dummies\nfor col in X_with_dummies.columns:\n    if X_with_dummies[col].dtype == bool:\n        X_with_dummies[col] = X_with_dummies[col].astype(int)\n    elif X_with_dummies[col].dtype == 'object':\n        X_with_dummies[col] = pd.to_numeric(X_with_dummies[col], errors='coerce')\n\n\nX_with_dummies\n\n\n  \n    \n\n\n\n\n\n\nhehousut_dummy_12\nhehousut_dummy_2\nhehousut_dummy_3\nhehousut_dummy_4\nhehousut_dummy_5\nhehousut_dummy_6\nhehousut_dummy_7\nhetelhhd_dummy_2\nhetelavl_dummy_2\nhefaminc_dummy_10\n...\nprchld_dummy_15.0\nprchld_dummy_2.0\nprchld_dummy_3.0\nprchld_dummy_4.0\nprchld_dummy_5.0\nprchld_dummy_6.0\nprchld_dummy_7.0\nprchld_dummy_8.0\nprchld_dummy_9.0\nprnmchld\n\n\n\n\n651\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n652\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n653\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n654\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n655\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n318282\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n318283\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n318284\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n318399\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-0.534288\n\n\n318400\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0.494731\n\n\n\n\n17819 rows × 684 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# def calculate_vif_optimized(df, threshold=5):\n#     \"\"\"\n#     Calculate VIF iteratively, removing highest VIF features until all are below threshold\n#     Uses batch processing and early stopping for efficiency\n#     \"\"\"\n#     print(\"\\nCalculating VIF scores iteratively...\")\n#     numerical_df = df.select_dtypes(include=np.number).copy()\n\n#     # Initialize variables\n#     features_to_drop = []\n#     max_vif = float('inf')\n\n#     while len(numerical_df.columns) &gt; 0 and max_vif &gt; threshold:\n#         # Calculate VIF in batches of 50 features\n#         vif_data = []\n#         batch_size = 50\n\n#         for i in range(0, len(numerical_df.columns), batch_size):\n#             batch_cols = numerical_df.columns[i:i + batch_size]\n#             batch_df = numerical_df[batch_cols]\n\n#             batch_vif = [\n#                 (col, variance_inflation_factor(batch_df.values, j))\n#                 for j, col in enumerate(batch_cols)\n#             ]\n#             vif_data.extend(batch_vif)\n\n#         # Convert to DataFrame\n#         vif_df = pd.DataFrame(vif_data, columns=['Feature', 'VIF'])\n\n#         # Find highest VIF\n#         max_vif_row = vif_df.loc[vif_df['VIF'].idxmax()]\n#         max_vif = max_vif_row['VIF']\n\n#         if max_vif &gt; threshold:\n#             feature_to_drop = max_vif_row['Feature']\n#             features_to_drop.append(feature_to_drop)\n#             numerical_df.drop(columns=[feature_to_drop], inplace=True)\n\n#             # Print progress every 10 features\n#             if len(features_to_drop) % 10 == 0:\n#                 print(f\"Dropped {len(features_to_drop)} features. Current max VIF: {max_vif:.2f}\")\n\n#     print(f\"\\nTotal features dropped: {len(features_to_drop)}\")\n#     return features_to_drop, vif_df\n\n# # Calculate VIF and get features to drop\n# high_vif_features, final_vif = calculate_vif_optimized(X_with_dummies, threshold=10)\n\n# # Drop high VIF features\n# X_clean = X_with_dummies.drop(columns=high_vif_features)\n\n# # Print summary\n# print(\"\\nFinal VIF Results (top 10 highest):\")\n# print(final_vif.sort_values('VIF', ascending=False).head(10))\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_with_dummies, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nFinal shapes after VIF and split:\")\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"X_test: {X_test.shape}\")\n\n\nFinal shapes after VIF and split:\nX_train: (14255, 684)\nX_test: (3564, 684)\n\n\n\nimport statsmodels.api as sm\nimport numpy as np\n\nprint(\"\\nFitting Probit model for effect sizes and p-values...\")\n\n# 1. Remove low variance columns\nvariance_threshold = 0.01  # Adjust this value as needed\nvariances = X_train.var()\nlow_variance_columns = variances[variances &lt; variance_threshold].index\nX_train_filtered = X_train.drop(columns=low_variance_columns)\nprint(f\"Removed {len(low_variance_columns)} low variance columns\")\n\n# 2. Handle perfect multicollinearity\n# Drop highly correlated features\ndef remove_highly_correlated_features(df, threshold=0.95):\n    corr_matrix = df.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] &gt; threshold)]\n    return df.drop(columns=to_drop), to_drop\n\nX_train_filtered, dropped_corr = remove_highly_correlated_features(X_train_filtered)\nprint(f\"Removed {len(dropped_corr)} highly correlated features\")\n\n# 3. Add constant and fit model\nX_train_probit = sm.add_constant(X_train_filtered)\n\n# 4. Ensure all data is numeric\nX_train_probit = X_train_probit.astype(float)\n\n# 5. Fit Probit model with robust covariance\ntry:\n    probit_model = sm.Probit(y_train, X_train_probit)\n    probit_results = probit_model.fit(method='newton', cov_type='HC0')\n\n    print(\"\\nProbit Model Summary:\")\n    print(probit_results.summary())\n\n    # Calculate marginal effects\n    print(\"\\nMarginal Effects (Probit):\")\n    marginal_effects = probit_results.get_margeff(at='overall')\n    print(marginal_effects.summary())\n\n    # Print top significant variables\n    significant_vars = pd.DataFrame({\n        'Variable': X_train_probit.columns,\n        'Coefficient': probit_results.params,\n        'Std Error': probit_results.bse,\n        'Z-Score': probit_results.tvalues,\n        'P-Value': probit_results.pvalues\n    }).sort_values('P-Value')\n\n    print(\"\\nTop 10 Most Significant Variables:\")\n    print(significant_vars.head(10))\n\nexcept Exception as e:\n    print(f\"Error in model fitting: {str(e)}\")\n    print(\"\\nData shape:\", X_train_probit.shape)\n    print(\"\\nNumber of NaN values:\", X_train_probit.isna().sum().sum())\n\n\nFitting Probit model for effect sizes and p-values...\nRemoved 471 low variance columns\nRemoved 10 highly correlated features\nOptimization terminated successfully.\n         Current function value: 0.356300\n         Iterations 9\n\nProbit Model Summary:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                 pttlwk   No. Observations:                14255\nModel:                         Probit   Df Residuals:                    14055\nMethod:                           MLE   Df Model:                          199\nDate:                Thu, 06 Feb 2025   Pseudo R-squ.:                  0.3545\nTime:                        08:33:29   Log-Likelihood:                -5079.1\nconverged:                       True   LL-Null:                       -7868.1\nCovariance Type:                  HC0   LLR p-value:                     0.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -0.4438        nan        nan        nan         nan         nan\nhetelhhd_dummy_2        -0.3904      0.125     -3.120      0.002      -0.636      -0.145\nhetelavl_dummy_2         0.0665      0.167      0.399      0.690      -0.260       0.393\nhefaminc_dummy_10        0.0909      0.160      0.568      0.570      -0.223       0.405\nhefaminc_dummy_11       -0.1193      0.144     -0.826      0.409      -0.402       0.164\nhefaminc_dummy_12        0.0291      0.188      0.155      0.877      -0.339       0.397\nhefaminc_dummy_13        0.1481      0.108      1.368      0.171      -0.064       0.360\nhefaminc_dummy_14        0.1722      0.113      1.525      0.127      -0.049       0.394\nhefaminc_dummy_15        0.2896      0.101      2.859      0.004       0.091       0.488\nhefaminc_dummy_16        0.4533      0.122      3.720      0.000       0.214       0.692\nhefaminc_dummy_7        -0.1080      0.226     -0.477      0.633      -0.552       0.336\nhefaminc_dummy_8         0.2058      0.158      1.299      0.194      -0.105       0.516\nhefaminc_dummy_9        -0.2087      0.181     -1.151      0.250      -0.564       0.147\nhrnumhou                -0.0828      0.028     -3.000      0.003      -0.137      -0.029\nhrhtype_dummy_3         -0.0427      0.107     -0.398      0.690      -0.253       0.168\nhrhtype_dummy_4          0.0338      0.104      0.326      0.744      -0.169       0.237\nhrhtype_dummy_6         -0.0910      0.177     -0.513      0.608      -0.438       0.256\nhrhtype_dummy_7         -0.1437      0.182     -0.791      0.429      -0.500       0.212\nHUBUS_dummy_2           -0.3604      0.080     -4.485      0.000      -0.518      -0.203\ngestfips_dummy_36        0.0539      0.188      0.287      0.774      -0.315       0.422\ngtcbsa_dummy_10580       0.1101        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.1873      0.351     -0.534      0.594      -0.875       0.500\ngtcbsa_dummy_15380      -0.1131   5.19e+07  -2.18e-09      1.000   -1.02e+08    1.02e+08\ngtcbsa_dummy_35620       0.1921    1.6e+06    1.2e-07      1.000   -3.13e+06    3.13e+06\ngtcbsa_dummy_37980      -0.2836        nan        nan        nan         nan         nan\ngtcbsa_dummy_40380      -0.0566   5.96e+07   -9.5e-10      1.000   -1.17e+08    1.17e+08\ngtcbsa_dummy_45060      -0.1722        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.4605      0.248      1.857      0.063      -0.026       0.947\ngtco_dummy_103          -0.1515      0.125     -1.210      0.226      -0.397       0.094\ngtco_dummy_119          -0.2373      0.131     -1.813      0.070      -0.494       0.019\ngtco_dummy_13            0.0841      0.097      0.871      0.384      -0.105       0.273\ngtco_dummy_17            0.0980      0.115      0.849      0.396      -0.128       0.324\ngtco_dummy_23           -0.0961      0.100     -0.959      0.338      -0.293       0.100\ngtco_dummy_27            0.3721      0.096      3.857      0.000       0.183       0.561\ngtco_dummy_3             0.1696      0.094      1.813      0.070      -0.014       0.353\ngtco_dummy_31            0.2710      0.132      2.055      0.040       0.013       0.530\ngtco_dummy_35            0.0497      0.129      0.385      0.700      -0.203       0.302\ngtco_dummy_39           -0.0034      0.136     -0.025      0.980      -0.270       0.263\ngtco_dummy_47            0.7798      0.156      4.989      0.000       0.473       1.086\ngtco_dummy_5             0.5346      0.146      3.660      0.000       0.248       0.821\ngtco_dummy_55           -0.1912      0.170     -1.122      0.262      -0.525       0.143\ngtco_dummy_59           -0.3640      0.131     -2.783      0.005      -0.620      -0.108\ngtco_dummy_61            0.8676      0.159      5.457      0.000       0.556       1.179\ngtco_dummy_67           -0.1184      0.411     -0.288      0.773      -0.924       0.687\ngtco_dummy_7            -0.0162   4.53e+06  -3.57e-09      1.000   -8.89e+06    8.89e+06\ngtco_dummy_71            0.0603      0.148      0.409      0.683      -0.229       0.349\ngtco_dummy_81            0.7921      0.161      4.911      0.000       0.476       1.108\ngtco_dummy_85            0.1173      0.232      0.507      0.612      -0.336       0.571\ngtco_dummy_87           -1.0649      0.195     -5.458      0.000      -1.447      -0.683\ngtcbsast_dummy_2        -0.1585      0.101     -1.570      0.116      -0.356       0.039\ngtcbsast_dummy_3        -0.1579        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0213        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.8703      0.179     -4.871      0.000      -1.220      -0.520\ngtindvpc_dummy_2        -0.2450      0.176     -1.394      0.163      -0.589       0.099\ngtcbsasz_dummy_2        -0.3653      0.219     -1.665      0.096      -0.795       0.065\ngtcbsasz_dummy_3        -0.3957      0.240     -1.650      0.099      -0.866       0.074\ngtcbsasz_dummy_4        -0.0621        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.1698        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0915   8.05e+05  -1.14e-07      1.000   -1.58e+06    1.58e+06\ngtcsa_dummy_104         -0.2497      0.207     -1.208      0.227      -0.655       0.155\ngtcsa_dummy_428          0.2514      0.228      1.104      0.270      -0.195       0.698\nperrp_dummy_41.0         0.1192      0.213      0.560      0.576      -0.298       0.537\nperrp_dummy_42.0        -0.2038      0.145     -1.410      0.159      -0.487       0.080\nperrp_dummy_44.0         0.3822      0.169      2.267      0.023       0.052       0.713\nperrp_dummy_48.0        -0.3012      0.164     -1.836      0.066      -0.623       0.020\nperrp_dummy_50.0        -0.1561      0.267     -0.585      0.559      -0.680       0.367\nperrp_dummy_51.0        -0.0459      0.280     -0.164      0.870      -0.594       0.502\nperrp_dummy_52.0        -0.4274      0.318     -1.346      0.178      -1.050       0.195\nperrp_dummy_55.0         0.6969      0.180      3.872      0.000       0.344       1.050\nprtage                  -0.0426      0.021     -2.026      0.043      -0.084      -0.001\npemaritl_dummy_2.0      -0.1861      0.146     -1.273      0.203      -0.473       0.100\npemaritl_dummy_3.0       0.0187      0.137      0.136      0.891      -0.250       0.288\npemaritl_dummy_4.0      -0.0314      0.105     -0.298      0.766      -0.238       0.175\npemaritl_dummy_5.0      -0.1715      0.139     -1.238      0.216      -0.443       0.100\npemaritl_dummy_6.0      -0.0865      0.105     -0.826      0.409      -0.292       0.119\npesex_dummy_2.0          0.2232      0.035      6.335      0.000       0.154       0.292\npeeduca_dummy_33.0       0.3720      0.378      0.984      0.325      -0.369       1.113\npeeduca_dummy_37.0       0.1358      0.325      0.418      0.676      -0.500       0.772\npeeduca_dummy_38.0       0.0823      0.326      0.252      0.801      -0.557       0.721\npeeduca_dummy_39.0       0.3780      0.200      1.890      0.059      -0.014       0.770\npeeduca_dummy_40.0       0.6360      0.202      3.145      0.002       0.240       1.032\npeeduca_dummy_41.0       0.5762      0.222      2.600      0.009       0.142       1.010\npeeduca_dummy_42.0       0.6776      0.206      3.290      0.001       0.274       1.081\npeeduca_dummy_43.0       0.8655      0.200      4.330      0.000       0.474       1.257\npeeduca_dummy_44.0       1.0688      0.202      5.287      0.000       0.673       1.465\npeeduca_dummy_45.0       1.1099      0.219      5.058      0.000       0.680       1.540\npeeduca_dummy_46.0       1.2331      0.216      5.711      0.000       0.810       1.656\nptdtrace_dummy_2.0      -0.0249      0.055     -0.455      0.649      -0.132       0.082\nptdtrace_dummy_4.0      -0.0388      0.074     -0.523      0.601      -0.184       0.106\nprdthsp_dummy_2.0        0.1004      0.147      0.685      0.493      -0.187       0.388\nprdthsp_dummy_4.0       -0.3129      0.209     -1.497      0.134      -0.723       0.097\nprdthsp_dummy_6.0       -0.2805      0.202     -1.391      0.164      -0.676       0.115\nprdthsp_dummy_7.0        0.0820      0.159      0.516      0.606      -0.230       0.394\nPUCHINHH_dummy_3.0      -0.2696      0.195     -1.381      0.167      -0.652       0.113\nPUCHINHH_dummy_9.0      -0.0507      0.138     -0.367      0.714      -0.321       0.220\nprfamrel_dummy_1.0      -0.0493      0.142     -0.348      0.728      -0.327       0.228\nprfamrel_dummy_4.0      -0.2085      0.245     -0.850      0.396      -0.689       0.272\nprfamtyp_dummy_3.0      -0.0399      0.255     -0.156      0.876      -0.539       0.460\nprfamtyp_dummy_5.0      -0.3361      0.213     -1.580      0.114      -0.753       0.081\npehspnon_dummy_2.0       0.1137      0.104      1.091      0.275      -0.091       0.318\npenatvty_dummy_207.0     0.0231      0.166      0.139      0.889      -0.302       0.348\npenatvty_dummy_210.0     0.0226      0.165      0.137      0.891      -0.301       0.346\npenatvty_dummy_303.0     0.1393      0.304      0.458      0.647      -0.457       0.736\npenatvty_dummy_329.0     0.1313      0.222      0.591      0.555      -0.304       0.567\npenatvty_dummy_333.0    -0.3091      0.254     -1.215      0.224      -0.808       0.189\npenatvty_dummy_365.0     0.1572      0.327      0.481      0.631      -0.484       0.798\npenatvty_dummy_57.0      0.3278      0.143      2.288      0.022       0.047       0.609\npemntvty_dummy_207.0    -0.1621      0.231     -0.703      0.482      -0.614       0.290\npemntvty_dummy_210.0     0.3692      0.150      2.465      0.014       0.076       0.663\npemntvty_dummy_233.0     0.3973      0.362      1.098      0.272      -0.312       1.106\npemntvty_dummy_303.0    -0.4136      0.335     -1.235      0.217      -1.070       0.243\npemntvty_dummy_329.0    -0.3135      0.326     -0.962      0.336      -0.952       0.325\npemntvty_dummy_332.0    -0.4948      0.165     -2.991      0.003      -0.819      -0.171\npemntvty_dummy_333.0     0.4170      0.216      1.934      0.053      -0.006       0.840\npemntvty_dummy_364.0    -0.6624      0.368     -1.802      0.071      -1.383       0.058\npemntvty_dummy_365.0     1.2720      0.230      5.533      0.000       0.821       1.723\npemntvty_dummy_57.0     -0.0017      0.069     -0.025      0.980      -0.137       0.134\npemntvty_dummy_73.0     -0.1555      0.232     -0.670      0.503      -0.610       0.299\npefntvty_dummy_120.0    -0.1514      0.134     -1.133      0.257      -0.413       0.110\npefntvty_dummy_207.0     0.2339      0.227      1.031      0.303      -0.211       0.679\npefntvty_dummy_233.0    -0.4991      0.378     -1.321      0.186      -1.239       0.241\npefntvty_dummy_303.0    -0.0906      0.288     -0.314      0.753      -0.656       0.475\npefntvty_dummy_329.0     0.5558      0.298      1.868      0.062      -0.027       1.139\npefntvty_dummy_364.0     0.4193      0.371      1.129      0.259      -0.309       1.147\npefntvty_dummy_365.0    -1.0838      0.290     -3.732      0.000      -1.653      -0.515\npefntvty_dummy_57.0      0.0957      0.069      1.388      0.165      -0.039       0.231\npefntvty_dummy_73.0      0.3499      0.231      1.512      0.130      -0.104       0.803\nprcitshp_dummy_4.0       0.4262      0.148      2.876      0.004       0.136       0.717\nprcitshp_dummy_5.0       0.4360      0.161      2.706      0.007       0.120       0.752\nprinuyer_dummy_11.0     -0.0443      0.139     -0.319      0.750      -0.316       0.228\nprinuyer_dummy_12.0     -0.1863      0.148     -1.256      0.209      -0.477       0.104\nprinuyer_dummy_14.0     -0.0306      0.146     -0.209      0.834      -0.317       0.256\nprinuyer_dummy_15.0     -0.3445      0.178     -1.933      0.053      -0.694       0.005\nprinuyer_dummy_16.0     -0.1977      0.128     -1.541      0.123      -0.449       0.054\nprinuyer_dummy_17.0     -0.2555      0.122     -2.094      0.036      -0.495      -0.016\nprinuyer_dummy_18.0     -0.0259      0.150     -0.172      0.863      -0.320       0.268\nprinuyer_dummy_19.0     -0.0810      0.153     -0.528      0.597      -0.381       0.219\nprinuyer_dummy_20.0     -0.4099      0.164     -2.504      0.012      -0.731      -0.089\nprinuyer_dummy_21.0     -0.0698      0.150     -0.466      0.641      -0.363       0.224\nprinuyer_dummy_22.0     -0.0406      0.142     -0.287      0.774      -0.318       0.237\nprinuyer_dummy_23.0     -0.1634      0.161     -1.017      0.309      -0.478       0.152\nprinuyer_dummy_24.0     -0.4663      0.160     -2.914      0.004      -0.780      -0.153\nprinuyer_dummy_25.0     -0.4639      0.148     -3.136      0.002      -0.754      -0.174\nprinuyer_dummy_26.0     -0.2901      0.161     -1.803      0.071      -0.606       0.025\nprinuyer_dummy_27.0     -0.5785      0.189     -3.068      0.002      -0.948      -0.209\nprinuyer_dummy_28.0     -0.1560      0.170     -0.919      0.358      -0.489       0.177\npemjot_dummy_2.0        -0.4350      0.064     -6.818      0.000      -0.560      -0.310\npehruslt                -0.0201      0.021     -0.977      0.329      -0.060       0.020\npehractt                 0.0488      0.021      2.346      0.019       0.008       0.090\npeio1cow_dummy_2.0      -0.6072      0.114     -5.311      0.000      -0.831      -0.383\npeio1cow_dummy_3.0      -0.9695      0.117     -8.289      0.000      -1.199      -0.740\npeio1cow_dummy_4.0      -0.4798      0.108     -4.428      0.000      -0.692      -0.267\npeio1cow_dummy_5.0      -0.3845      0.119     -3.242      0.001      -0.617      -0.152\npeio1cow_dummy_6.0      -0.1254      0.126     -0.997      0.319      -0.372       0.121\npeio1cow_dummy_7.0      -0.2443      0.125     -1.951      0.051      -0.490       0.001\nprdtind1_dummy_19.0      0.2093      0.117      1.792      0.073      -0.020       0.438\nprdtind1_dummy_21.0     -0.0161      0.103     -0.156      0.876      -0.218       0.186\nprdtind1_dummy_22.0     -0.4119      0.072     -5.711      0.000      -0.553      -0.271\nprdtind1_dummy_23.0     -0.8310      0.130     -6.409      0.000      -1.085      -0.577\nprdtind1_dummy_32.0      0.2446      0.069      3.548      0.000       0.109       0.380\nprdtind1_dummy_33.0      0.5550      0.103      5.365      0.000       0.352       0.758\nprdtind1_dummy_34.0     -0.0839      0.108     -0.774      0.439      -0.296       0.129\nprdtind1_dummy_36.0      0.3042      0.058      5.250      0.000       0.191       0.418\nprdtind1_dummy_38.0     -0.0279      0.095     -0.292      0.770      -0.215       0.159\nprdtind1_dummy_4.0      -0.3925      0.097     -4.053      0.000      -0.582      -0.203\nprdtind1_dummy_40.0     -0.3013      0.080     -3.743      0.000      -0.459      -0.144\nprdtind1_dummy_41.0     -0.3179      0.096     -3.320      0.001      -0.506      -0.130\nprdtind1_dummy_42.0     -0.1392      0.077     -1.810      0.070      -0.290       0.012\nprdtind1_dummy_43.0     -0.1846      0.098     -1.880      0.060      -0.377       0.008\nprdtind1_dummy_44.0     -0.4101      0.103     -3.998      0.000      -0.611      -0.209\nprdtind1_dummy_46.0     -0.7857      0.114     -6.917      0.000      -1.008      -0.563\nprdtind1_dummy_48.0     -0.6080      0.178     -3.407      0.001      -0.958      -0.258\nprdtind1_dummy_49.0     -0.0754      0.119     -0.633      0.527      -0.309       0.158\nprdtind1_dummy_51.0     -0.1874      0.104     -1.808      0.071      -0.391       0.016\nprdtocc1_dummy_10.0     -1.0416      0.087    -11.907      0.000      -1.213      -0.870\nprdtocc1_dummy_11.0     -0.7223      0.098     -7.346      0.000      -0.915      -0.530\nprdtocc1_dummy_12.0     -1.0094      0.172     -5.859      0.000      -1.347      -0.672\nprdtocc1_dummy_13.0     -0.6446      0.129     -4.994      0.000      -0.898      -0.392\nprdtocc1_dummy_14.0     -0.8193      0.114     -7.172      0.000      -1.043      -0.595\nprdtocc1_dummy_15.0     -0.5606      0.132     -4.255      0.000      -0.819      -0.302\nprdtocc1_dummy_16.0     -0.1442      0.066     -2.201      0.028      -0.273      -0.016\nprdtocc1_dummy_17.0     -0.1676      0.057     -2.914      0.004      -0.280      -0.055\nprdtocc1_dummy_19.0     -0.8087      0.130     -6.209      0.000      -1.064      -0.553\nprdtocc1_dummy_2.0       0.2930      0.055      5.292      0.000       0.185       0.402\nprdtocc1_dummy_20.0     -0.8588      0.137     -6.246      0.000      -1.128      -0.589\nprdtocc1_dummy_21.0     -0.9260      0.117     -7.929      0.000      -1.155      -0.697\nprdtocc1_dummy_22.0     -1.6501      0.205     -8.037      0.000      -2.053      -1.248\nprdtocc1_dummy_3.0       0.4683      0.071      6.626      0.000       0.330       0.607\nprdtocc1_dummy_4.0      -0.0518      0.095     -0.544      0.587      -0.239       0.135\nprdtocc1_dummy_5.0      -0.0389      0.115     -0.337      0.736      -0.265       0.187\nprdtocc1_dummy_6.0      -0.2713      0.092     -2.962      0.003      -0.451      -0.092\nprdtocc1_dummy_7.0      -0.0872      0.101     -0.866      0.386      -0.284       0.110\nprdtocc1_dummy_8.0      -0.7237      0.081     -8.986      0.000      -0.882      -0.566\nprdtocc1_dummy_9.0      -0.0342      0.081     -0.422      0.673      -0.193       0.125\npternwa                  0.0320      0.023      1.386      0.166      -0.013       0.077\nptwk_dummy_1            -0.0838      0.207     -0.404      0.686      -0.490       0.322\nprchld_dummy_1.0        -0.0076      0.085     -0.090      0.929      -0.174       0.159\nprchld_dummy_10.0       -0.1151      0.099     -1.159      0.246      -0.310       0.079\nprchld_dummy_2.0        -0.1091      0.108     -1.012      0.312      -0.321       0.102\nprchld_dummy_3.0        -0.0836      0.067     -1.251      0.211      -0.215       0.047\nprchld_dummy_4.0        -0.1046      0.068     -1.530      0.126      -0.239       0.029\nprchld_dummy_5.0        -0.0709      0.107     -0.661      0.509      -0.281       0.139\nprchld_dummy_8.0        -0.1087      0.122     -0.887      0.375      -0.349       0.131\nprnmchld                 0.0549      0.028      1.977      0.048       0.000       0.109\n========================================================================================\n\nMarginal Effects (Probit):\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                 pttlwk\nMethod:                          dydx\nAt:                           overall\n========================================================================================\n                          dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nhetelhhd_dummy_2        -0.0778        nan        nan        nan         nan         nan\nhetelavl_dummy_2         0.0133      0.032      0.409      0.682      -0.050       0.077\nhefaminc_dummy_10        0.0181      0.026      0.687      0.492      -0.034       0.070\nhefaminc_dummy_11       -0.0238      0.033     -0.714      0.475      -0.089       0.041\nhefaminc_dummy_12        0.0058      0.036      0.163      0.871      -0.064       0.076\nhefaminc_dummy_13        0.0295        nan        nan        nan         nan         nan\nhefaminc_dummy_14        0.0343        nan        nan        nan         nan         nan\nhefaminc_dummy_15        0.0577        nan        nan        nan         nan         nan\nhefaminc_dummy_16        0.0903        nan        nan        nan         nan         nan\nhefaminc_dummy_7        -0.0215      0.049     -0.438      0.661      -0.118       0.075\nhefaminc_dummy_8         0.0410        nan        nan        nan         nan         nan\nhefaminc_dummy_9        -0.0416      0.038     -1.084      0.278      -0.117       0.034\nhrnumhou                -0.0165        nan        nan        nan         nan         nan\nhrhtype_dummy_3         -0.0085      0.021     -0.399      0.690      -0.050       0.033\nhrhtype_dummy_4          0.0067      0.020      0.336      0.737      -0.033       0.046\nhrhtype_dummy_6         -0.0181      0.034     -0.538      0.590      -0.084       0.048\nhrhtype_dummy_7         -0.0286      0.032     -0.900      0.368      -0.091       0.034\nHUBUS_dummy_2           -0.0718        nan        nan        nan         nan         nan\ngestfips_dummy_36        0.0107      0.038      0.281      0.779      -0.064       0.086\ngtcbsa_dummy_10580       0.0219        nan        nan        nan         nan         nan\ngtcbsa_dummy_13780      -0.0373      0.068     -0.549      0.583      -0.171       0.096\ngtcbsa_dummy_15380      -0.0226   1.03e+07  -2.19e-09      1.000   -2.02e+07    2.02e+07\ngtcbsa_dummy_35620       0.0383   2.78e+05   1.38e-07      1.000   -5.45e+05    5.45e+05\ngtcbsa_dummy_37980      -0.0565   9.01e+04  -6.28e-07      1.000   -1.77e+05    1.77e+05\ngtcbsa_dummy_40380      -0.0113   1.19e+07  -9.51e-10      1.000   -2.33e+07    2.33e+07\ngtcbsa_dummy_45060      -0.0343        nan        nan        nan         nan         nan\ngtcbsa_dummy_45940       0.0918        nan        nan        nan         nan         nan\ngtco_dummy_103          -0.0302      0.024     -1.278      0.201      -0.077       0.016\ngtco_dummy_119          -0.0473      0.020     -2.383      0.017      -0.086      -0.008\ngtco_dummy_13            0.0168      0.020      0.854      0.393      -0.022       0.055\ngtco_dummy_17            0.0195      0.021      0.935      0.350      -0.021       0.060\ngtco_dummy_23           -0.0192      0.017     -1.129      0.259      -0.052       0.014\ngtco_dummy_27            0.0742        nan        nan        nan         nan         nan\ngtco_dummy_3             0.0338      0.006      5.359      0.000       0.021       0.046\ngtco_dummy_31            0.0540        nan        nan        nan         nan         nan\ngtco_dummy_35            0.0099      0.026      0.381      0.704      -0.041       0.061\ngtco_dummy_39           -0.0007      0.027     -0.025      0.980      -0.054       0.052\ngtco_dummy_47            0.1554        nan        nan        nan         nan         nan\ngtco_dummy_5             0.1066        nan        nan        nan         nan         nan\ngtco_dummy_55           -0.0381      0.011     -3.559      0.000      -0.059      -0.017\ngtco_dummy_59           -0.0726        nan        nan        nan         nan         nan\ngtco_dummy_61            0.1729        nan        nan        nan         nan         nan\ngtco_dummy_67           -0.0236      0.081     -0.292      0.771      -0.182       0.135\ngtco_dummy_7            -0.0032   9.03e+05  -3.58e-09      1.000   -1.77e+06    1.77e+06\ngtco_dummy_71            0.0120      0.027      0.445      0.656      -0.041       0.065\ngtco_dummy_81            0.1579        nan        nan        nan         nan         nan\ngtco_dummy_85            0.0234      0.049      0.480      0.631      -0.072       0.119\ngtco_dummy_87           -0.2123        nan        nan        nan         nan         nan\ngtcbsast_dummy_2        -0.0316      0.018     -1.771      0.077      -0.067       0.003\ngtcbsast_dummy_3        -0.0315        nan        nan        nan         nan         nan\ngtcbsast_dummy_4         0.0042        nan        nan        nan         nan         nan\ngtindvpc_dummy_1        -0.1735        nan        nan        nan         nan         nan\ngtindvpc_dummy_2        -0.0488      0.022     -2.212      0.027      -0.092      -0.006\ngtcbsasz_dummy_2        -0.0728      0.021     -3.396      0.001      -0.115      -0.031\ngtcbsasz_dummy_3        -0.0789      0.017     -4.561      0.000      -0.113      -0.045\ngtcbsasz_dummy_4        -0.0124        nan        nan        nan         nan         nan\ngtcbsasz_dummy_5        -0.0338        nan        nan        nan         nan         nan\ngtcbsasz_dummy_7        -0.0182   1.56e+05  -1.17e-07      1.000   -3.05e+05    3.05e+05\ngtcsa_dummy_104         -0.0498      0.037     -1.359      0.174      -0.122       0.022\ngtcsa_dummy_428          0.0501      0.034      1.478      0.139      -0.016       0.117\nperrp_dummy_41.0         0.0238      0.017      1.410      0.159      -0.009       0.057\nperrp_dummy_42.0        -0.0406      0.048     -0.851      0.395      -0.134       0.053\nperrp_dummy_44.0         0.0762        nan        nan        nan         nan         nan\nperrp_dummy_48.0        -0.0600      0.056     -1.081      0.280      -0.169       0.049\nperrp_dummy_50.0        -0.0311      0.064     -0.487      0.626      -0.156       0.094\nperrp_dummy_51.0        -0.0091      0.059     -0.155      0.877      -0.125       0.107\nperrp_dummy_52.0        -0.0852      0.080     -1.061      0.288      -0.243       0.072\nperrp_dummy_55.0         0.1389        nan        nan        nan         nan         nan\nprtage                  -0.0085        nan        nan        nan         nan         nan\npemaritl_dummy_2.0      -0.0371      0.012     -3.160      0.002      -0.060      -0.014\npemaritl_dummy_3.0       0.0037      0.027      0.136      0.892      -0.050       0.058\npemaritl_dummy_4.0      -0.0063      0.020     -0.312      0.755      -0.046       0.033\npemaritl_dummy_5.0      -0.0342      0.019     -1.768      0.077      -0.072       0.004\npemaritl_dummy_6.0      -0.0172      0.016     -1.053      0.292      -0.049       0.015\npesex_dummy_2.0          0.0445        nan        nan        nan         nan         nan\npeeduca_dummy_33.0       0.0741      0.088      0.838      0.402      -0.099       0.248\npeeduca_dummy_37.0       0.0271      0.071      0.382      0.702      -0.112       0.166\npeeduca_dummy_38.0       0.0164      0.069      0.237      0.813      -0.119       0.152\npeeduca_dummy_39.0       0.0753      0.050      1.498      0.134      -0.023       0.174\npeeduca_dummy_40.0       0.1268      0.042      3.015      0.003       0.044       0.209\npeeduca_dummy_41.0       0.1148      0.049      2.341      0.019       0.019       0.211\npeeduca_dummy_42.0       0.1351      0.038      3.541      0.000       0.060       0.210\npeeduca_dummy_43.0       0.1725        nan        nan        nan         nan         nan\npeeduca_dummy_44.0       0.2130        nan        nan        nan         nan         nan\npeeduca_dummy_45.0       0.2212        nan        nan        nan         nan         nan\npeeduca_dummy_46.0       0.2458        nan        nan        nan         nan         nan\nptdtrace_dummy_2.0      -0.0050      0.010     -0.480      0.632      -0.025       0.015\nptdtrace_dummy_4.0      -0.0077      0.014     -0.549      0.583      -0.035       0.020\nprdthsp_dummy_2.0        0.0200      0.033      0.600      0.548      -0.045       0.085\nprdthsp_dummy_4.0       -0.0624        nan        nan        nan         nan         nan\nprdthsp_dummy_6.0       -0.0559        nan        nan        nan         nan         nan\nprdthsp_dummy_7.0        0.0163      0.034      0.482      0.630      -0.050       0.083\nPUCHINHH_dummy_3.0      -0.0537        nan        nan        nan         nan         nan\nPUCHINHH_dummy_9.0      -0.0101      0.023     -0.436      0.663      -0.056       0.035\nprfamrel_dummy_1.0      -0.0098      0.035     -0.277      0.782      -0.079       0.060\nprfamrel_dummy_4.0      -0.0416      0.047     -0.884      0.376      -0.134       0.051\nprfamtyp_dummy_3.0      -0.0079      0.048     -0.165      0.869      -0.102       0.086\nprfamtyp_dummy_5.0      -0.0670      0.063     -1.058      0.290      -0.191       0.057\npehspnon_dummy_2.0       0.0227      0.024      0.926      0.355      -0.025       0.071\npenatvty_dummy_207.0     0.0046      0.035      0.133      0.894      -0.063       0.072\npenatvty_dummy_210.0     0.0045      0.034      0.134      0.893      -0.061       0.070\npenatvty_dummy_303.0     0.0278      0.060      0.461      0.645      -0.090       0.146\npenatvty_dummy_329.0     0.0262      0.045      0.577      0.564      -0.063       0.115\npenatvty_dummy_333.0    -0.0616      0.026     -2.405      0.016      -0.112      -0.011\npenatvty_dummy_365.0     0.0313      0.064      0.490      0.624      -0.094       0.157\npenatvty_dummy_57.0      0.0653      0.101      0.644      0.520      -0.134       0.264\npemntvty_dummy_207.0    -0.0323      0.044     -0.731      0.465      -0.119       0.054\npemntvty_dummy_210.0     0.0736        nan        nan        nan         nan         nan\npemntvty_dummy_233.0     0.0792      0.034      2.296      0.022       0.012       0.147\npemntvty_dummy_303.0    -0.0824      0.053     -1.553      0.120      -0.186       0.022\npemntvty_dummy_329.0    -0.0625      0.063     -0.988      0.323      -0.186       0.062\npemntvty_dummy_332.0    -0.0986        nan        nan        nan         nan         nan\npemntvty_dummy_333.0     0.0831        nan        nan        nan         nan         nan\npemntvty_dummy_364.0    -0.1320      0.051     -2.587      0.010      -0.232      -0.032\npemntvty_dummy_365.0     0.2535        nan        nan        nan         nan         nan\npemntvty_dummy_57.0     -0.0003      0.014     -0.024      0.981      -0.029       0.028\npemntvty_dummy_73.0     -0.0310      0.043     -0.725      0.469      -0.115       0.053\npefntvty_dummy_120.0    -0.0302        nan        nan        nan         nan         nan\npefntvty_dummy_207.0     0.0466      0.036      1.304      0.192      -0.023       0.117\npefntvty_dummy_233.0    -0.0995      0.004    -24.044      0.000      -0.108      -0.091\npefntvty_dummy_303.0    -0.0181      0.054     -0.332      0.740      -0.125       0.089\npefntvty_dummy_329.0     0.1108      0.019      5.941      0.000       0.074       0.147\npefntvty_dummy_364.0     0.0836      0.072      1.154      0.248      -0.058       0.225\npefntvty_dummy_365.0    -0.2160        nan        nan        nan         nan         nan\npefntvty_dummy_57.0      0.0191      0.030      0.643      0.520      -0.039       0.077\npefntvty_dummy_73.0      0.0697      0.058      1.196      0.232      -0.045       0.184\nprcitshp_dummy_4.0       0.0849      0.105      0.810      0.418      -0.120       0.290\nprcitshp_dummy_5.0       0.0869      0.105      0.825      0.410      -0.120       0.293\nprinuyer_dummy_11.0     -0.0088      0.024     -0.372      0.710      -0.055       0.038\nprinuyer_dummy_12.0     -0.0371        nan        nan        nan         nan         nan\nprinuyer_dummy_14.0     -0.0061      0.028     -0.220      0.826      -0.060       0.048\nprinuyer_dummy_15.0     -0.0687        nan        nan        nan         nan         nan\nprinuyer_dummy_16.0     -0.0394        nan        nan        nan         nan         nan\nprinuyer_dummy_17.0     -0.0509        nan        nan        nan         nan         nan\nprinuyer_dummy_18.0     -0.0052      0.029     -0.180      0.857      -0.061       0.051\nprinuyer_dummy_19.0     -0.0161      0.026     -0.628      0.530      -0.066       0.034\nprinuyer_dummy_20.0     -0.0817        nan        nan        nan         nan         nan\nprinuyer_dummy_21.0     -0.0139      0.024     -0.584      0.559      -0.061       0.033\nprinuyer_dummy_22.0     -0.0081      0.026     -0.312      0.755      -0.059       0.043\nprinuyer_dummy_23.0     -0.0326      0.017     -1.917      0.055      -0.066       0.001\nprinuyer_dummy_24.0     -0.0930        nan        nan        nan         nan         nan\nprinuyer_dummy_25.0     -0.0925        nan        nan        nan         nan         nan\nprinuyer_dummy_26.0     -0.0578        nan        nan        nan         nan         nan\nprinuyer_dummy_27.0     -0.1153        nan        nan        nan         nan         nan\nprinuyer_dummy_28.0     -0.0311      0.020     -1.559      0.119      -0.070       0.008\npemjot_dummy_2.0        -0.0867        nan        nan        nan         nan         nan\npehruslt                -0.0040      0.003     -1.313      0.189      -0.010       0.002\npehractt                 0.0097        nan        nan        nan         nan         nan\npeio1cow_dummy_2.0      -0.1210        nan        nan        nan         nan         nan\npeio1cow_dummy_3.0      -0.1932        nan        nan        nan         nan         nan\npeio1cow_dummy_4.0      -0.0956        nan        nan        nan         nan         nan\npeio1cow_dummy_5.0      -0.0766        nan        nan        nan         nan         nan\npeio1cow_dummy_6.0      -0.0250      0.029     -0.858      0.391      -0.082       0.032\npeio1cow_dummy_7.0      -0.0487      0.026     -1.909      0.056      -0.099       0.001\nprdtind1_dummy_19.0      0.0417      0.014      2.981      0.003       0.014       0.069\nprdtind1_dummy_21.0     -0.0032      0.020     -0.160      0.873      -0.042       0.036\nprdtind1_dummy_22.0     -0.0821        nan        nan        nan         nan         nan\nprdtind1_dummy_23.0     -0.1656        nan        nan        nan         nan         nan\nprdtind1_dummy_32.0      0.0488        nan        nan        nan         nan         nan\nprdtind1_dummy_33.0      0.1106        nan        nan        nan         nan         nan\nprdtind1_dummy_34.0     -0.0167      0.020     -0.838      0.402      -0.056       0.022\nprdtind1_dummy_36.0      0.0606        nan        nan        nan         nan         nan\nprdtind1_dummy_38.0     -0.0056      0.018     -0.306      0.759      -0.041       0.030\nprdtind1_dummy_4.0      -0.0782        nan        nan        nan         nan         nan\nprdtind1_dummy_40.0     -0.0600        nan        nan        nan         nan         nan\nprdtind1_dummy_41.0     -0.0634        nan        nan        nan         nan         nan\nprdtind1_dummy_42.0     -0.0277        nan        nan        nan         nan         nan\nprdtind1_dummy_43.0     -0.0368        nan        nan        nan         nan         nan\nprdtind1_dummy_44.0     -0.0817        nan        nan        nan         nan         nan\nprdtind1_dummy_46.0     -0.1566        nan        nan        nan         nan         nan\nprdtind1_dummy_48.0     -0.1212        nan        nan        nan         nan         nan\nprdtind1_dummy_49.0     -0.0150      0.019     -0.788      0.431      -0.052       0.022\nprdtind1_dummy_51.0     -0.0374      0.012     -3.125      0.002      -0.061      -0.014\nprdtocc1_dummy_10.0     -0.2076        nan        nan        nan         nan         nan\nprdtocc1_dummy_11.0     -0.1440        nan        nan        nan         nan         nan\nprdtocc1_dummy_12.0     -0.2012        nan        nan        nan         nan         nan\nprdtocc1_dummy_13.0     -0.1285        nan        nan        nan         nan         nan\nprdtocc1_dummy_14.0     -0.1633        nan        nan        nan         nan         nan\nprdtocc1_dummy_15.0     -0.1117        nan        nan        nan         nan         nan\nprdtocc1_dummy_16.0     -0.0287        nan        nan        nan         nan         nan\nprdtocc1_dummy_17.0     -0.0334        nan        nan        nan         nan         nan\nprdtocc1_dummy_19.0     -0.1612        nan        nan        nan         nan         nan\nprdtocc1_dummy_2.0       0.0584        nan        nan        nan         nan         nan\nprdtocc1_dummy_20.0     -0.1712        nan        nan        nan         nan         nan\nprdtocc1_dummy_21.0     -0.1846        nan        nan        nan         nan         nan\nprdtocc1_dummy_22.0     -0.3289        nan        nan        nan         nan         nan\nprdtocc1_dummy_3.0       0.0933        nan        nan        nan         nan         nan\nprdtocc1_dummy_4.0      -0.0103      0.018     -0.583      0.560      -0.045       0.024\nprdtocc1_dummy_5.0      -0.0078      0.022     -0.350      0.726      -0.051       0.036\nprdtocc1_dummy_6.0      -0.0541        nan        nan        nan         nan         nan\nprdtocc1_dummy_7.0      -0.0174      0.018     -0.960      0.337      -0.053       0.018\nprdtocc1_dummy_8.0      -0.1443        nan        nan        nan         nan         nan\nprdtocc1_dummy_9.0      -0.0068      0.016     -0.424      0.672      -0.038       0.025\npternwa                  0.0064      0.003      2.006      0.045       0.000       0.013\nptwk_dummy_1            -0.0167      0.040     -0.415      0.678      -0.096       0.062\nprchld_dummy_1.0        -0.0015      0.017     -0.090      0.929      -0.035       0.032\nprchld_dummy_10.0       -0.0229      0.016     -1.435      0.151      -0.054       0.008\nprchld_dummy_2.0        -0.0218      0.019     -1.159      0.246      -0.059       0.015\nprchld_dummy_3.0        -0.0167      0.011     -1.459      0.145      -0.039       0.006\nprchld_dummy_4.0        -0.0209      0.007     -2.911      0.004      -0.035      -0.007\nprchld_dummy_5.0        -0.0141      0.019     -0.753      0.451      -0.051       0.023\nprchld_dummy_8.0        -0.0217      0.021     -1.020      0.308      -0.063       0.020\nprnmchld                 0.0109        nan        nan        nan         nan         nan\n========================================================================================\n\nTop 10 Most Significant Variables:\n                                Variable  Coefficient  Std Error    Z-Score  \\\nprdtocc1_dummy_10.0  prdtocc1_dummy_10.0    -1.041564   0.087472 -11.907428   \nprdtocc1_dummy_8.0    prdtocc1_dummy_8.0    -0.723746   0.080538  -8.986380   \npeio1cow_dummy_3.0    peio1cow_dummy_3.0    -0.969504   0.116961  -8.289127   \nprdtocc1_dummy_22.0  prdtocc1_dummy_22.0    -1.650114   0.205311  -8.037127   \nprdtocc1_dummy_21.0  prdtocc1_dummy_21.0    -0.925981   0.116783  -7.929049   \nprdtocc1_dummy_11.0  prdtocc1_dummy_11.0    -0.722348   0.098332  -7.345973   \nprdtocc1_dummy_14.0  prdtocc1_dummy_14.0    -0.819323   0.114242  -7.171822   \nprdtind1_dummy_46.0  prdtind1_dummy_46.0    -0.785719   0.113591  -6.917108   \npemjot_dummy_2.0        pemjot_dummy_2.0    -0.434968   0.063793  -6.818380   \nprdtocc1_dummy_3.0    prdtocc1_dummy_3.0     0.468295   0.070678   6.625764   \n\n                          P-Value  \nprdtocc1_dummy_10.0  1.082656e-32  \nprdtocc1_dummy_8.0   2.555069e-19  \npeio1cow_dummy_3.0   1.140827e-16  \nprdtocc1_dummy_22.0  9.196906e-16  \nprdtocc1_dummy_21.0  2.208309e-15  \nprdtocc1_dummy_11.0  2.042670e-13  \nprdtocc1_dummy_14.0  7.400639e-13  \nprdtind1_dummy_46.0  4.609557e-12  \npemjot_dummy_2.0     9.207254e-12  \nprdtocc1_dummy_3.0   3.454563e-11  \n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/discrete/discrete_margins.py:343: RuntimeWarning: invalid value encountered in sqrt\n  return cov_me, np.sqrt(np.diag(cov_me))\n\n\n\n# Convert to DMatrix format\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': ['logloss', 'auc'],\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42\n}\n\n# Perform cross-validation\nprint(\"\\nPerforming cross-validation...\")\nnum_round = 1000\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_round,\n    nfold=5,\n    metrics=['auc', 'error'],\n    early_stopping_rounds=20,\n    verbose_eval=True\n)\n\n# Print CV results\nprint(\"\\nCross-validation results:\")\nprint(f\"Best AUC: {cv_results['test-auc-mean'].max():.4f} (+/- {cv_results['test-auc-std'].min():.4f})\")\nprint(f\"Best Error: {cv_results['test-error-mean'].min():.4f} (+/- {cv_results['test-error-std'].min():.4f})\")\n\n# Train final model\nprint(\"\\nTraining final model...\")\nfinal_model = xgb.train(params, dtrain, num_round)\n\n# Make predictions\ny_pred = final_model.predict(dtest)\ny_pred_binary = (y_pred &gt; 0.5).astype(int)\n\n# Basic metrics\nprint(\"\\nCalculating performance metrics...\")\naccuracy = accuracy_score(y_test, y_pred_binary)\nroc_auc = roc_auc_score(y_test, y_pred)\n\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_binary))\n\nprint(\"\\nConfusion Matrix:\")\nconf_matrix = confusion_matrix(y_test, y_pred_binary)\nprint(conf_matrix)\n\n\nPerforming cross-validation...\n[0] train-auc:0.76180+0.01116   train-error:0.24076+0.00120 test-auc:0.75189+0.01767    test-error:0.24076+0.00481\n[1] train-auc:0.81591+0.00558   train-error:0.24076+0.00120 test-auc:0.80476+0.01110    test-error:0.24076+0.00481\n[2] train-auc:0.82405+0.00306   train-error:0.24076+0.00120 test-auc:0.81268+0.00610    test-error:0.24076+0.00481\n[3] train-auc:0.83087+0.00315   train-error:0.24076+0.00120 test-auc:0.81905+0.00661    test-error:0.24076+0.00481\n[4] train-auc:0.83203+0.00321   train-error:0.24074+0.00123 test-auc:0.81966+0.00749    test-error:0.24076+0.00481\n[5] train-auc:0.83691+0.00384   train-error:0.23743+0.00385 test-auc:0.82363+0.00705    test-error:0.23907+0.00539\n[6] train-auc:0.84269+0.00298   train-error:0.22590+0.00318 test-auc:0.82859+0.00640    test-error:0.23016+0.00490\n[7] train-auc:0.84602+0.00357   train-error:0.21568+0.00479 test-auc:0.83038+0.00697    test-error:0.22182+0.00659\n[8] train-auc:0.84989+0.00256   train-error:0.20451+0.00359 test-auc:0.83337+0.00618    test-error:0.21312+0.00547\n[9] train-auc:0.85271+0.00200   train-error:0.19632+0.00280 test-auc:0.83605+0.00549    test-error:0.20365+0.00468\n[10]    train-auc:0.85554+0.00191   train-error:0.19239+0.00290 test-auc:0.83848+0.00644    test-error:0.20077+0.00453\n[11]    train-auc:0.85825+0.00247   train-error:0.18857+0.00281 test-auc:0.84061+0.00659    test-error:0.19825+0.00534\n[12]    train-auc:0.86157+0.00212   train-error:0.18641+0.00282 test-auc:0.84300+0.00664    test-error:0.19818+0.00639\n[13]    train-auc:0.86295+0.00191   train-error:0.18101+0.00348 test-auc:0.84428+0.00640    test-error:0.19277+0.00570\n[14]    train-auc:0.86469+0.00203   train-error:0.17838+0.00301 test-auc:0.84640+0.00619    test-error:0.19046+0.00478\n[15]    train-auc:0.86724+0.00211   train-error:0.17746+0.00248 test-auc:0.84859+0.00601    test-error:0.18976+0.00619\n[16]    train-auc:0.86932+0.00286   train-error:0.17536+0.00241 test-auc:0.85062+0.00499    test-error:0.18772+0.00697\n[17]    train-auc:0.87064+0.00272   train-error:0.17380+0.00266 test-auc:0.85144+0.00536    test-error:0.18653+0.00697\n[18]    train-auc:0.87209+0.00272   train-error:0.17304+0.00267 test-auc:0.85277+0.00543    test-error:0.18604+0.00728\n[19]    train-auc:0.87351+0.00239   train-error:0.17175+0.00260 test-auc:0.85417+0.00596    test-error:0.18534+0.00748\n[20]    train-auc:0.87491+0.00255   train-error:0.17173+0.00288 test-auc:0.85495+0.00590    test-error:0.18506+0.00849\n[21]    train-auc:0.87632+0.00258   train-error:0.17089+0.00303 test-auc:0.85612+0.00565    test-error:0.18351+0.00908\n[22]    train-auc:0.87813+0.00274   train-error:0.16943+0.00297 test-auc:0.85744+0.00549    test-error:0.18302+0.00922\n[23]    train-auc:0.87939+0.00277   train-error:0.16898+0.00305 test-auc:0.85822+0.00563    test-error:0.18337+0.00912\n[24]    train-auc:0.88055+0.00249   train-error:0.16777+0.00322 test-auc:0.85933+0.00585    test-error:0.18183+0.00949\n[25]    train-auc:0.88178+0.00245   train-error:0.16706+0.00242 test-auc:0.86033+0.00573    test-error:0.18099+0.00919\n[26]    train-auc:0.88290+0.00206   train-error:0.16633+0.00212 test-auc:0.86136+0.00586    test-error:0.18078+0.00837\n[27]    train-auc:0.88426+0.00189   train-error:0.16542+0.00208 test-auc:0.86249+0.00613    test-error:0.17945+0.00789\n[28]    train-auc:0.88519+0.00199   train-error:0.16450+0.00215 test-auc:0.86306+0.00641    test-error:0.17952+0.00844\n[29]    train-auc:0.88611+0.00204   train-error:0.16375+0.00254 test-auc:0.86408+0.00669    test-error:0.17832+0.00768\n[30]    train-auc:0.88707+0.00200   train-error:0.16326+0.00245 test-auc:0.86490+0.00650    test-error:0.17818+0.00807\n[31]    train-auc:0.88805+0.00205   train-error:0.16214+0.00213 test-auc:0.86565+0.00654    test-error:0.17685+0.00843\n[32]    train-auc:0.88881+0.00207   train-error:0.16093+0.00244 test-auc:0.86622+0.00627    test-error:0.17587+0.00873\n[33]    train-auc:0.88971+0.00208   train-error:0.16040+0.00222 test-auc:0.86692+0.00628    test-error:0.17566+0.00892\n[34]    train-auc:0.89049+0.00221   train-error:0.15968+0.00208 test-auc:0.86753+0.00607    test-error:0.17461+0.00821\n[35]    train-auc:0.89149+0.00200   train-error:0.15901+0.00187 test-auc:0.86818+0.00619    test-error:0.17461+0.00798\n[36]    train-auc:0.89240+0.00218   train-error:0.15817+0.00195 test-auc:0.86906+0.00600    test-error:0.17348+0.00738\n[37]    train-auc:0.89320+0.00217   train-error:0.15738+0.00163 test-auc:0.86959+0.00598    test-error:0.17306+0.00702\n[38]    train-auc:0.89421+0.00205   train-error:0.15709+0.00203 test-auc:0.87023+0.00595    test-error:0.17334+0.00678\n[39]    train-auc:0.89517+0.00184   train-error:0.15637+0.00167 test-auc:0.87059+0.00574    test-error:0.17194+0.00713\n[40]    train-auc:0.89602+0.00201   train-error:0.15584+0.00193 test-auc:0.87119+0.00580    test-error:0.17194+0.00703\n[41]    train-auc:0.89674+0.00209   train-error:0.15503+0.00178 test-auc:0.87165+0.00561    test-error:0.17096+0.00666\n[42]    train-auc:0.89751+0.00187   train-error:0.15461+0.00197 test-auc:0.87219+0.00581    test-error:0.17075+0.00634\n[43]    train-auc:0.89815+0.00172   train-error:0.15370+0.00181 test-auc:0.87248+0.00584    test-error:0.17005+0.00686\n[44]    train-auc:0.89874+0.00174   train-error:0.15345+0.00168 test-auc:0.87294+0.00592    test-error:0.16969+0.00705\n[45]    train-auc:0.89939+0.00179   train-error:0.15279+0.00194 test-auc:0.87343+0.00574    test-error:0.16927+0.00803\n[46]    train-auc:0.89998+0.00179   train-error:0.15263+0.00186 test-auc:0.87389+0.00577    test-error:0.16892+0.00755\n[47]    train-auc:0.90047+0.00176   train-error:0.15253+0.00176 test-auc:0.87417+0.00575    test-error:0.16899+0.00709\n[48]    train-auc:0.90125+0.00180   train-error:0.15179+0.00176 test-auc:0.87472+0.00574    test-error:0.16857+0.00719\n[49]    train-auc:0.90187+0.00182   train-error:0.15130+0.00196 test-auc:0.87509+0.00555    test-error:0.16892+0.00750\n[50]    train-auc:0.90239+0.00183   train-error:0.15091+0.00190 test-auc:0.87533+0.00560    test-error:0.16822+0.00761\n[51]    train-auc:0.90290+0.00182   train-error:0.15063+0.00203 test-auc:0.87571+0.00550    test-error:0.16801+0.00721\n[52]    train-auc:0.90362+0.00182   train-error:0.14988+0.00198 test-auc:0.87622+0.00546    test-error:0.16752+0.00730\n[53]    train-auc:0.90422+0.00196   train-error:0.14884+0.00213 test-auc:0.87670+0.00533    test-error:0.16759+0.00682\n[54]    train-auc:0.90477+0.00187   train-error:0.14860+0.00170 test-auc:0.87705+0.00538    test-error:0.16759+0.00637\n[55]    train-auc:0.90536+0.00181   train-error:0.14825+0.00186 test-auc:0.87739+0.00550    test-error:0.16780+0.00662\n[56]    train-auc:0.90606+0.00176   train-error:0.14800+0.00160 test-auc:0.87788+0.00547    test-error:0.16766+0.00656\n[57]    train-auc:0.90656+0.00176   train-error:0.14777+0.00178 test-auc:0.87829+0.00556    test-error:0.16752+0.00583\n[58]    train-auc:0.90731+0.00179   train-error:0.14684+0.00175 test-auc:0.87876+0.00541    test-error:0.16668+0.00635\n[59]    train-auc:0.90777+0.00167   train-error:0.14642+0.00207 test-auc:0.87901+0.00550    test-error:0.16605+0.00673\n[60]    train-auc:0.90829+0.00167   train-error:0.14598+0.00205 test-auc:0.87925+0.00539    test-error:0.16556+0.00710\n[61]    train-auc:0.90882+0.00157   train-error:0.14498+0.00203 test-auc:0.87944+0.00531    test-error:0.16499+0.00690\n[62]    train-auc:0.90936+0.00156   train-error:0.14470+0.00223 test-auc:0.87978+0.00519    test-error:0.16450+0.00697\n[63]    train-auc:0.90992+0.00167   train-error:0.14442+0.00195 test-auc:0.87999+0.00507    test-error:0.16415+0.00670\n[64]    train-auc:0.91044+0.00156   train-error:0.14383+0.00174 test-auc:0.88045+0.00511    test-error:0.16345+0.00726\n[65]    train-auc:0.91086+0.00151   train-error:0.14344+0.00162 test-auc:0.88068+0.00503    test-error:0.16380+0.00741\n[66]    train-auc:0.91127+0.00157   train-error:0.14332+0.00179 test-auc:0.88093+0.00495    test-error:0.16310+0.00731\n[67]    train-auc:0.91164+0.00154   train-error:0.14297+0.00176 test-auc:0.88121+0.00494    test-error:0.16268+0.00743\n[68]    train-auc:0.91219+0.00160   train-error:0.14206+0.00168 test-auc:0.88160+0.00489    test-error:0.16219+0.00719\n[69]    train-auc:0.91258+0.00150   train-error:0.14200+0.00152 test-auc:0.88181+0.00497    test-error:0.16226+0.00687\n[70]    train-auc:0.91308+0.00161   train-error:0.14179+0.00193 test-auc:0.88207+0.00488    test-error:0.16233+0.00693\n[71]    train-auc:0.91358+0.00164   train-error:0.14139+0.00185 test-auc:0.88226+0.00473    test-error:0.16275+0.00633\n[72]    train-auc:0.91404+0.00157   train-error:0.14093+0.00168 test-auc:0.88246+0.00480    test-error:0.16268+0.00669\n[73]    train-auc:0.91444+0.00146   train-error:0.14044+0.00138 test-auc:0.88255+0.00479    test-error:0.16198+0.00682\n[74]    train-auc:0.91488+0.00135   train-error:0.14011+0.00109 test-auc:0.88289+0.00487    test-error:0.16191+0.00697\n[75]    train-auc:0.91521+0.00132   train-error:0.13981+0.00124 test-auc:0.88314+0.00472    test-error:0.16163+0.00666\n[76]    train-auc:0.91560+0.00132   train-error:0.13962+0.00117 test-auc:0.88334+0.00475    test-error:0.16142+0.00674\n[77]    train-auc:0.91613+0.00137   train-error:0.13897+0.00112 test-auc:0.88357+0.00479    test-error:0.16100+0.00636\n[78]    train-auc:0.91647+0.00139   train-error:0.13857+0.00142 test-auc:0.88367+0.00473    test-error:0.16121+0.00665\n[79]    train-auc:0.91684+0.00148   train-error:0.13825+0.00157 test-auc:0.88379+0.00466    test-error:0.16072+0.00675\n[80]    train-auc:0.91724+0.00156   train-error:0.13783+0.00135 test-auc:0.88388+0.00466    test-error:0.16107+0.00667\n[81]    train-auc:0.91762+0.00170   train-error:0.13771+0.00129 test-auc:0.88404+0.00453    test-error:0.16121+0.00680\n[82]    train-auc:0.91804+0.00175   train-error:0.13716+0.00165 test-auc:0.88433+0.00442    test-error:0.16093+0.00652\n[83]    train-auc:0.91835+0.00175   train-error:0.13683+0.00183 test-auc:0.88458+0.00429    test-error:0.16093+0.00660\n[84]    train-auc:0.91872+0.00177   train-error:0.13622+0.00193 test-auc:0.88485+0.00416    test-error:0.16072+0.00572\n[85]    train-auc:0.91914+0.00172   train-error:0.13583+0.00189 test-auc:0.88491+0.00410    test-error:0.16128+0.00586\n[86]    train-auc:0.91939+0.00173   train-error:0.13553+0.00190 test-auc:0.88500+0.00414    test-error:0.16135+0.00572\n[87]    train-auc:0.91973+0.00179   train-error:0.13529+0.00202 test-auc:0.88523+0.00409    test-error:0.16128+0.00550\n[88]    train-auc:0.92011+0.00178   train-error:0.13509+0.00217 test-auc:0.88546+0.00407    test-error:0.16072+0.00566\n[89]    train-auc:0.92047+0.00186   train-error:0.13492+0.00227 test-auc:0.88571+0.00400    test-error:0.16022+0.00496\n[90]    train-auc:0.92077+0.00188   train-error:0.13436+0.00224 test-auc:0.88584+0.00402    test-error:0.16029+0.00489\n[91]    train-auc:0.92107+0.00179   train-error:0.13427+0.00223 test-auc:0.88609+0.00421    test-error:0.16015+0.00530\n[92]    train-auc:0.92132+0.00185   train-error:0.13420+0.00214 test-auc:0.88620+0.00412    test-error:0.16022+0.00538\n[93]    train-auc:0.92181+0.00186   train-error:0.13346+0.00231 test-auc:0.88652+0.00409    test-error:0.16022+0.00522\n[94]    train-auc:0.92211+0.00187   train-error:0.13320+0.00251 test-auc:0.88668+0.00404    test-error:0.16043+0.00531\n[95]    train-auc:0.92245+0.00189   train-error:0.13290+0.00275 test-auc:0.88684+0.00412    test-error:0.16036+0.00564\n[96]    train-auc:0.92273+0.00186   train-error:0.13262+0.00270 test-auc:0.88708+0.00411    test-error:0.15980+0.00585\n[97]    train-auc:0.92296+0.00185   train-error:0.13223+0.00244 test-auc:0.88728+0.00411    test-error:0.16015+0.00553\n[98]    train-auc:0.92323+0.00193   train-error:0.13204+0.00256 test-auc:0.88747+0.00405    test-error:0.15994+0.00584\n[99]    train-auc:0.92351+0.00189   train-error:0.13225+0.00244 test-auc:0.88762+0.00411    test-error:0.15980+0.00599\n[100]   train-auc:0.92376+0.00192   train-error:0.13192+0.00242 test-auc:0.88771+0.00402    test-error:0.15910+0.00562\n[101]   train-auc:0.92401+0.00186   train-error:0.13178+0.00245 test-auc:0.88783+0.00402    test-error:0.15875+0.00497\n[102]   train-auc:0.92421+0.00184   train-error:0.13145+0.00221 test-auc:0.88794+0.00402    test-error:0.15868+0.00491\n[103]   train-auc:0.92455+0.00189   train-error:0.13120+0.00271 test-auc:0.88812+0.00401    test-error:0.15840+0.00459\n[104]   train-auc:0.92484+0.00192   train-error:0.13109+0.00266 test-auc:0.88825+0.00407    test-error:0.15847+0.00448\n[105]   train-auc:0.92510+0.00192   train-error:0.13130+0.00259 test-auc:0.88838+0.00406    test-error:0.15868+0.00468\n[106]   train-auc:0.92537+0.00198   train-error:0.13076+0.00265 test-auc:0.88853+0.00401    test-error:0.15861+0.00487\n[107]   train-auc:0.92566+0.00185   train-error:0.13039+0.00245 test-auc:0.88858+0.00401    test-error:0.15882+0.00514\n[108]   train-auc:0.92592+0.00193   train-error:0.13016+0.00254 test-auc:0.88878+0.00388    test-error:0.15840+0.00470\n[109]   train-auc:0.92621+0.00197   train-error:0.12987+0.00215 test-auc:0.88890+0.00389    test-error:0.15833+0.00437\n[110]   train-auc:0.92650+0.00196   train-error:0.12973+0.00220 test-auc:0.88908+0.00399    test-error:0.15833+0.00438\n[111]   train-auc:0.92676+0.00198   train-error:0.12910+0.00219 test-auc:0.88924+0.00401    test-error:0.15896+0.00484\n[112]   train-auc:0.92708+0.00182   train-error:0.12866+0.00182 test-auc:0.88944+0.00419    test-error:0.15861+0.00484\n[113]   train-auc:0.92734+0.00173   train-error:0.12885+0.00182 test-auc:0.88956+0.00433    test-error:0.15875+0.00481\n[114]   train-auc:0.92762+0.00158   train-error:0.12841+0.00162 test-auc:0.88968+0.00435    test-error:0.15798+0.00515\n[115]   train-auc:0.92784+0.00157   train-error:0.12822+0.00151 test-auc:0.88984+0.00455    test-error:0.15840+0.00543\n[116]   train-auc:0.92801+0.00162   train-error:0.12785+0.00178 test-auc:0.88991+0.00449    test-error:0.15805+0.00512\n[117]   train-auc:0.92827+0.00170   train-error:0.12759+0.00154 test-auc:0.89006+0.00442    test-error:0.15819+0.00504\n[118]   train-auc:0.92855+0.00173   train-error:0.12764+0.00168 test-auc:0.89017+0.00431    test-error:0.15784+0.00472\n[119]   train-auc:0.92875+0.00159   train-error:0.12738+0.00156 test-auc:0.89027+0.00434    test-error:0.15798+0.00460\n[120]   train-auc:0.92916+0.00172   train-error:0.12685+0.00185 test-auc:0.89041+0.00426    test-error:0.15798+0.00451\n[121]   train-auc:0.92940+0.00168   train-error:0.12689+0.00207 test-auc:0.89047+0.00422    test-error:0.15805+0.00474\n[122]   train-auc:0.92968+0.00176   train-error:0.12667+0.00200 test-auc:0.89059+0.00423    test-error:0.15826+0.00480\n[123]   train-auc:0.93003+0.00151   train-error:0.12634+0.00192 test-auc:0.89077+0.00444    test-error:0.15805+0.00511\n[124]   train-auc:0.93039+0.00142   train-error:0.12597+0.00194 test-auc:0.89087+0.00440    test-error:0.15784+0.00517\n[125]   train-auc:0.93072+0.00140   train-error:0.12578+0.00193 test-auc:0.89106+0.00442    test-error:0.15791+0.00495\n[126]   train-auc:0.93100+0.00132   train-error:0.12545+0.00176 test-auc:0.89125+0.00447    test-error:0.15784+0.00521\n[127]   train-auc:0.93128+0.00136   train-error:0.12515+0.00153 test-auc:0.89134+0.00443    test-error:0.15742+0.00542\n[128]   train-auc:0.93156+0.00133   train-error:0.12501+0.00159 test-auc:0.89149+0.00447    test-error:0.15742+0.00534\n[129]   train-auc:0.93173+0.00132   train-error:0.12480+0.00134 test-auc:0.89150+0.00445    test-error:0.15721+0.00563\n[130]   train-auc:0.93193+0.00129   train-error:0.12445+0.00101 test-auc:0.89161+0.00447    test-error:0.15700+0.00605\n[131]   train-auc:0.93219+0.00124   train-error:0.12420+0.00096 test-auc:0.89177+0.00445    test-error:0.15721+0.00610\n[132]   train-auc:0.93233+0.00126   train-error:0.12411+0.00107 test-auc:0.89186+0.00451    test-error:0.15707+0.00568\n[133]   train-auc:0.93256+0.00120   train-error:0.12380+0.00103 test-auc:0.89203+0.00456    test-error:0.15665+0.00595\n[134]   train-auc:0.93276+0.00117   train-error:0.12345+0.00120 test-auc:0.89213+0.00454    test-error:0.15637+0.00582\n[135]   train-auc:0.93306+0.00109   train-error:0.12304+0.00097 test-auc:0.89230+0.00461    test-error:0.15637+0.00516\n[136]   train-auc:0.93341+0.00119   train-error:0.12262+0.00136 test-auc:0.89247+0.00448    test-error:0.15609+0.00452\n[137]   train-auc:0.93353+0.00118   train-error:0.12252+0.00143 test-auc:0.89254+0.00441    test-error:0.15559+0.00408\n[138]   train-auc:0.93374+0.00114   train-error:0.12201+0.00152 test-auc:0.89257+0.00440    test-error:0.15517+0.00424\n[139]   train-auc:0.93389+0.00107   train-error:0.12178+0.00139 test-auc:0.89264+0.00437    test-error:0.15496+0.00419\n[140]   train-auc:0.93414+0.00101   train-error:0.12175+0.00122 test-auc:0.89276+0.00424    test-error:0.15517+0.00471\n[141]   train-auc:0.93431+0.00095   train-error:0.12140+0.00113 test-auc:0.89283+0.00424    test-error:0.15496+0.00500\n[142]   train-auc:0.93445+0.00095   train-error:0.12134+0.00120 test-auc:0.89288+0.00429    test-error:0.15531+0.00494\n[143]   train-auc:0.93474+0.00093   train-error:0.12113+0.00110 test-auc:0.89293+0.00422    test-error:0.15566+0.00516\n[144]   train-auc:0.93497+0.00104   train-error:0.12096+0.00108 test-auc:0.89298+0.00420    test-error:0.15538+0.00500\n[145]   train-auc:0.93517+0.00101   train-error:0.12068+0.00119 test-auc:0.89301+0.00427    test-error:0.15503+0.00562\n[146]   train-auc:0.93544+0.00106   train-error:0.12043+0.00112 test-auc:0.89311+0.00424    test-error:0.15468+0.00540\n[147]   train-auc:0.93566+0.00106   train-error:0.12019+0.00114 test-auc:0.89325+0.00425    test-error:0.15405+0.00535\n[148]   train-auc:0.93580+0.00107   train-error:0.12005+0.00113 test-auc:0.89333+0.00429    test-error:0.15440+0.00530\n[149]   train-auc:0.93610+0.00123   train-error:0.12005+0.00115 test-auc:0.89358+0.00425    test-error:0.15412+0.00534\n[150]   train-auc:0.93633+0.00124   train-error:0.11964+0.00118 test-auc:0.89377+0.00428    test-error:0.15370+0.00540\n[151]   train-auc:0.93658+0.00115   train-error:0.11961+0.00108 test-auc:0.89382+0.00426    test-error:0.15384+0.00525\n[152]   train-auc:0.93671+0.00116   train-error:0.11961+0.00104 test-auc:0.89396+0.00424    test-error:0.15356+0.00541\n[153]   train-auc:0.93699+0.00115   train-error:0.11908+0.00103 test-auc:0.89407+0.00423    test-error:0.15321+0.00527\n[154]   train-auc:0.93717+0.00114   train-error:0.11877+0.00096 test-auc:0.89416+0.00424    test-error:0.15335+0.00528\n[155]   train-auc:0.93740+0.00122   train-error:0.11854+0.00085 test-auc:0.89423+0.00424    test-error:0.15321+0.00505\n[156]   train-auc:0.93756+0.00117   train-error:0.11850+0.00073 test-auc:0.89433+0.00432    test-error:0.15321+0.00458\n[157]   train-auc:0.93777+0.00124   train-error:0.11812+0.00098 test-auc:0.89441+0.00436    test-error:0.15307+0.00466\n[158]   train-auc:0.93797+0.00134   train-error:0.11794+0.00113 test-auc:0.89441+0.00436    test-error:0.15328+0.00455\n[159]   train-auc:0.93816+0.00129   train-error:0.11782+0.00119 test-auc:0.89441+0.00442    test-error:0.15328+0.00451\n[160]   train-auc:0.93833+0.00135   train-error:0.11764+0.00126 test-auc:0.89443+0.00445    test-error:0.15307+0.00484\n[161]   train-auc:0.93854+0.00136   train-error:0.11736+0.00140 test-auc:0.89457+0.00443    test-error:0.15293+0.00404\n[162]   train-auc:0.93866+0.00136   train-error:0.11720+0.00136 test-auc:0.89464+0.00443    test-error:0.15279+0.00419\n[163]   train-auc:0.93886+0.00144   train-error:0.11701+0.00131 test-auc:0.89474+0.00440    test-error:0.15202+0.00423\n[164]   train-auc:0.93899+0.00140   train-error:0.11654+0.00121 test-auc:0.89481+0.00443    test-error:0.15209+0.00417\n[165]   train-auc:0.93925+0.00147   train-error:0.11627+0.00119 test-auc:0.89479+0.00447    test-error:0.15230+0.00444\n[166]   train-auc:0.93939+0.00147   train-error:0.11601+0.00122 test-auc:0.89487+0.00445    test-error:0.15202+0.00428\n[167]   train-auc:0.93968+0.00133   train-error:0.11570+0.00098 test-auc:0.89503+0.00448    test-error:0.15216+0.00388\n[168]   train-auc:0.93981+0.00127   train-error:0.11556+0.00101 test-auc:0.89516+0.00451    test-error:0.15209+0.00359\n[169]   train-auc:0.93993+0.00125   train-error:0.11533+0.00085 test-auc:0.89524+0.00444    test-error:0.15237+0.00324\n[170]   train-auc:0.94008+0.00122   train-error:0.11526+0.00071 test-auc:0.89534+0.00446    test-error:0.15251+0.00319\n[171]   train-auc:0.94030+0.00113   train-error:0.11515+0.00058 test-auc:0.89542+0.00446    test-error:0.15265+0.00332\n[172]   train-auc:0.94054+0.00104   train-error:0.11445+0.00076 test-auc:0.89543+0.00447    test-error:0.15279+0.00353\n[173]   train-auc:0.94080+0.00107   train-error:0.11387+0.00090 test-auc:0.89550+0.00450    test-error:0.15286+0.00384\n[174]   train-auc:0.94096+0.00107   train-error:0.11382+0.00109 test-auc:0.89550+0.00444    test-error:0.15251+0.00330\n[175]   train-auc:0.94125+0.00102   train-error:0.11366+0.00098 test-auc:0.89555+0.00436    test-error:0.15258+0.00297\n[176]   train-auc:0.94139+0.00103   train-error:0.11328+0.00104 test-auc:0.89560+0.00438    test-error:0.15251+0.00285\n[177]   train-auc:0.94153+0.00104   train-error:0.11319+0.00094 test-auc:0.89571+0.00437    test-error:0.15279+0.00309\n[178]   train-auc:0.94169+0.00101   train-error:0.11312+0.00099 test-auc:0.89582+0.00442    test-error:0.15258+0.00303\n[179]   train-auc:0.94194+0.00093   train-error:0.11310+0.00102 test-auc:0.89600+0.00444    test-error:0.15237+0.00297\n[180]   train-auc:0.94207+0.00091   train-error:0.11305+0.00093 test-auc:0.89613+0.00446    test-error:0.15230+0.00234\n[181]   train-auc:0.94218+0.00090   train-error:0.11289+0.00086 test-auc:0.89621+0.00450    test-error:0.15244+0.00249\n[182]   train-auc:0.94229+0.00092   train-error:0.11273+0.00083 test-auc:0.89628+0.00453    test-error:0.15223+0.00225\n[183]   train-auc:0.94245+0.00092   train-error:0.11243+0.00105 test-auc:0.89633+0.00452    test-error:0.15202+0.00279\n\nCross-validation results:\nBest AUC: 0.8947 (+/- 0.0039)\nBest Error: 0.1520 (+/- 0.0040)\n\nTraining final model...\n\nCalculating performance metrics...\n\nAccuracy: 0.8836\nROC AUC: 0.9277\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.94      0.92      2706\n           1       0.79      0.71      0.74       858\n\n    accuracy                           0.88      3564\n   macro avg       0.85      0.82      0.83      3564\nweighted avg       0.88      0.88      0.88      3564\n\n\nConfusion Matrix:\n[[2544  162]\n [ 253  605]]\n\n\n\n# Additional Metrics\nprint(\"\\nCalculating additional performance metrics...\")\n\n# Calculate metrics\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Calculate detailed classification metrics\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\ntotal = tn + fp + fn + tp\n\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nspecificity = tn / (tn + fp)\nfalse_positive_rate = fp / (fp + tn)\nfalse_negative_rate = fn / (fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\nprevalence = (tp + fn) / total\nnegative_predictive_value = tn / (tn + fn)\npositive_likelihood_ratio = recall / false_positive_rate\nnegative_likelihood_ratio = false_negative_rate / specificity\n\n# Print comprehensive metrics\nprint(\"\\nComprehensive Model Performance Metrics:\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\nprint(f\"\\nPrecision: {precision:.4f}\")\nprint(f\"Recall/Sensitivity: {recall:.4f}\")\nprint(f\"Specificity: {specificity:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Prevalence: {prevalence:.4f}\")\nprint(f\"Negative Predictive Value: {negative_predictive_value:.4f}\")\nprint(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\nprint(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")\n\n# Create visualizations\nplt.figure(figsize=(20, 15))\n\n# Plot 1: ROC Curve\nplt.subplot(2, 2, 1)\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, color='darkorange',\n         label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\n\n# Plot 2: Precision-Recall Curve\nplt.subplot(2, 2, 2)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\nplt.plot(recall_curve, precision_curve,\n         label=f'Precision-Recall curve (AP = {average_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\n\n# Plot 3: Feature Importance\nplt.subplot(2, 2, 3)\nimportance = final_model.get_score(importance_type='weight')\nimportance_df = pd.DataFrame(list(importance.items()),\n                           columns=['Feature', 'Importance'])\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\nsns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\nplt.title('Top 10 Feature Importances')\n\n# Plot 4: Confusion Matrix Heatmap\nplt.subplot(2, 2, 4)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# SHAP Analysis\nprint(\"\\nGenerating SHAP values...\")\nexplainer = shap.TreeExplainer(final_model)\nshap_values = explainer.shap_values(X_test)\n\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\nplt.title('SHAP Feature Importance')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test)\nplt.title('SHAP Summary Plot')\nplt.tight_layout()\nplt.show()\n\n# Calibration plot\nplt.figure(figsize=(8, 6))\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Plot')\nplt.tight_layout()\nplt.show()\n\n\nCalculating additional performance metrics...\n\nComprehensive Model Performance Metrics:\nRMSE: 0.2913\nMSE: 0.0849\nMAE: 0.1675\nR-squared: 0.5357\n\nPrecision: 0.7888\nRecall/Sensitivity: 0.7051\nSpecificity: 0.9401\nF1 Score: 0.7446\nPrevalence: 0.2407\nNegative Predictive Value: 0.9095\nPositive Likelihood Ratio: 11.7783\nNegative Likelihood Ratio: 0.3136\n\n\n\n\n\n\n\n\n\n\nGenerating SHAP values..."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "First few rows of dataframe",
    "section": "",
    "text": "First few rows of dataframe\n\n\n\n\n\n\n\n\n\n\nSCRAM\nCYCLE\nEST_ST\nEST_MSA\nREGION\nHWEIGHT\nPWEIGHT\nTBIRTH_YEAR\nABIRTH_YEAR\nRHISPANIC\nAHISPANIC\nRRACE\nARACE\nEEDUC\nAEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nGENID_DESCRIBE\nSEXUAL_ORIENTATION\nTHHLD_NUMPER\nAHHLD_NUMPER\nTHHLD_NUMKID\nAHHLD_NUMKID\nTHHLD_NUMADLT\nKIDS_LT5Y\nKIDS_5_11Y\nKIDS_12_17Y\nENRPUBCHK\nENRPRVCHK\nENRHMSCHK\nTENROLLPUB\nTENROLLPRV\nTENROLLHMSCH\nENROLLNONE\nACTVDUTY1\nACTVDUTY2\nACTVDUTY3\nACTVDUTY4\nACTVDUTY5\nRECVDVACC\nHADCOVIDRV\nSYMPTOMS\nLONGCOVID\nSYMPTMNOW\nWRKLOSSRV\nANYWORK\nKINDWORK\nRSNNOWRKRV\nEXPNS_DIF\nTWDAYS\nCURFOODSUF\nCHILDFOOD\nFOODRSNRV1\nFOODRSNRV2\nFOODRSNRV3\nFOODRSNRV4\nFREEFOOD\nSCHLFDHLP_RV1\nSCHLFDHLP_RV2\nSCHLFDHLP_RV3\nSCHLFDHLP_RV4\nSCHLFDHLP_RV5\nFDBENEFIT1\nANXIOUS\nWORRY\nINTEREST\nDOWN\nHLTHINS1\nHLTHINS2\nHLTHINS3\nHLTHINS4\nHLTHINS5\nHLTHINS6\nHLTHINS7\nHLTHINS8\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nRENTCHNG\nLIVQTRRV\nRENTCUR\nMORTCUR\nTMNTHSBHND\nEVICT\nFORCLOSE\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nSYMPTMIMPCT\nPRICECHNG\nPRICESTRESS\nPRICECONCRN\nTWDAYS_RESP\nFRMLA_YN\nFRMLA_AGE\nFRMLA_DIFFCLT\nGAS1\nGAS2\nGAS3\nGAS4\nSCHLFDHLP_RV6\nSCHLFDHLP_RV7\nSCHLFDHLP_RV8\nFDBENEFIT2\nSCHLFDEXPNS\nND_DISPLACE\nND_TYPE1\nND_TYPE2\nND_TYPE3\nND_TYPE4\nND_TYPE5\nND_HOWLONG\nND_DAMAGE\nND_FDSHRTAGE\nND_WATER\nND_ELCTRC\nND_UNSANITARY\nND_ISOLATE\nND_CRIME\nND_SCAM\nFDBENEFIT3\nBABY_FED\nMHLTH_NEED\nMHLTH_GET\nMHLTH_SATISFD\nMHLTH_DIFFCLT\nMOVEWHY1\nMOVEWHY2\nMOVEWHY3\nMOVEWHY4\nMOVEWHY5\nMOVEWHY6\nMOVEWHY7\nMOVEWHY8\nMOVED\nWHENCOVIDRV1\nWHENCOVIDRV2\nWHENCOVIDRV3\nVETERAN1\nVETERAN2\nVETERAN3\nVETERAN4\nVETERAN5\nCHILDCARE\nCHILDCARE_RSLT1\nCHILDCARE_RSLT2\nCHILDCARE_RSLT3\nCHILDCARE_RSLT4\nCHILDCARE_RSLT5\nCHILDCARE_RSLT6\nCHILDCARE_RSLT7\nCHILDCARE_RSLT8\nCHILDCARE_RSLT9\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nSUPPORT4\nSUPPORT1EXP\nRVACCDATE\nRSVVACC\n\n\n\n\n0\nP020000001\n2\n32\nNaN\n4\n704.966315\n2067.690868\n1976\n1\n1\n2\n2\n2\n6\n2\n4\n2\n2\n2\n2\n3\n2\n0\n2\n3\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n1\n-99\n2\n-88\n-99\n1\n1\n-99\n2\n-88\n-88\n-88\n-88\n-88\n1\n4\n4\n4\n4\n-99\n-99\n-99\n1\n-99\n-99\n-99\n-99\n3\n1\n3\n1\n3\n2\n3\n1\n3\n2\n6\n1\n-88\n-88\n-88\n-88\n2\n2\n3\n2\n-88\n4\n-88\n2\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-99\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n5\n3\n2\n1\n4\n1\n2\n-99\n-88\n\n\n1\nP020000002\n2\n53\nNaN\n4\n716.582115\n1359.474802\n1961\n2\n1\n2\n1\n2\n5\n2\n3\n2\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n-99\n4\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n3\n3\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n\n\n2\nP020000003\n2\n6\n31080.0\n4\n2439.529962\n4554.378984\n1988\n2\n1\n2\n1\n2\n7\n2\n1\n2\n2\n2\n3\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n3\n2\n2\n2\n1\n1\n-88\n1\n1\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n7\n-88\n4\n-88\n4\n1\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n-99\n1\n-99\n1\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n4\n2\n2\n1\n1\n4\n1\n-88\n\n\n3\nP020000004\n2\n48\nNaN\n2\n3945.461037\n7550.581707\n1956\n2\n1\n2\n1\n2\n5\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n2\n-88\n-88\n-88\n2\n1\n-99\n-88\n2\n4\n2\n-88\n1\n-99\n-99\n-99\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n2\n-88\n1\n2\n1\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n3\n3\n4\n2\n4\n1\n1\n\n\n4\nP020000005\n2\n53\n42660.0\n4\n489.900163\n929.421644\n1970\n2\n1\n2\n1\n2\n6\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n2\n2\n2\n2\n1\n1\n-88\n1\n3\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n8\n-88\n1\n2\n1\n4\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n2\n1\n1\n1\n4\n2\n-88\n\n\n\n\n\n\n\nSource: SAE.ipynb\n\n\nEST_ST distribution\n\n\n\nEST_ST\n06    1485\n53     842\n48     841\n51     596\n25     587\n08     572\n24     561\n42     496\n12     482\n41     459\n11     449\n13     449\n04     448\n17     424\n36     423\n26     413\n27     392\n49     373\n34     340\n37     323\n09     306\n55     303\n29     278\n39     261\n18     250\n47     248\n33     210\n19     203\n35     202\n20     202\n21     192\n45     185\n16     177\n40     164\n50     160\n31     155\n44     150\n32     145\n01     140\n02     138\n23     138\n05     137\n10     128\n30     118\n15     107\n22     103\n46     101\n28      83\n54      80\n38      71\n56      49\nName: count, dtype: int64\n\n\n\n\n\nEST_MSA distribution\n\n\n\nEST_MSA\n47900.0    1125\n42660.0     628\n41860.0     586\n35620.0     537\n14460.0     522\n37980.0     441\n31080.0     404\n16980.0     388\n19100.0     386\n12060.0     370\n38060.0     350\n26420.0     256\n19820.0     234\n33100.0     194\n40140.0     176\nName: count, dtype: int64\n\n\n\n\n\nState-Wise Telecommuting Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\nCity-Wise Telecommuting Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting Frequency Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Educational Attainment\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Employment Sector\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by Number of People in the Household\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecommuting by presence of Children in the Household"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resource List - RUCI LAB",
    "section": "",
    "text": "HPS, CPS, PUMS Binary Classification Modeling & Explainability\nResults & Analysis\n\n\nSpatial Microsimulation using the Household Pulse Survey & American Community Survey\nR markdown notebook\n\n\nLiterature Review (Use of Big Data & ML in Public Transport)\nSheet link\n\n\nOrdinal Regression Models (for Teleworking Days) using Household Pulse Survey\nEDA performed on the dataset\n\n\nData Dictionary (CDC, EPA, USGS)\nGo to dictionary\n\n\nMaps on Felt\n\nNew Brunswick Area Map Go to map\nJersey City Area Map Go to map\n\n\n\nBox Link\nGo to box"
  },
  {
    "objectID": "SAE.html",
    "href": "SAE.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\n\npd.set_option('display.max_columns', None)\n\n\ndf1 = pd.read_csv('hps_04_00_02_puf.csv')\ndf1.head()\n\n\n\n\n\n\n\n\nSCRAM\nCYCLE\nEST_ST\nEST_MSA\nREGION\nHWEIGHT\nPWEIGHT\nTBIRTH_YEAR\nABIRTH_YEAR\nRHISPANIC\nAHISPANIC\nRRACE\nARACE\nEEDUC\nAEDUC\nMS\nEGENID_BIRTH\nAGENID_BIRTH\nGENID_DESCRIBE\nSEXUAL_ORIENTATION\nTHHLD_NUMPER\nAHHLD_NUMPER\nTHHLD_NUMKID\nAHHLD_NUMKID\nTHHLD_NUMADLT\nKIDS_LT5Y\nKIDS_5_11Y\nKIDS_12_17Y\nENRPUBCHK\nENRPRVCHK\nENRHMSCHK\nTENROLLPUB\nTENROLLPRV\nTENROLLHMSCH\nENROLLNONE\nACTVDUTY1\nACTVDUTY2\nACTVDUTY3\nACTVDUTY4\nACTVDUTY5\nRECVDVACC\nHADCOVIDRV\nSYMPTOMS\nLONGCOVID\nSYMPTMNOW\nWRKLOSSRV\nANYWORK\nKINDWORK\nRSNNOWRKRV\nEXPNS_DIF\nTWDAYS\nCURFOODSUF\nCHILDFOOD\nFOODRSNRV1\nFOODRSNRV2\nFOODRSNRV3\nFOODRSNRV4\nFREEFOOD\nSCHLFDHLP_RV1\nSCHLFDHLP_RV2\nSCHLFDHLP_RV3\nSCHLFDHLP_RV4\nSCHLFDHLP_RV5\nFDBENEFIT1\nANXIOUS\nWORRY\nINTEREST\nDOWN\nHLTHINS1\nHLTHINS2\nHLTHINS3\nHLTHINS4\nHLTHINS5\nHLTHINS6\nHLTHINS7\nHLTHINS8\nPRIVHLTH\nPUBHLTH\nSEEING\nHEARING\nREMEMBERING\nMOBILITY\nSELFCARE\nUNDERSTAND\nTENURE\nRENTCHNG\nLIVQTRRV\nRENTCUR\nMORTCUR\nTMNTHSBHND\nEVICT\nFORCLOSE\nENERGY\nHSE_TEMP\nENRGY_BILL\nINCOME\nSYMPTMIMPCT\nPRICECHNG\nPRICESTRESS\nPRICECONCRN\nTWDAYS_RESP\nFRMLA_YN\nFRMLA_AGE\nFRMLA_DIFFCLT\nGAS1\nGAS2\nGAS3\nGAS4\nSCHLFDHLP_RV6\nSCHLFDHLP_RV7\nSCHLFDHLP_RV8\nFDBENEFIT2\nSCHLFDEXPNS\nND_DISPLACE\nND_TYPE1\nND_TYPE2\nND_TYPE3\nND_TYPE4\nND_TYPE5\nND_HOWLONG\nND_DAMAGE\nND_FDSHRTAGE\nND_WATER\nND_ELCTRC\nND_UNSANITARY\nND_ISOLATE\nND_CRIME\nND_SCAM\nFDBENEFIT3\nBABY_FED\nMHLTH_NEED\nMHLTH_GET\nMHLTH_SATISFD\nMHLTH_DIFFCLT\nMOVEWHY1\nMOVEWHY2\nMOVEWHY3\nMOVEWHY4\nMOVEWHY5\nMOVEWHY6\nMOVEWHY7\nMOVEWHY8\nMOVED\nWHENCOVIDRV1\nWHENCOVIDRV2\nWHENCOVIDRV3\nVETERAN1\nVETERAN2\nVETERAN3\nVETERAN4\nVETERAN5\nCHILDCARE\nCHILDCARE_RSLT1\nCHILDCARE_RSLT2\nCHILDCARE_RSLT3\nCHILDCARE_RSLT4\nCHILDCARE_RSLT5\nCHILDCARE_RSLT6\nCHILDCARE_RSLT7\nCHILDCARE_RSLT8\nCHILDCARE_RSLT9\nSOCIAL1\nSOCIAL2\nSUPPORT1\nSUPPORT2\nSUPPORT3\nSUPPORT4\nSUPPORT1EXP\nRVACCDATE\nRSVVACC\n\n\n\n\n0\nP020000001\n2\n32\nNaN\n4\n704.966315\n2067.690868\n1976\n1\n1\n2\n2\n2\n6\n2\n4\n2\n2\n2\n2\n3\n2\n0\n2\n3\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n1\n-99\n2\n-88\n-99\n1\n1\n-99\n2\n-88\n-88\n-88\n-88\n-88\n1\n4\n4\n4\n4\n-99\n-99\n-99\n1\n-99\n-99\n-99\n-99\n3\n1\n3\n1\n3\n2\n3\n1\n3\n2\n6\n1\n-88\n-88\n-88\n-88\n2\n2\n3\n2\n-88\n4\n-88\n2\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n1\n-99\n-99\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n5\n3\n2\n1\n4\n1\n2\n-99\n-88\n\n\n1\nP020000002\n2\n53\nNaN\n4\n716.582115\n1359.474802\n1961\n2\n1\n2\n1\n2\n5\n2\n3\n2\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-99\n-88\n-88\n-99\n4\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n3\n3\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n-99\n-99\n-99\n-99\n-99\n-88\n-99\n\n\n2\nP020000003\n2\n6\n31080.0\n4\n2439.529962\n4554.378984\n1988\n2\n1\n2\n1\n2\n7\n2\n1\n2\n2\n2\n3\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n3\n2\n2\n2\n1\n1\n-88\n1\n1\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n7\n-88\n4\n-88\n4\n1\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n-99\n1\n-99\n1\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n4\n2\n2\n1\n1\n4\n1\n-88\n\n\n3\nP020000004\n2\n48\nNaN\n2\n3945.461037\n7550.581707\n1956\n2\n1\n2\n1\n2\n5\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n2\n-88\n-88\n-88\n2\n1\n-99\n-88\n2\n4\n2\n-88\n1\n-99\n-99\n-99\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n2\n-88\n1\n2\n1\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n3\n3\n4\n2\n4\n1\n1\n\n\n4\nP020000005\n2\n53\n42660.0\n4\n489.900163\n929.421644\n1970\n2\n1\n2\n1\n2\n6\n2\n1\n1\n2\n-88\n2\n2\n2\n0\n2\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-99\n-99\n-99\n-99\n1\n1\n2\n2\n2\n2\n1\n1\n-88\n1\n3\n1\n-88\n-88\n-88\n-88\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-99\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n-88\n2\n-88\n1\n-88\n-88\n-88\n4\n4\n4\n8\n-88\n1\n2\n1\n4\n-88\n-88\n-88\n-99\n-99\n-99\n1\n-88\n-88\n-88\n-99\n-88\n2\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n1\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-99\n-99\n1\n1\n-99\n-99\n-99\n-99\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n-88\n2\n5\n2\n1\n1\n1\n4\n2\n-88\n\n\n\n\n\n\n\n\ndf1['EST_ST'] = df1['EST_ST'].astype(str).str.pad(2, side='left', fillchar='0')\n\n\n# Filter the dataset to include only records with TWDAYS values 1, 2, or 3\nfiltered_data = df1[df1['TWDAYS_RESP'].isin([1, 2, 3])]\n\n# Basic EDA on the filtered dataset\n# Count of records by state\nstate_distribution = filtered_data['EST_ST'].value_counts()\nmsa_distribution = filtered_data['EST_MSA'].value_counts()\n# Summary statistics for telework days\ntelework_days_summary = filtered_data['TWDAYS_RESP'].describe()\n\n\nprint(state_distribution)\n\nEST_ST\n06    1485\n53     842\n48     841\n51     596\n25     587\n08     572\n24     561\n42     496\n12     482\n41     459\n11     449\n13     449\n04     448\n17     424\n36     423\n26     413\n27     392\n49     373\n34     340\n37     323\n09     306\n55     303\n29     278\n39     261\n18     250\n47     248\n33     210\n19     203\n35     202\n20     202\n21     192\n45     185\n16     177\n40     164\n50     160\n31     155\n44     150\n32     145\n01     140\n02     138\n23     138\n05     137\n10     128\n30     118\n15     107\n22     103\n46     101\n28      83\n54      80\n38      71\n56      49\nName: count, dtype: int64\n\n\n\nprint(msa_distribution)\n\nEST_MSA\n47900.0    1125\n42660.0     628\n41860.0     586\n35620.0     537\n14460.0     522\n37980.0     441\n31080.0     404\n16980.0     388\n19100.0     386\n12060.0     370\n38060.0     350\n26420.0     256\n19820.0     234\n33100.0     194\n40140.0     176\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Telecommuting Frequency Distribution\nplt.figure(figsize=(10, 6))\nsns.countplot(x='TWDAYS_RESP', data=filtered_data)\nplt.title('Distribution of Telecommuting Frequency')\nplt.xlabel('Number of Telework Days')\nplt.ylabel('Count')\nplt.xticks([0, 1, 2], ['1-2 Days', '3-4 Days', '5+ Days'])\nplt.show()\n\n\n\n\n\n\n\n\n\n# State-wise Telecommuting Patterns\nplt.figure(figsize=(15, 8))\nstate_counts = filtered_data['EST_ST'].value_counts().sort_index()\nsns.barplot(x=state_counts.index, y=state_counts.values)\nplt.title('State-wise Telecommuting Patterns')\nplt.xlabel('State Identifier')\nplt.ylabel('Count of Telecommuting Reports')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Given the potentially large number of MSAs, we'll focus on the top 10 MSAs by count of telecommuting reports for clarity in visualization.\n\n# Identifying the top 10 MSAs by count of telecommuting reports\n# top_msas = filtered_data['EST_MSA'].value_counts().nlargest(10).index\n\n# # Filtering data for top 10 MSAs\n# top_msa_data = filtered_data[filtered_data['EST_MSA'].isin(top_msas)]\n\n# # Visualizing Telecommuting by MSA\n# plt.figure(figsize=(14, 8))\n# sns.countplot(y='EST_MSA', hue='TWDAYS', data=top_msa_data, palette='coolwarm', order=top_msas)\n# plt.title('Telecommuting by Metropolitan Statistical Area (Top 10 MSAs)')\n# plt.xlabel('Count')\n# plt.ylabel('MSA')\n# plt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\n# plt.tight_layout()\n\n# plt.show()\n\nplt.figure(figsize=(15, 8))\ncity_counts = filtered_data['EST_MSA'].value_counts().sort_index()\nsns.barplot(x=city_counts.index, y=city_counts.values)\nplt.title('MSA-wise Telecommuting Patterns')\nplt.xlabel('MSA Identifier')\nplt.ylabel('Count of Telecommuting Reports')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\nfiltered_data = filtered_data[filtered_data['EEDUC'].isin(range(1, 8))]\neducation_map = {\n    1: \"Less than high school\",\n    2: \"Some high school\",\n    3: \"High school graduate or equivalent (for example GED)\",\n    4: \"Some college, but degree not received or is in progress\",\n    5: \"Associate’s degree (for example AA, AS)\",\n    6: \"Bachelor's degree (for example BA, BS, AB)\",\n    7: \"Graduate degree (for example master's, professional, doctorate)\"\n}\nfiltered_data['EEDUC'] = filtered_data['EEDUC'].map(education_map)\n\n# Now plotting the filtered and mapped data\nplt.figure(figsize=(12, 8))\nsns.countplot(x='EEDUC', hue='TWDAYS_RESP', data=filtered_data)\nplt.title('Telecommuting by Educational Attainment')\nplt.xlabel('Educational Attainment')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfiltered_data = filtered_data[filtered_data['KINDWORK'].isin(range(1, 6))]\n\n# Mapping the 'KINDWORK' codes to the text labels\nwork_kind_map = {\n    1: \"Government\",\n    2: \"Private company\",\n    3: \"Non-profit organization including tax exempt and charitable organizations\",\n    4: \"Self-employed\",\n    5: \"Working in a family business\"\n}\nfiltered_data['KINDWORK'] = filtered_data['KINDWORK'].map(work_kind_map)\n\n# Now plotting the filtered and mapped data\nplt.figure(figsize=(12, 8))\nsns.countplot(x='KINDWORK', hue='TWDAYS_RESP', data=filtered_data)\nplt.title('Telecommuting by Employment Sector')\nplt.xlabel('Employment Sector')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualizing Telecommuting by Total Number of People in the Household\nplt.figure(figsize=(14, 7))\nsns.countplot(x='THHLD_NUMPER', hue='TWDAYS_RESP', data=filtered_data, palette='viridis')\nplt.title('Telecommuting by Total Number of People in the Household')\nplt.xlabel('Total Number of People in Household')\nplt.ylabel('Count')\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Visualizing Telecommuting by Presence of Children in the Household\n# For simplicity, combining children count variables into a binary presence of children in household variable\nfiltered_data['CHILDREN_PRESENT'] = filtered_data[['KIDS_LT5Y', 'KIDS_5_11Y', 'KIDS_12_17Y']].max(axis=1) &gt; 0\nplt.figure(figsize=(10, 6))\nsns.countplot(x='CHILDREN_PRESENT', hue='TWDAYS_RESP', data=filtered_data, palette='Set2')\nplt.title('Telecommuting by Presence of Children in the Household')\nplt.xlabel('Children Present in Household')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.legend(title='Telework Days', labels=['1-2 Days', '3-4 Days', '5+ Days'])\nplt.tight_layout()\n\nplt.show()"
  }
]